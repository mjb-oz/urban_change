{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Classification Urban Change Detection\n",
    "This notebook is the complete workflow for urban change detection algorithm.\n",
    "The goal of this process is to be able to identigfy pixels that have been urbanised (changed from vegetation to the built environment) during the operation of the LandSat Earth Observation Satellites (since 1987).\n",
    "\n",
    "This notebook lets you:\n",
    "- create training data to train the classifier on\n",
    "- classify the data according to 4 broad landcover classes\n",
    "- view the results of your classification process\n",
    "- identify if and when a pixel that was previously not urban becomes dominantly urban (change detection)\n",
    "- view the results of the change detection\n",
    "\n",
    "The markdown cells have been designed to work with the 'Table Of Contents(2)' Jupyter notebook extension.\n",
    "This is highly recommended, if you don't have it yet (and are working on the VDI on the 'agdc-py3-prod module'\n",
    "select \"Edit\" on the menu bar above, click the \"nbextension config\" button at the bottom of the menu, and enable\n",
    "the extension. The 'Collapsible Headings' extension is also highly recommended.\n",
    "\n",
    "This was written Mike Barnes as part of his third graduate rotation, during January 2018.\n",
    "Any questions, please contact me at michael.barnes@ga.gov.au"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import datacube\n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.storage import masking\n",
    "from datacube.storage.masking import mask_to_dict\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import gdal\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider, FloatSlider, Dropdown\n",
    "from IPython.display import display\n",
    "\n",
    "from skimage import exposure\n",
    "from scipy.signal import lfilter\n",
    "\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Loading Data and Building the Xarray\n",
    "This project built on some existing work by Peter Tan. An output from Peter's urban change detection algorithm is raster files with all the relevant NBAR (analysis ready satellite derived surface reflectance readings) data saved to the output directory. To speed the loading and analysis during this script, this notebook will use those exisitng files if they are available. Otherwise it will load the data from the Digital Earth Australia archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: checkForLocalFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkForLocalFiles(study_area):\n",
    "    rootdir = os.listdir('../')\n",
    "    if study_area in rootdir:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: getData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(study_area):\n",
    "    # if the study area is a string, and is accessible locally, load it\n",
    "    if isinstance(study_area, str):\n",
    "        if checkForLocalFiles(study_area):\n",
    "            data = getLocalData(study_area)\n",
    "            return data\n",
    "    # if the study area is a string and is on the list, load it\n",
    "        else:\n",
    "            data = DCLoadName(study_area)\n",
    "    # if the study area is a list of coordinates, use them to load the data\n",
    "    elif isinstance(study_area, list) and len(study_area) == 4:\n",
    "        data = DCLoad(study_area)\n",
    "        \n",
    "    # if the study area isn't loaded locally, transfrom the DC originated xarray into \"my\" format  \n",
    "    if not checkForLocalFiles(study_area):\n",
    "        data = transformXarrayToCustomStyle(data)\n",
    "        return data\n",
    "    else:\n",
    "        print('Data Loading Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: DCLoadName\n",
    "This function is a wrapper for the DCLoad function, that allows previously used study areas to be easily restudied\n",
    "by easily loading exactly the same area of interest (AOI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DCLoadName(study_area):   \n",
    "    if study_area == 'mtbarker':\n",
    "        lat_min = -35.05\n",
    "        lat_max = -35.08\n",
    "        lon_min = 138.85\n",
    "        lon_max = 138.895  \n",
    "    elif study_area == 'swmelb':\n",
    "        lat_min = -37.879\n",
    "        lat_max = -37.91\n",
    "        lon_min = 144.705\n",
    "        lon_max = 144.76  \n",
    "    elif study_area == 'gunghalin':\n",
    "        lat_min = -35.18\n",
    "        lat_max = -35.21\n",
    "        lon_min = 149.14\n",
    "        lon_max = 149.17\n",
    "    elif study_area == 'goldengrove': \n",
    "        lat_min = -34.77\n",
    "        lat_max = -34.8\n",
    "        lon_min = 138.66\n",
    "        lon_max = 138.73\n",
    "    elif study_area == 'molonglo':\n",
    "        lat_min = -35.3\n",
    "        lat_max = -35.33\n",
    "        lon_min = 149.015\n",
    "        lon_max = 149.06\n",
    "    elif study_area == 'nperth':\n",
    "        lat_min = -31.686\n",
    "        lat_max = -31.73\n",
    "        lon_min = 115.79\n",
    "        lon_max = 115.813\n",
    "    elif study_area == 'swbris':\n",
    "        lat_min = -27.66\n",
    "        lat_max = -27.7 \n",
    "        lon_min = 152.877\n",
    "        lon_max = 152.93\n",
    "    elif study_area == 'swsyd':\n",
    "        lat_min = -33.993\n",
    "        lat_max = -34.04\n",
    "        lon_min = 150.715 \n",
    "        lon_max = 150.78\n",
    "    \n",
    "    return DCLoad([lat_min, lat_max, lon_min, lon_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: DCLoad\n",
    "This function is a variation of a datacube query supplied by Erin Telfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DCLoad(study_area):\n",
    "    # to time how long the load takes\n",
    "    start = datetime.datetime.now()\n",
    "    print('Loading data') \n",
    "    print('Load Started At: ' + str(start))\n",
    "    \n",
    "    # define temporal range \n",
    "    start_of_epoch = '1987-01-01'\n",
    "    end_of_epoch =  '2017-10-31'\n",
    "\n",
    "    # define bands of interest\n",
    "    bands_of_interest = ['blue', 'green', 'red', \n",
    "                         'nir', 'swir1', 'swir2']\n",
    "\n",
    "    # Landsat sensors of interest are defined\n",
    "    sensors = ['ls8', 'ls7', 'ls5'] \n",
    "\n",
    "    # unpack input parameter\n",
    "    lat_min, lat_max, lon_min, lon_max = study_area    \n",
    "\n",
    "    print('Bounding box: ' + str(lat_min) + ' S, ' + str(lon_min) +\n",
    "          ' E to ' + str(lat_max) + ' S, ' + str(lon_max) + ' E' )\n",
    "    print('Epoch: ' + start_of_epoch + ' to ' + end_of_epoch)\n",
    "    print('Sensors: ' + str(sensors))\n",
    "    print('Bands of Interest: ' + str(bands_of_interest))\n",
    "\n",
    "    # create query\n",
    "    query = {'time': (start_of_epoch, end_of_epoch),}\n",
    "    query['x'] = (lon_min, lon_max)\n",
    "    query['y'] = (lat_max, lat_min)\n",
    "    query['crs'] = 'EPSG:4326'\n",
    "\n",
    "    #Create cloud mask. This will define which pixel quality (PQ) artefacts are removed from the results.\n",
    "    # It should be noted the \"land_sea\" code will remove all ocean/sea pixels.\n",
    "    mask_components = {'cloud_acca':'no_cloud',\n",
    "    'cloud_shadow_acca' :'no_cloud_shadow',\n",
    "    'cloud_shadow_fmask' : 'no_cloud_shadow',\n",
    "    'cloud_fmask' :'no_cloud',\n",
    "    'blue_saturated' : False,\n",
    "    'green_saturated' : False,\n",
    "    'red_saturated' : False,\n",
    "    'nir_saturated' : False,\n",
    "    'swir1_saturated' : False,\n",
    "    'swir2_saturated' : False,\n",
    "    'contiguous':True,\n",
    "    'land_sea': 'land'}\n",
    "\n",
    "    # Connect to DataCube\n",
    "    dc = datacube.Datacube(app='Urban Change Detection')\n",
    "    \n",
    "    # Data for each Landsat sensor is retrieved and saved in a dict for concatenation\n",
    "    sensor_clean = {}\n",
    "    \n",
    "    for sensor in sensors:\n",
    "        # Load the NBAR and corresponding PQ\n",
    "        sensor_nbar = dc.load(product= sensor+'_nbar_albers', group_by='solar_day', \n",
    "                              measurements = bands_of_interest,  **query)\n",
    "        sensor_pq = dc.load(product= sensor+'_pq_albers', group_by='solar_day', \n",
    "                            fuse_func=ga_pq_fuser, **query)\n",
    "\n",
    "        # Retrieve the projection information before masking/sorting\n",
    "        crs = sensor_nbar.crs\n",
    "        crswkt = sensor_nbar.crs.wkt\n",
    "        affine = sensor_nbar.affine        \n",
    "\n",
    "        # Combing the pq so it is a single \n",
    "        sensor_all = xr.auto_combine([sensor_pq,sensor_nbar])\n",
    "        sensor_clean[sensor] = sensor_all\n",
    "\n",
    "        print('Loaded %s' % sensor) \n",
    "\n",
    "    print('Concatenating')\n",
    "    nbar_clean = xr.concat(sensor_clean.values(), 'time')\n",
    "    nbar_clean = nbar_clean.sortby('time')\n",
    "    nbar_clean.attrs['crs'] = crs\n",
    "    nbar_clean.attrs['affin|e'] = affine    \n",
    "\n",
    "    print ('Load and Xarray build complete')\n",
    "    print('Process took ' + str(datetime.datetime.now() - start))\n",
    "    \n",
    "    # return xarray\n",
    "    return nbar_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: getLocalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLocalData(study_area):\n",
    "    \"\"\"A quick helper function to load the output files from Peter's code for the given location.\n",
    "    It returns and Xarray of the landsat data for that study area.\"\"\"\n",
    "    # build a list of all files in the directory (ie the folder for that location)\n",
    "    location = '../' + study_area + '/'\n",
    "    files = os.listdir(location)\n",
    "\n",
    "    print('Loading data from: ' + location)\n",
    "    \n",
    "    # build a list of all the NBAR*.img file names and which bands they represent\n",
    "    NBARfiles = []\n",
    "    bands = []\n",
    "    for file in files:\n",
    "        if file[-4::] == '.img' and file[0:4] == 'NBAR':\n",
    "            NBARfiles.append(file)\n",
    "            bands.append(file.split('NBAR_')[1].split('.img')[0])\n",
    "\n",
    "    # open all the .img files with NBAR in the name, convert to numpy array, swap axes so order is (x, y, t)\n",
    "    # and save to dict\n",
    "    raw_data = {}\n",
    "    for i in range(len(NBARfiles)):\n",
    "        raw_data[bands[i]] = gdal.Open(location + NBARfiles[i]).ReadAsArray().swapaxes(0,2)\n",
    "#     num_scenes = len(raw_data['red'][0][0])   # delete this?\n",
    "\n",
    "    # build a list of all the dates represented by each band in the NBAR files\n",
    "    # reuse the list of NBAR file names, but this time access the .hdr file\n",
    "    in_dates = False\n",
    "    dates = []\n",
    "    for line in open(location + NBARfiles[0].split('.img')[0] + '.hdr'):\n",
    "        if line[0] == '}':\n",
    "            continue\n",
    "        if in_dates:\n",
    "            dates.append(line.split(',')[0].strip())\n",
    "        if line[0:10] == 'band names':\n",
    "            in_dates = True\n",
    "\n",
    "    # save list of satellite originated bands\n",
    "    sat_bands = bands.copy()\n",
    "\n",
    "    # add the yet to be calculated derivative bands to the overall bands list\n",
    "    bands += ['cloud_mask']\n",
    "\n",
    "    # building the Xarray\n",
    "    # define the size for the numpy array that will hold all the data for conversion into XArray\n",
    "    x = len(raw_data['red'])\n",
    "    y = len(raw_data['red'][0])\n",
    "    t = len(raw_data['red'][0][0])\n",
    "    n = len(bands)\n",
    "\n",
    "    # create an empty numpy array of the correct size\n",
    "    alldata = np.zeros((x, y, t, n), dtype=np.float32)\n",
    "\n",
    "    # populate the numpy array with the satellite data\n",
    "    # turn all no data NBAR values to NaNs\n",
    "    for i in range(len(sat_bands)):\n",
    "        alldata[:,:,:,i] = raw_data[sat_bands[i]]\n",
    "        alldata[:,:,:,i][alldata[:,:,:,i] == -999] = np.nan\n",
    "\n",
    "    # convert the numpy array into an xarray, with appropriate lables, and axes names\n",
    "    data = xr.DataArray(alldata, coords = {'x':range(x), 'y':range(y), 'date':dates, 'band':bands},\n",
    "                 dims=['x', 'y', 'date', 'band'])\n",
    "    \n",
    "    # import cloudmask and add to xarray\n",
    "    cloudmask = gdal.Open(location + '/tsmask.img').ReadAsArray().swapaxes(0,2)\n",
    "    data.loc[:,:,:,'cloud_mask'] = cloudmask\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: transformXarrayToCustomStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformXarrayToCustomStyle(data_new):\n",
    "    # loop through and extract all the values into a list of 3D numpy arrays\n",
    "    bands = ['blue','green','red','nir','swir1','swir2','pixelquality']\n",
    "    all_data = []\n",
    "    for band in bands:\n",
    "        all_data.append(data_new.variables[band].values.swapaxes(2,0).astype(np.float32))\n",
    "\n",
    "    # replace 'pixelquality' with 'cloud_mask'  \n",
    "    bands[bands.index('pixelquality')] = 'cloud_mask'\n",
    "\n",
    "    # stack the list of 3D arrays along a 4th dimension\n",
    "    data_flipped = np.stack(all_data, axis = -1)\n",
    "    # set all bad NBAR values to np.nan\n",
    "    data_flipped[data_flipped == -999] = np.nan\n",
    "    # set all good pixels to 0 based on PQ indicator\n",
    "    # see https://www.sciencedirect.com/science/article/pii/S0034425717301086 for PQ description\n",
    "    data_flipped[data_flipped[:,:,:,-1] == 16383] = 0\n",
    "\n",
    "    # save the new shape for use in defining the Xarray\n",
    "    new_shape = data_flipped.shape\n",
    "\n",
    "    # build a list of all the dates (as strings)\n",
    "    dates = []\n",
    "    for time in range(len(data_new.time)):\n",
    "        dates.append(np.datetime_as_string(data_new.time[time].values)[0:10])\n",
    "\n",
    "    # assemble the new Xarray\n",
    "    newdatafixed = xr.DataArray(data_flipped, coords = {'x': range(new_shape[0]), 'y': range(new_shape[1]),\n",
    "                                                        'date': dates, 'band': bands}, dims=['x','y','date','band'])\n",
    "    return newdatafixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Generator Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### function: drawTrainingPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def drawTrainingPlot(study_area, scene_num, covertype, scene_picks_arr):\n",
    "    \"\"\"Allows easy extension to extra subplots in the training plot figure\"\"\"\n",
    "    ax1, scene_picks_arr = drawTrainingScene(study_area, scene_num, covertype, scene_picks_arr)\n",
    "    plt.draw()\n",
    "    \n",
    "    return scene_picks_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### function: drawTrainingScene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def drawTrainingScene(study_area, scene_num, covertype, scene_picks_arr):\n",
    "    \"\"\"This function draws the desired scene (study area and scene number).\n",
    "    It also presents the existing training data for that scene if any exists.\n",
    "    It returns the axes object for the image, along with a numpy array which is\n",
    "    the existing picks for that scene\"\"\"\n",
    "    \n",
    "    # get data for selected study area\n",
    "#     data = getData(study_area)\n",
    "    \n",
    "    # colour map included incase of need to display false colour or other in the future\n",
    "    # could change this to an ordereddict and remove the RGB list created below...?\n",
    "    # or have RGB a list, and use that in data.sel(band=RGB).values\n",
    "    colourmap = {'R':'red', 'G':'green', 'B':'blue'}\n",
    "    \n",
    "    # combine the data for the 3 bands to be displayed into a single numpy array\n",
    "    h = data.shape[1]\n",
    "    w = data.shape[0]\n",
    "    t = data.shape[2]\n",
    "    if scene_num > (t -1):\n",
    "        scene_num = t - 1\n",
    "    RGB = ['R','G','B']\n",
    "    date = str(data[:,:,scene_num].date.values)\n",
    "    \n",
    "    # create array to store the RGB info in, and fill by looping through the colourmap variable\n",
    "    # note the .T at the end, because the data array is setup as a (x,y,t), but imshow works (y,x)\n",
    "    rawimg = np.zeros((h, w, 3), dtype=np.float32)\n",
    "    for i in range(len(RGB)):     \n",
    "        rawimg[:,:,i] = data[:,:,scene_num].sel(band=colourmap[RGB[i]]).T\n",
    "        \n",
    "    # equalizing for all bands together\n",
    "    # goal is to make is human interpretable\n",
    "    img_toshow = exposure.equalize_hist(rawimg, mask = np.isfinite(rawimg))    \n",
    "\n",
    "    # displaying the results and formatting the axes etc\n",
    "    plt.imshow(img_toshow)\n",
    "    ax = plt.gca()\n",
    "    ax.set_title('True Colour Landsat Scene, taken\\n' + date + ', over ' + study_area)\n",
    "    \n",
    "   \n",
    "    if scene_picks_arr is None:\n",
    "        \n",
    "        if study_area in trainingdata.index and scene_num in trainingdata.loc[study_area].index:\n",
    "            # if there aren't any picks yet, make the array\n",
    "            scene_picks_arr = np.zeros((h,w), dtype=np.float32)\n",
    "            # fill it with np.nan\n",
    "            scene_picks_arr[scene_picks_arr == 0] = np.nan\n",
    "            # make a dict with the key (study_area, scene_num)\n",
    "            scene_picks_arr = {(study_area, scene_num): scene_picks_arr}\n",
    "            temp = trainingdata.loc[(study_area, scene_num)]\n",
    "            # loop through all relevant training points, and populate the array\n",
    "            for i in range(len(temp)):\n",
    "                position = temp.iloc[i].name\n",
    "                scene_picks_arr[(study_area, scene_num)][position[0], position[1]] = temp['landcover'].iloc[i]\n",
    "        else:\n",
    "            # if not the right scene/location combination, set to None\n",
    "            scene_picks_arr = None\n",
    "    \n",
    "    # if there are picks, then plot them up, coloured as per the environment level variable colours\n",
    "    # should I better tie cmap colours to colours\n",
    "    if scene_picks_arr is not None and (study_area,scene_num) in (scene_picks_arr.keys()):\n",
    "        cmap = colors.ListedColormap(colours)\n",
    "        # plot the training pixels\n",
    "        ax.imshow(scene_picks_arr[(study_area, scene_num)], cmap)\n",
    "        legend_patches = []\n",
    "        # build the legend\n",
    "        for cover in landcover.keys():\n",
    "            legend_patches.append(mpatches.Patch(color = colours[landcover[cover]-1], label = cover))\n",
    "        ax.legend(handles = legend_patches)\n",
    "    else:\n",
    "        # if not the right scene/location combination, set to None\n",
    "        scene_picks_arr = None\n",
    "    \n",
    "    return ax, scene_picks_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### function: train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# some broad scope variables specific to the plotting that need setting up and seem to very fragile\n",
    "# so I'm too scared to move them in case something breaks!\n",
    "global xpos\n",
    "global ypos\n",
    "xpos = 0\n",
    "ypos = 0\n",
    "global scene_picks_arr\n",
    "scene_picks_arr = None\n",
    "colours = ['r', 'b', 'm', 'c']\n",
    "\n",
    "def train(study_area, scene_num, covertype):\n",
    "\n",
    "    def onclick(event):\n",
    "        # defining what to do on a click event\n",
    "        \n",
    "        # I don't understand why this need to be declared global again, but it breaks without these lines\n",
    "        global xpos\n",
    "        global ypos\n",
    "        # need to cast to int as result is a float, and can't index a list with a float\n",
    "        xpos = int(event.xdata)\n",
    "        ypos = int(event.ydata)\n",
    "        # save the results of the click to the training data dataframe\n",
    "        trainingdata.loc[(study_area, scene_num, ypos, xpos)] = landcover[covertype]\n",
    "        # add the results to the current scenes overlay\n",
    "        scene_picks_arr[(study_area,scene_num)][ypos, xpos] = landcover[covertype]\n",
    "        # redraw with the trained pixels updated on the image\n",
    "        drawTrainingPlot(study_area, scene_num, covertype, scene_picks_arr)\n",
    "    \n",
    "    # control the figure size\n",
    "    fig = plt.figure(figsize=[10,10])\n",
    "    axs = fig.axes\n",
    "    plt.subplots_adjust(hspace = 0.6)\n",
    "    \n",
    "    # draw the figure\n",
    "    global scene_picks_arr\n",
    "    scene_picks_arr = drawTrainingPlot(study_area, scene_num, covertype, scene_picks_arr)\n",
    "    #connect the click event action to the figure\n",
    "    cid = fig.canvas.mpl_connect('button_press_event', onclick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up broad scope variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Previous Training Data (or make blank dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>landcover</th>\n",
       "      <th>blue</th>\n",
       "      <th>green</th>\n",
       "      <th>red</th>\n",
       "      <th>nir</th>\n",
       "      <th>swir1</th>\n",
       "      <th>swir2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>study_area</th>\n",
       "      <th>scene_num</th>\n",
       "      <th>row</th>\n",
       "      <th>column</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">mtbarker</th>\n",
       "      <th rowspan=\"30\" valign=\"top\">1</th>\n",
       "      <th>38</th>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "      <td>247.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>4094.0</td>\n",
       "      <td>1305.0</td>\n",
       "      <td>558.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <th>104</th>\n",
       "      <td>1</td>\n",
       "      <td>420.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>3877.0</td>\n",
       "      <td>2284.0</td>\n",
       "      <td>1088.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "      <td>420.0</td>\n",
       "      <td>677.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>3833.0</td>\n",
       "      <td>2284.0</td>\n",
       "      <td>1044.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <th>106</th>\n",
       "      <td>1</td>\n",
       "      <td>362.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>3877.0</td>\n",
       "      <td>2192.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <th>102</th>\n",
       "      <td>1</td>\n",
       "      <td>324.0</td>\n",
       "      <td>636.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>4483.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "      <td>305.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>4093.0</td>\n",
       "      <td>1886.0</td>\n",
       "      <td>823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">25</th>\n",
       "      <th>109</th>\n",
       "      <td>1</td>\n",
       "      <td>324.0</td>\n",
       "      <td>596.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>4527.0</td>\n",
       "      <td>1948.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>677.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>3746.0</td>\n",
       "      <td>2131.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">29</th>\n",
       "      <th>148</th>\n",
       "      <td>1</td>\n",
       "      <td>439.0</td>\n",
       "      <td>958.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>5734.0</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "      <td>362.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>5390.0</td>\n",
       "      <td>1917.0</td>\n",
       "      <td>823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <th>152</th>\n",
       "      <td>1</td>\n",
       "      <td>458.0</td>\n",
       "      <td>1038.0</td>\n",
       "      <td>847.0</td>\n",
       "      <td>5691.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>911.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <th>151</th>\n",
       "      <td>1</td>\n",
       "      <td>382.0</td>\n",
       "      <td>838.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>5820.0</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "      <td>458.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>5906.0</td>\n",
       "      <td>2070.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <th>136</th>\n",
       "      <td>1</td>\n",
       "      <td>477.0</td>\n",
       "      <td>958.0</td>\n",
       "      <td>811.0</td>\n",
       "      <td>5305.0</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <th>144</th>\n",
       "      <td>1</td>\n",
       "      <td>477.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>811.0</td>\n",
       "      <td>4874.0</td>\n",
       "      <td>2315.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <th>145</th>\n",
       "      <td>1</td>\n",
       "      <td>515.0</td>\n",
       "      <td>1038.0</td>\n",
       "      <td>883.0</td>\n",
       "      <td>5219.0</td>\n",
       "      <td>2254.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <th>147</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <th>140</th>\n",
       "      <td>1</td>\n",
       "      <td>477.0</td>\n",
       "      <td>838.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>4571.0</td>\n",
       "      <td>2346.0</td>\n",
       "      <td>1044.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <th>137</th>\n",
       "      <td>1</td>\n",
       "      <td>477.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>811.0</td>\n",
       "      <td>4917.0</td>\n",
       "      <td>2223.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "      <td>382.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>488.0</td>\n",
       "      <td>5563.0</td>\n",
       "      <td>1704.0</td>\n",
       "      <td>647.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <th>167</th>\n",
       "      <td>1</td>\n",
       "      <td>420.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>5606.0</td>\n",
       "      <td>1704.0</td>\n",
       "      <td>647.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <th>168</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>677.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>5348.0</td>\n",
       "      <td>1734.0</td>\n",
       "      <td>647.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <th>167</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>5262.0</td>\n",
       "      <td>1673.0</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>5348.0</td>\n",
       "      <td>1642.0</td>\n",
       "      <td>602.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "      <td>401.0</td>\n",
       "      <td>798.0</td>\n",
       "      <td>596.0</td>\n",
       "      <td>5348.0</td>\n",
       "      <td>1704.0</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <th>166</th>\n",
       "      <td>1</td>\n",
       "      <td>382.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>5391.0</td>\n",
       "      <td>1765.0</td>\n",
       "      <td>779.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <th>169</th>\n",
       "      <td>1</td>\n",
       "      <td>382.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>5262.0</td>\n",
       "      <td>1856.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <th>169</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>5606.0</td>\n",
       "      <td>1734.0</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <th>166</th>\n",
       "      <td>1</td>\n",
       "      <td>305.0</td>\n",
       "      <td>596.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>5434.0</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>647.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>556.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>5348.0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>558.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">swsyd</th>\n",
       "      <th rowspan=\"30\" valign=\"top\">830</th>\n",
       "      <th>117</th>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>170.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>2402.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>170.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2402.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>637.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>170.0</td>\n",
       "      <td>421.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>2402.0</td>\n",
       "      <td>1376.0</td>\n",
       "      <td>563.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>133.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2262.0</td>\n",
       "      <td>1128.0</td>\n",
       "      <td>416.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>243.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>654.0</td>\n",
       "      <td>3306.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>316.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>2890.0</td>\n",
       "      <td>2322.0</td>\n",
       "      <td>1300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>352.0</td>\n",
       "      <td>858.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>3374.0</td>\n",
       "      <td>2322.0</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>424.0</td>\n",
       "      <td>858.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>3651.0</td>\n",
       "      <td>2421.0</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>496.0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>1061.0</td>\n",
       "      <td>3029.0</td>\n",
       "      <td>3416.0</td>\n",
       "      <td>1963.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>352.0</td>\n",
       "      <td>858.0</td>\n",
       "      <td>829.0</td>\n",
       "      <td>3859.0</td>\n",
       "      <td>2820.0</td>\n",
       "      <td>1300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>243.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>479.0</td>\n",
       "      <td>2891.0</td>\n",
       "      <td>1378.0</td>\n",
       "      <td>711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <th>262</th>\n",
       "      <td>1</td>\n",
       "      <td>281.0</td>\n",
       "      <td>641.0</td>\n",
       "      <td>363.0</td>\n",
       "      <td>4821.0</td>\n",
       "      <td>1924.0</td>\n",
       "      <td>784.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <th>258</th>\n",
       "      <td>1</td>\n",
       "      <td>281.0</td>\n",
       "      <td>714.0</td>\n",
       "      <td>363.0</td>\n",
       "      <td>5369.0</td>\n",
       "      <td>2122.0</td>\n",
       "      <td>784.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <th>253</th>\n",
       "      <td>1</td>\n",
       "      <td>317.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>479.0</td>\n",
       "      <td>5573.0</td>\n",
       "      <td>2271.0</td>\n",
       "      <td>711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <th>250</th>\n",
       "      <td>1</td>\n",
       "      <td>317.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>6254.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>784.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <th>177</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <th>180</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <th>129</th>\n",
       "      <td>2</td>\n",
       "      <td>1245.0</td>\n",
       "      <td>1722.0</td>\n",
       "      <td>1869.0</td>\n",
       "      <td>2334.0</td>\n",
       "      <td>2819.0</td>\n",
       "      <td>2699.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <th>123</th>\n",
       "      <td>2</td>\n",
       "      <td>2386.0</td>\n",
       "      <td>3131.0</td>\n",
       "      <td>3350.0</td>\n",
       "      <td>3514.0</td>\n",
       "      <td>3961.0</td>\n",
       "      <td>3508.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <th>125</th>\n",
       "      <td>2</td>\n",
       "      <td>3489.0</td>\n",
       "      <td>4232.0</td>\n",
       "      <td>4193.0</td>\n",
       "      <td>4134.0</td>\n",
       "      <td>5450.0</td>\n",
       "      <td>5274.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <th>130</th>\n",
       "      <td>2</td>\n",
       "      <td>1174.0</td>\n",
       "      <td>1722.0</td>\n",
       "      <td>1581.0</td>\n",
       "      <td>1707.0</td>\n",
       "      <td>2968.0</td>\n",
       "      <td>3066.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <th>55</th>\n",
       "      <td>2</td>\n",
       "      <td>961.0</td>\n",
       "      <td>1364.0</td>\n",
       "      <td>1465.0</td>\n",
       "      <td>1777.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>2110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <th>53</th>\n",
       "      <td>2</td>\n",
       "      <td>961.0</td>\n",
       "      <td>1435.0</td>\n",
       "      <td>1523.0</td>\n",
       "      <td>3029.0</td>\n",
       "      <td>2620.0</td>\n",
       "      <td>2110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <th>76</th>\n",
       "      <td>2</td>\n",
       "      <td>640.0</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>1407.0</td>\n",
       "      <td>2890.0</td>\n",
       "      <td>2520.0</td>\n",
       "      <td>1594.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <th>71</th>\n",
       "      <td>2</td>\n",
       "      <td>460.0</td>\n",
       "      <td>785.0</td>\n",
       "      <td>944.0</td>\n",
       "      <td>2612.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>1447.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <th>77</th>\n",
       "      <td>2</td>\n",
       "      <td>604.0</td>\n",
       "      <td>930.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>2403.0</td>\n",
       "      <td>2122.0</td>\n",
       "      <td>1594.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <th>79</th>\n",
       "      <td>2</td>\n",
       "      <td>640.0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>2612.0</td>\n",
       "      <td>2073.0</td>\n",
       "      <td>1520.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <th>84</th>\n",
       "      <td>2</td>\n",
       "      <td>496.0</td>\n",
       "      <td>930.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>2542.0</td>\n",
       "      <td>2122.0</td>\n",
       "      <td>1594.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <th>89</th>\n",
       "      <td>2</td>\n",
       "      <td>568.0</td>\n",
       "      <td>1219.0</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>3028.0</td>\n",
       "      <td>2520.0</td>\n",
       "      <td>1741.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <th>90</th>\n",
       "      <td>2</td>\n",
       "      <td>604.0</td>\n",
       "      <td>931.0</td>\n",
       "      <td>944.0</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>1924.0</td>\n",
       "      <td>1300.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1322 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 landcover    blue   green     red     nir  \\\n",
       "study_area scene_num row column                                              \n",
       "mtbarker   1         38  105             1   247.0   434.0   344.0  4094.0   \n",
       "                     33  104             1   420.0   717.0   560.0  3877.0   \n",
       "                     32  105             1   420.0   677.0   524.0  3833.0   \n",
       "                     28  106             1   362.0   717.0   524.0  3877.0   \n",
       "                     26  102             1   324.0   636.0   452.0  4483.0   \n",
       "                     24  100             1   305.0   555.0   380.0  4093.0   \n",
       "                     25  109             1   324.0   596.0   416.0  4527.0   \n",
       "                         116             1   363.0   677.0   524.0  3746.0   \n",
       "                     29  148             1   439.0   958.0   739.0  5734.0   \n",
       "                         150             1   362.0   757.0   560.0  5390.0   \n",
       "                     31  152             1   458.0  1038.0   847.0  5691.0   \n",
       "                     35  151             1   382.0   838.0   632.0  5820.0   \n",
       "                     37  150             1   458.0   918.0   739.0  5906.0   \n",
       "                     68  136             1   477.0   958.0   811.0  5305.0   \n",
       "                     61  144             1   477.0   918.0   811.0  4874.0   \n",
       "                     58  145             1   515.0  1038.0   883.0  5219.0   \n",
       "                     55  147             1     NaN     NaN     NaN     NaN   \n",
       "                     56  140             1   477.0   838.0   739.0  4571.0   \n",
       "                     59  137             1   477.0   998.0   811.0  4917.0   \n",
       "                     54  165             1   382.0   758.0   488.0  5563.0   \n",
       "                     55  167             1   420.0   758.0   524.0  5606.0   \n",
       "                     53  168             1   363.0   677.0   524.0  5348.0   \n",
       "                     52  167             1   363.0   758.0   560.0  5262.0   \n",
       "                     51  165             1   363.0   757.0   524.0  5348.0   \n",
       "                     49  165             1   401.0   798.0   596.0  5348.0   \n",
       "                     47  166             1   382.0   757.0   632.0  5391.0   \n",
       "                     46  169             1   382.0   717.0   632.0  5262.0   \n",
       "                     50  169             1   363.0   758.0   560.0  5606.0   \n",
       "                     60  166             1   305.0   596.0   416.0  5434.0   \n",
       "                     59  165             1   363.0   556.0   416.0  5348.0   \n",
       "...                                    ...     ...     ...     ...     ...   \n",
       "swsyd      830       117 10              1   170.0   347.0   362.0  2402.0   \n",
       "                     124 7               1   170.0   347.0   303.0  2402.0   \n",
       "                     105 56              1   170.0   421.0   245.0  2402.0   \n",
       "                     97  56              1   133.0   347.0   303.0  2262.0   \n",
       "                     210 18              1   243.0   713.0   654.0  3306.0   \n",
       "                     204 21              1   316.0   713.0   712.0  2890.0   \n",
       "                     192 27              1   352.0   858.0   770.0  3374.0   \n",
       "                     191 25              1   424.0   858.0   770.0  3651.0   \n",
       "                     238 21              1   496.0  1003.0  1061.0  3029.0   \n",
       "                     239 33              1   352.0   858.0   829.0  3859.0   \n",
       "                     233 41              1   243.0   494.0   479.0  2891.0   \n",
       "                     201 262             1   281.0   641.0   363.0  4821.0   \n",
       "                     194 258             1   281.0   714.0   363.0  5369.0   \n",
       "                     189 253             1   317.0   713.0   479.0  5573.0   \n",
       "                     175 250             1   317.0   713.0   362.0  6254.0   \n",
       "                     234 177             2     NaN     NaN     NaN     NaN   \n",
       "                     235 180             2     NaN     NaN     NaN     NaN   \n",
       "                     230 129             2  1245.0  1722.0  1869.0  2334.0   \n",
       "                     225 123             2  2386.0  3131.0  3350.0  3514.0   \n",
       "                     214 125             2  3489.0  4232.0  4193.0  4134.0   \n",
       "                     213 130             2  1174.0  1722.0  1581.0  1707.0   \n",
       "                     215 55              2   961.0  1364.0  1465.0  1777.0   \n",
       "                     213 53              2   961.0  1435.0  1523.0  3029.0   \n",
       "                     192 76              2   640.0  1075.0  1407.0  2890.0   \n",
       "                     188 71              2   460.0   785.0   944.0  2612.0   \n",
       "                     178 77              2   604.0   930.0  1118.0  2403.0   \n",
       "                     175 79              2   640.0  1003.0  1234.0  2612.0   \n",
       "                     177 84              2   496.0   930.0  1118.0  2542.0   \n",
       "                     182 89              2   568.0  1219.0  1350.0  3028.0   \n",
       "                     188 90              2   604.0   931.0   944.0  1985.0   \n",
       "\n",
       "                                  swir1   swir2  \n",
       "study_area scene_num row column                  \n",
       "mtbarker   1         38  105     1305.0   558.0  \n",
       "                     33  104     2284.0  1088.0  \n",
       "                     32  105     2284.0  1044.0  \n",
       "                     28  106     2192.0   999.0  \n",
       "                     26  102     2009.0   823.0  \n",
       "                     24  100     1886.0   823.0  \n",
       "                     25  109     1948.0   867.0  \n",
       "                         116     2131.0   999.0  \n",
       "                     29  148     2040.0   867.0  \n",
       "                         150     1917.0   823.0  \n",
       "                     31  152     2009.0   911.0  \n",
       "                     35  151     2040.0   867.0  \n",
       "                     37  150     2070.0   867.0  \n",
       "                     68  136     2040.0   867.0  \n",
       "                     61  144     2315.0  1000.0  \n",
       "                     58  145     2254.0  1000.0  \n",
       "                     55  147        NaN     NaN  \n",
       "                     56  140     2346.0  1044.0  \n",
       "                     59  137     2223.0  1000.0  \n",
       "                     54  165     1704.0   647.0  \n",
       "                     55  167     1704.0   647.0  \n",
       "                     53  168     1734.0   647.0  \n",
       "                     52  167     1673.0   735.0  \n",
       "                     51  165     1642.0   602.0  \n",
       "                     49  165     1704.0   735.0  \n",
       "                     47  166     1765.0   779.0  \n",
       "                     46  169     1856.0   867.0  \n",
       "                     50  169     1734.0   735.0  \n",
       "                     60  166     1551.0   647.0  \n",
       "                     59  165     1489.0   558.0  \n",
       "...                                 ...     ...  \n",
       "swsyd      830       117 10      1028.0   343.0  \n",
       "                     124 7       1028.0   637.0  \n",
       "                     105 56      1376.0   563.0  \n",
       "                     97  56      1128.0   416.0  \n",
       "                     210 18      2024.0  1226.0  \n",
       "                     204 21      2322.0  1300.0  \n",
       "                     192 27      2322.0  1226.0  \n",
       "                     191 25      2421.0  1226.0  \n",
       "                     238 21      3416.0  1963.0  \n",
       "                     239 33      2820.0  1300.0  \n",
       "                     233 41      1378.0   711.0  \n",
       "                     201 262     1924.0   784.0  \n",
       "                     194 258     2122.0   784.0  \n",
       "                     189 253     2271.0   711.0  \n",
       "                     175 250     2023.0   784.0  \n",
       "                     234 177        NaN     NaN  \n",
       "                     235 180        NaN     NaN  \n",
       "                     230 129     2819.0  2699.0  \n",
       "                     225 123     3961.0  3508.0  \n",
       "                     214 125     5450.0  5274.0  \n",
       "                     213 130     2968.0  3066.0  \n",
       "                     215 55      2024.0  2110.0  \n",
       "                     213 53      2620.0  2110.0  \n",
       "                     192 76      2520.0  1594.0  \n",
       "                     188 71      2023.0  1447.0  \n",
       "                     178 77      2122.0  1594.0  \n",
       "                     175 79      2073.0  1520.0  \n",
       "                     177 84      2122.0  1594.0  \n",
       "                     182 89      2520.0  1741.0  \n",
       "                     188 90      1924.0  1300.0  \n",
       "\n",
       "[1322 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load previous training data\n",
    "# by taking the last (ie most recent if the standard date is attached to the file) .pkl file\n",
    "files = os.listdir('../')\n",
    "pickles = []\n",
    "for file in files:\n",
    "    if file[-3::] == 'pkl':\n",
    "        pickles.append(file)\n",
    "trainingdata = pd.read_pickle('../' + pickles[-1])\n",
    "# trainingdata = trainingdata.drop(columns = trainingdata.columns[1::])\n",
    "\n",
    "# # setup a multilevel heirachrical index dataframe to store the results\n",
    "# # storing the training data in this format is way more memory efficient than in an Xarray of same size as data\n",
    "# # but it takes a lot of processing and manipulation to get it into a more useable form\n",
    "\n",
    "# trainidx = pd.MultiIndex(levels = [[]]*4, labels = [[]]*4, names=['study_area', 'scene_num', 'row','column'])\n",
    "# traincols = ['landcover']\n",
    "# trainingdata = pd.DataFrame(index = trainidx, columns = traincols)\n",
    "\n",
    "# view the current status\n",
    "trainingdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other broad scope variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier to work with integers than strings, so map the planned training classes to integers\n",
    "landcover = {'vegetation':1,'urban':2,'earth':3,'water':4}\n",
    "# range of pretermined study areas to use as sources for training data\n",
    "study_areas = ['mtbarker', 'swmelb', 'gunghalin', 'goldengrove', 'molonglo', 'nperth', 'swbris', 'swsyd', 'custom']\n",
    "\n",
    "# not in broad scope yet\n",
    "sat_bands = ['blue','green','red','nir','swir1','swir2']\n",
    "dc_bands = sat_bands.copy() + ['cloud_mask']\n",
    "\n",
    "colours = ['r', 'b', 'm', 'c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The Training Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed64eed784214e2e82e52ada538f87a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>Dropdown</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Dropdown(description='Study Area', options=('mtbarker', 'swmelb', 'gunghalin', 'goldengrove', 'molonglo', 'nperth', 'swbris', 'swsyd', 'custom'), value='mtbarker')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a study area drop down list and display it\n",
    "study_area_dd = Dropdown(options=study_areas, value = study_areas[0], description='Study Area', disabled = False)\n",
    "display(study_area_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# work with the value of the dropdown list\n",
    "study_area = study_area_dd.value\n",
    "if study_area == 'custom':\n",
    "    coords = ['lat_min', 'lat_max', 'lon_min', 'lon_max']\n",
    "    spatial_query = collections.OrderedDict()\n",
    "    for coord in coords:\n",
    "        spatial_query[coord] = input(coord + ': ')\n",
    "    data = getData(list(spatial_query.values()))\n",
    "else:\n",
    "    data = getData(study_area)\n",
    "\n",
    "print('\\nStudy area data loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    interact(train,\n",
    "             study_area = fixed(study_area),\n",
    "             scene_num = IntSlider(value = 1, min = 0, max = data.shape[2]-1,description = \"Scene Number\"),\n",
    "             covertype = Dropdown(options=list(landcover.keys()), value=list(landcover.keys())[0], description='Landcover', disabled = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the outputs of the training data generation process\n",
    "trainingdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Results Manipulation and Classifier Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Building the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Aim is to get the data from the dataframe (which holds references to the pixel's location, along with the\n",
    "# assigned class for that pixel), use it to extract the spectral data for that pixel, format it appropriately\n",
    "# and pass it to the classification algorithm to teach it.\n",
    "\n",
    "# useful variables for pulling out data from Xarray\n",
    "sat_bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']\n",
    "dc_bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'cloud_mask']\n",
    "\n",
    "# make the required columns\n",
    "sat_bands_loc = []\n",
    "for band in sat_bands:\n",
    "    if band in trainingdata.columns:\n",
    "        continue\n",
    "    trainingdata[band] = np.nan\n",
    "    sat_bands_loc.append(trainingdata.columns.get_loc(band))\n",
    "    \n",
    "# loop through the different locations used for the training data.\n",
    "for loc in trainingdata.index.levels[0]:\n",
    "    \n",
    "    # build the Xarray for that location\n",
    "    data = getData(loc)\n",
    "    # only look at the training data for that location\n",
    "    subset = trainingdata.loc[loc]\n",
    "    # for each row (ie each pick) at that location\n",
    "    for i in range(len(subset)):\n",
    "        # unpack the multilevel pandas index into components for accessing the correct Xarray pixel\n",
    "        scene, y, x = subset.iloc[i].name\n",
    "        vals = data[x, y, scene].sel(band=dc_bands)\n",
    "        if vals.sel(band='cloud_mask').values == 0:\n",
    "            # if the pixel is valid (no cloud), take the spectral bands\n",
    "            if np.isfinite(vals.sel(band=sat_bands).values).all():\n",
    "                # if all the bands have readings (no NaNs), save the relevant bits into X and Y\n",
    "                trainingdata.loc[(loc, scene, y, x), sat_bands] = vals.sel(band=sat_bands).values\n",
    "\n",
    "# save the latest version of trainingdata somewhere good\n",
    "time = str(datetime.datetime.now()).split('.')[0].replace(' ','_')\n",
    "trainingdata.to_pickle('../traningdata_' + time + '.pkl')                \n",
    "                \n",
    "trainingdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: makeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def makeClassifier(model):\n",
    "    if model == 'svc':\n",
    "        clf = svm.SVC()\n",
    "    if model == 'rfc':\n",
    "        clf = RandomForestClassifier()\n",
    "    # scale and normalize the data\n",
    "    X_fortraining = preprocessing.scale(trainingdata.dropna(axis=0, how = 'any')[sat_bands].values)\n",
    "    X_fortraining = preprocessing.normalize(X_fortraining)\n",
    "\n",
    "    # create a support vector classifer, and fit the data to it\n",
    "    # might be worth trying a RandomForest Classifier\n",
    "    clf.fit(X_fortraining,trainingdata.dropna(axis=0, how = 'any')['landcover'].values)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting Remaining Data for Classification & Classifying It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformatAndClassify(study_area, clf):\n",
    "    # get the data\n",
    "    data = getData(study_area)\n",
    "    \n",
    "    #setting up the xarray to store the results for easy plotting later\n",
    "    for newband in ['landcover','predicted_landcover']:\n",
    "        temp = data[:,:,:].sel(band='red').copy()\n",
    "        temp.band.values = newband\n",
    "        temp.values[:] = np.nan\n",
    "        data = xr.concat([data, temp], dim='band')\n",
    "\n",
    "    # useful variable for down the track\n",
    "    shape = data.values.shape\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    print('Classifying ' + study_area + ' at ' + str(start))\n",
    "    \n",
    "    # classify one scene at a time, save the results to the xarray\n",
    "    # rewrite this to work pixel by pixel on timeseries, will tie in better to change detection\n",
    "    for scene in range(shape[2]):\n",
    "\n",
    "        # setting up dataframe multilevel indexes\n",
    "        col_idx = list(range(shape[0])) * shape[1]\n",
    "        row_idx = []\n",
    "        for i in range(shape[1]):\n",
    "            row_idx += [i] * shape[0]\n",
    "        scene_idx = [scene] * (shape[0] * shape[1])\n",
    "\n",
    "        # reshape the data into a 2D flat array for scikit learn\n",
    "        flattened = data[:,:,scene].sel(band=dc_bands).values.reshape(shape[0] * shape[1], len(dc_bands))\n",
    "\n",
    "        # add the data to a new DataFrame, set up the columns and index\n",
    "        alldata = pd.DataFrame(flattened)\n",
    "        alldata.columns = dc_bands\n",
    "        alldata['row'] = row_idx\n",
    "        alldata['column'] = col_idx\n",
    "        alldata['scene_num'] = scene_idx\n",
    "        alldata['study_area'] = study_area\n",
    "        alldata = alldata.set_index(['study_area','scene_num','row','column'])\n",
    "\n",
    "        # join in the training data. This is a SQL left join, so only adds data to current study area\n",
    "        alldata = alldata.reset_index().join(trainingdata[['landcover']], on=trainingdata.index.names).set_index(alldata.index.names)\n",
    "\n",
    "        # reduce alldata down to valid pixels (ie cloudmask), and non-training pixels (ie landcover is still NaN)\n",
    "        datatoclassify = alldata[alldata['cloud_mask'] == 0 & np.isnan(alldata['landcover'])].copy()\n",
    "        # remove pixels with a np.nan as scikit-learn doesn't like them. Only keep spectral bands\n",
    "        datatoclassify = datatoclassify[np.isnan(datatoclassify['landcover'])][sat_bands]\n",
    "        # cast these relevant columns into a numpy array\n",
    "        datatoclassify_np = np.array(datatoclassify)\n",
    "\n",
    "        # to deal with an entirely clouded scene\n",
    "        if len(datatoclassify_np) == 0:\n",
    "            continue\n",
    "\n",
    "        # scale and normalize the data so it resembles the training data.\n",
    "        datatoclassify_np = preprocessing.scale(datatoclassify_np)\n",
    "        datatoclassify_np = preprocessing.normalize(datatoclassify_np)\n",
    "\n",
    "        # results of predict() are a 1 dimensional numpy array of the same length as the input data\n",
    "        # assign these results to a new column in the dataframe\n",
    "        datatoclassify['predicted_landcover'] = clf.predict(datatoclassify_np)\n",
    "\n",
    "        # SQL left join the results back onto the original data\n",
    "        alldata = alldata.reset_index().join(datatoclassify[['predicted_landcover']], on=trainingdata.index.names).set_index(alldata.index.names)\n",
    "\n",
    "        #save the training data and classification results into the Xarray\n",
    "        data[:,:,scene].loc[dict(band='landcover')] = alldata['landcover'].values.reshape(shape[0],shape[1])\n",
    "        data[:,:,scene].loc[dict(band='predicted_landcover')] = alldata['predicted_landcover'].values.reshape(shape[0],shape[1])\n",
    "\n",
    "    \n",
    "    print('Time taken to classify ' + study_area + ' was ' + str(datetime.datetime.now() - start))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: drawClassifiedScene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Viewing the Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def drawClassifiedScene(data, scene_num, alpha):\n",
    "    \n",
    "    # colour map included incase of need to display false colour or other in the future\n",
    "    # could change this to an ordereddict and remove the RGB list created below...?\n",
    "    colourmap = {'R':'red', 'G':'green', 'B':'blue'}\n",
    "    \n",
    "    # combine the data for the 3 bands to be displayed into a single numpy array\n",
    "    h = data.shape[1]\n",
    "    w = data.shape[0]\n",
    "    t = data.shape[2]\n",
    "    \n",
    "    if scene_num > (t -1):\n",
    "        scene_num = t - 1\n",
    "    RGB = ['R','G','B']\n",
    "    date = str(data[:,:,scene_num].date.values)\n",
    "    \n",
    "    # create array to store the RGB info in, and fill by looping through the colourmap variable\n",
    "    # note the .T at the end, because the data array is setup as a (x,y,t), but imshow works (y,x)\n",
    "    rawimg = np.zeros((h, w, 3), dtype=np.float32)\n",
    "    for i in range(len(RGB)):     \n",
    "        rawimg[:,:,i] = data[:,:,scene_num].sel(band=colourmap[RGB[i]]).T\n",
    "        \n",
    "    # equalizing for all bands together\n",
    "    # goal is to make is human interpretable\n",
    "    img_toshow = exposure.equalize_hist(rawimg, mask = np.isfinite(rawimg))    \n",
    "\n",
    "    # displaying the results and formatting the axes etc\n",
    "    plt.imshow(img_toshow)\n",
    "    ax = plt.gca()\n",
    "    ax.set_title('True Colour Landsat Scene, taken\\n' + date + ', over ' + study_area)\n",
    "    \n",
    "    # make the colour map for the cover classes\n",
    "    cmap = colors.ListedColormap(colours)\n",
    "    \n",
    "    # draw the classification results and the training data results\n",
    "    ax.imshow(data[:,:,scene_num].sel(band='predicted_landcover').values.T, cmap = cmap, alpha = alpha)\n",
    "    ax.imshow(data[:,:,scene_num].sel(band='landcover').values.T, cmap = cmap, alpha = 1)\n",
    "    \n",
    "    #draw a legend for the classification colours\n",
    "    legend_patches = []\n",
    "    for cover in landcover.keys():\n",
    "        legend_patches.append(mpatches.Patch(color = colours[landcover[cover]-1], label = cover))\n",
    "    ax.legend(handles = legend_patches)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## function: drawClassifiedPlots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def drawClassifiedPlots(data, scene_num, alpha):\n",
    "    \n",
    "    ax1 = plt.subplot2grid([2,4],[0,0], rowspan = 2, colspan = 2)\n",
    "    ax1.clear()\n",
    "    ax1 = drawClassifiedScene(data, scene_num, alpha = 0)\n",
    "    ax2 = plt.subplot2grid([2,4],[0,2], rowspan = 2, colspan = 2)\n",
    "    ax2.clear()\n",
    "    ax2 = drawClassifiedScene(data, scene_num, alpha)\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## function: check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def check(data, scene_num, alpha):\n",
    " \n",
    "    # control the figure size\n",
    "    fig = plt.figure(figsize=[12,7])\n",
    "    axs = fig.axes\n",
    "    plt.subplots_adjust(hspace = 0.6)\n",
    "    \n",
    "    # draw the figure\n",
    "    drawClassifiedPlots(data, scene_num, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Drawing the Reults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    interact(check,\n",
    "             data = fixed(data),\n",
    "             scene_num = IntSlider(value = 1, min = 0, max = 2000,description = \"Scene Number\"),\n",
    "             alpha= FloatSlider(value = 0.6, min = 0, max = 1, description = \"Classification Transparency\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series of Classifications into Change Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: modalFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this function gets called a lot. Needs to be significantly sped up!\n",
    "# dict instead of list.index()?\n",
    "# remove first line\n",
    "# remove length limit at start\n",
    "\n",
    "def modalFilter(df, column, index, span = 10):\n",
    "    df = df[~np.isnan(df[column])]\n",
    "    if index > (len(df) - span) + 1:\n",
    "        return np.nan\n",
    "    else:\n",
    "        mode_arr = df[column][index : index + span].mode().values\n",
    "        if len(mode_arr) > 1:\n",
    "            mode_arr2 = df[column][index : index + span + 1].mode().values\n",
    "            if len(mode_arr2) > 1:\n",
    "                heirarchy = [1, 3, 4, 2] # vegetation, earth, water, urban\n",
    "                max_priority = 4\n",
    "                i_max = 0\n",
    "                for i in mode_arr:\n",
    "                    if heirarchy.index(i) < max_priority:\n",
    "                        max_priority = heirarchy.index(i)\n",
    "                        i_max = i\n",
    "                return i_max\n",
    "            else:\n",
    "                return mode_arr2[0] \n",
    "        else:\n",
    "            return mode_arr[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: dateStringToFloat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateStringToFloat(s):\n",
    "    year = int(s[0:4])\n",
    "    month = int(s[5:7])\n",
    "    \n",
    "    if month in [1,2,3]:\n",
    "        year += 0.125\n",
    "    elif month in [4,5,6]:\n",
    "        year += 0.375\n",
    "    elif month in [7,8,9]:\n",
    "        year += 0.625\n",
    "    else:\n",
    "        year == 0.875\n",
    "    \n",
    "    return year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Change Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def changeDetector(study_area, data, clftype):\n",
    "\n",
    "    # setting up the results raster\n",
    "    shape = data.shape\n",
    "    changedates_arr = np.zeros((shape[1], shape[0]), dtype=np.float32)\n",
    "    changedates_arr[changedates_arr == 0] = np.nan\n",
    "\n",
    "    # variables for the modal filtering\n",
    "    mode_span = 10\n",
    "    MoM_span = 3\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    print('Change detection for ' + study_area + ' start time: ' + str(start))\n",
    "\n",
    "    # a very slow nested loop, keen to remove if possible\n",
    "    for x in range(shape[0]):\n",
    "        for y in range(shape[1]):\n",
    "            \n",
    "            # make a dataframe of the time-series of the predicted classifications\n",
    "            pixeldata = pd.DataFrame(data[x, y, :].sel(band='predicted_landcover').values, index = data.date.values, columns = ['predicted_landcover'])\n",
    "            # drop NaNs\n",
    "            pixeldata = pixeldata.dropna(axis = 0, how='any')\n",
    "\n",
    "            #make a new column to store mode in\n",
    "            pixeldata['mode'] = np.nan\n",
    "            pixeldata['mode_of_modes'] = np.nan\n",
    "            mode_loc = pixeldata.columns.get_loc('mode')\n",
    "\n",
    "            # remove any possibility of duplicate dates (based on an issue with swmelbourne study area)\n",
    "            pixeldata = pixeldata[~pixeldata.index.duplicated(keep = 'first')]\n",
    "\n",
    "            # description of current decision rule for assigning urban change:\n",
    "            #  calculate the mode over each 10 scenes, and assign that value to the first scene of the 10\n",
    "            #     if the first two modal values are both urban, assume already developed and move to next pixel\n",
    "            #     if not, calculate the mode of the modes, over each 3 modes, assigning the value to the first\n",
    "            #  find the first mode of modes that is urban for that pixel\n",
    "            #     use that data (MOM_change date) to find the group of 3 modes that contributed to the mode of modes\n",
    "            #     within the individual classifications for that pixel within that group of 30 classifications\n",
    "            #        if there is an instance of 2 classifications in a row being urban, use the date of the first of those\n",
    "            #        if not, simply use the first instance of an urban pixel in that range of 30\n",
    "\n",
    "            # Ideas for improvement:\n",
    "            #    - Deal with water pixels (eg if >25% of classifications are water, ignore)\n",
    "\n",
    "            # Limitations of Method:\n",
    "            #    - Many of them!\n",
    "            #    - Algorithm needs first 20 to establish baseline - so it can't detect early change\n",
    "            #    - Algorithm needs last 30 to build mode of modes - so it can't detect most recent change\n",
    "            #    - It's very slow - mulitple nested loops. Main speed up would come from being able to do\n",
    "            #       Modal filter as a df.apply(modalFilter)\n",
    "\n",
    "            # find mode of each 10 scenes, and store at first index of range\n",
    "            # for example the mode of scenes 10-19 will be stored in row 10.\n",
    "            for row in range(0, len(pixeldata), mode_span):\n",
    "                pixeldata.iloc[row, mode_loc] = modalFilter(pixeldata, 'predicted_landcover', row, span = mode_span)\n",
    "\n",
    "            # if either of the first two modes are urban, assume pixel is already urban at start of landsat archive    \n",
    "            if (pixeldata[~np.isnan(pixeldata['mode'])].iloc[0:2].values ==  2).any():\n",
    "                continue\n",
    "\n",
    "            # view or slice of data with modes\n",
    "            modes = pixeldata[~np.isnan(pixeldata['mode'])]\n",
    "\n",
    "            # third level of nested of loops :(\n",
    "            # applying modal filter to the modes, to create mode of modes\n",
    "            # save the result in the pixeldata dataframe\n",
    "            for row in range(0,len(modes),MoM_span):\n",
    "                pixeldata.loc[modes.iloc[row].name, 'mode_of_modes'] = modalFilter(modes, 'mode', row, span = MoM_span)\n",
    "\n",
    "            # decision criteria\n",
    "            if len(pixeldata[pixeldata['mode_of_modes'] == 2]) > 0:\n",
    "                MoM_changedate = pixeldata[pixeldata['mode_of_modes'] == 2].iloc[0].name\n",
    "                M_ss = pixeldata.loc[MoM_changedate::]\n",
    "                M_changedate = M_ss[M_ss['mode'] == 2].iloc[0].name\n",
    "                M_changedate_loc = pixeldata.index.get_loc(M_changedate)\n",
    "                pix_ss = pixeldata.iloc[M_changedate_loc - mode_span : M_changedate_loc + mode_span]\n",
    "                twoinarow = pix_ss[(pix_ss['predicted_landcover'] == pix_ss['predicted_landcover'].shift(-1)) & \n",
    "                                    (pix_ss['predicted_landcover'][pix_ss['predicted_landcover'] == 2])]\n",
    "                if len(twoinarow) > 0:\n",
    "                    changedate = twoinarow.iloc[0].name\n",
    "                    changedates_arr[y, x] = dateStringToFloat(changedate)\n",
    "                else:\n",
    "                    changedate = pix_ss[pix_ss['predicted_landcover'] == 2].iloc[0].name\n",
    "                changedates_arr[y, x] = dateStringToFloat(changedate)\n",
    "\n",
    "    # print how long the modal filtering and change detection took                \n",
    "    print(study_area + ' processing time: ' + str(datetime.datetime.now() - start))           \n",
    "\n",
    "    # save the results to a .pkl for future access\n",
    "    results_save_location = '../' + study_area + '/changeresults_' + clftype + '.pkl'\n",
    "    changedates_arr.dump(results_save_location)  \n",
    "    print('Results have been saved to', results_save_location, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping Through All Study Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Study Areas Loop Commenced at: 2018-01-16 16:59:16.606058\n",
      "\n",
      "Loading data from: ../mtbarker/\n",
      "Classifying mtbarker at 2018-01-16 16:59:24.896896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to classify mtbarker was 0:01:59.416662\n",
      "Change detection for mtbarker start time: 2018-01-16 17:01:24.323128\n",
      "mtbarker processing time: 1:11:14.320063\n",
      "Results have been saved to ../mtbarker/changeresults_svm_overnight.pkl \n",
      "\n",
      "Loading data from: ../swmelb/\n",
      "Classifying swmelb at 2018-01-16 18:13:08.077845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to classify swmelb was 0:03:58.867840\n",
      "Change detection for swmelb start time: 2018-01-16 18:17:06.950562\n",
      "swmelb processing time: 2:37:19.101395\n",
      "Results have been saved to ../swmelb/changeresults_svm_overnight.pkl \n",
      "\n",
      "Loading data from: ../gunghalin/\n",
      "Classifying gunghalin at 2018-01-16 20:55:14.489819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to classify gunghalin was 0:03:32.667342\n",
      "Change detection for gunghalin start time: 2018-01-16 20:58:47.171499\n",
      "gunghalin processing time: 1:11:34.168150\n",
      "Results have been saved to ../gunghalin/changeresults_svm_overnight.pkl \n",
      "\n",
      "Loading data from: ../mtbarker/\n",
      "Classifying mtbarker at 2018-01-16 22:10:27.981889\n",
      "Time taken to classify mtbarker was 0:05:51.128539\n",
      "Change detection for mtbarker start time: 2018-01-16 22:16:19.179498\n",
      "mtbarker processing time: 1:10:33.965798\n",
      "Results have been saved to ../mtbarker/changeresults_svm_overnight.pkl \n",
      "\n",
      "Loading data from: ../swmelb/\n",
      "Classifying swmelb at 2018-01-16 23:27:20.011729\n",
      "Time taken to classify swmelb was 0:12:35.162482\n",
      "Change detection for swmelb start time: 2018-01-16 23:39:55.201492\n",
      "swmelb processing time: 2:41:20.920893\n",
      "Results have been saved to ../swmelb/changeresults_svm_overnight.pkl \n",
      "\n",
      "Loading data from: ../gunghalin/\n",
      "Classifying gunghalin at 2018-01-17 02:21:28.580838\n",
      "Time taken to classify gunghalin was 0:08:56.095145\n",
      "Change detection for gunghalin start time: 2018-01-17 02:30:24.786980\n",
      "gunghalin processing time: 1:12:57.044344\n",
      "Results have been saved to ../gunghalin/changeresults_svm_overnight.pkl \n",
      "\n",
      "\n",
      "All study areas loop finished at: 2018-01-16 16:59:16.606058\n"
     ]
    }
   ],
   "source": [
    "bigrunstart = datetime.datetime.now()\n",
    "print('All Study Areas Loop Commenced at: ' + str(bigrunstart) + '\\n')\n",
    "\n",
    "for clftype in ['rfc','svc']:\n",
    "    for study_area in  ['mtbarker','swmelb','gunghalin']:    #study_areas[0:-1]:\n",
    "        clf = makeClassifier(clftype)  #valid options are 'rfc' and 'svc'\n",
    "        classified_data = reformatAndClassify(study_area, clf)\n",
    "        changeDetector(study_area, classified_data, clftype)\n",
    "    \n",
    "print('\\nAll study areas loop finished at: ' + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modal Filter Testing and Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mt barker pixel that definitely changed\n",
    "# pixeldata = pd.DataFrame(data[129, 39, :].sel(band='predicted_landcover').values, index = data.date.values, columns = ['predicted_landcover'])\n",
    "\n",
    "# mt barker pixel that was always urban\n",
    "# pixeldata = pd.DataFrame(data[40, 63, :].sel(band='predicted_landcover').values, index = data.date.values, columns = ['predicted_landcover'])\n",
    "\n",
    "# mt barker pixel that was exception causing\n",
    "pixeldata = pd.DataFrame(data[106, 120, :].sel(band='predicted_landcover').values, index = data.date.values, columns = ['predicted_landcover'])\n",
    "\n",
    "pixeldata = pixeldata.dropna(axis = 0, how='any')\n",
    "pixeldata['mode'] = np.nan\n",
    "pixeldata['mode_of_modes'] = np.nan\n",
    "mode_loc = pixeldata.columns.get_loc('mode')\n",
    "\n",
    "\n",
    "for row in range(0, len(pixeldata), mode_span):\n",
    "    pixeldata.iloc[row, mode_loc] = modalFilter(pixeldata, 'predicted_landcover', row, span = mode_span)\n",
    "\n",
    "# # if either of the first two modes are urban, assume pixel is already urban at start of landsat archive    \n",
    "if (pixeldata[~np.isnan(pixeldata['mode'])].iloc[0:2].values ==  2).any():\n",
    "    print('Already Urban')\n",
    "\n",
    "modes = pixeldata[~np.isnan(pixeldata['mode'])]\n",
    "\n",
    "for row in range(0,len(modes),MoM_span):\n",
    "    pixeldata.loc[modes.iloc[row].name, 'mode_of_modes'] = modalFilter(modes, 'mode', row, span = MoM_span)\n",
    "    \n",
    "if len(pixeldata[pixeldata['mode_of_modes'] == 2]) > 0:\n",
    "    MoM_changedate = pixeldata[pixeldata['mode_of_modes'] == 2].iloc[0].name\n",
    "    M_ss = pixeldata.loc[MoM_changedate::]\n",
    "    M_changedate = M_ss[M_ss['mode'] == 2].iloc[0].name\n",
    "    M_changedate_loc = pixeldata.index.get_loc(M_changedate)\n",
    "    \n",
    "    pix_ss = pixeldata.iloc[M_changedate_loc - mode_span : M_changedate_loc + mode_span]\n",
    "# pixeldata[np.isfinite(pixeldata['mode_of_modes'])]\n",
    "    twoinarow = pix_ss[(pix_ss['predicted_landcover'] == pix_ss['predicted_landcover'].shift(-1)) & \n",
    "                        (pix_ss['predicted_landcover'][pix_ss['predicted_landcover'] == 2])]\n",
    "\n",
    "    if len(twoinarow) > 0:\n",
    "        changedate = twoinarow.iloc[0].name\n",
    "    else:\n",
    "        changedate = pix_ss[pix_ss['predicted_landcover'] == 2].iloc[0].name\n",
    "changedate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing the Change Detection Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## drawAnalysedScene()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def drawAnalysedScene(data, scene_num, alpha, change_grid):\n",
    "    \n",
    "    # colour map included incase of need to display false colour or other in the future\n",
    "    # could change this to an ordereddict and remove the RGB list created below...?\n",
    "    colourmap = {'R':'red', 'G':'green', 'B':'blue'}\n",
    "    \n",
    "    # combine the data for the 3 bands to be displayed into a single numpy array\n",
    "    h = data.shape[1]\n",
    "    w = data.shape[0]\n",
    "    t = data.shape[2]\n",
    "    \n",
    "    if scene_num > (t -1):\n",
    "        scene_num = t - 1\n",
    "    RGB = ['R','G','B']\n",
    "    date = str(data[:,:,scene_num].date.values)\n",
    "    \n",
    "    # create array to store the RGB info in, and fill by looping through the colourmap variable\n",
    "    # note the .T at the end, because the data array is setup as a (x,y,t), but imshow works (y,x)\n",
    "    rawimg = np.zeros((h, w, 3), dtype=np.float32)\n",
    "    for i in range(len(RGB)):     \n",
    "        rawimg[:,:,i] = data[:,:,scene_num].sel(band=colourmap[RGB[i]]).T\n",
    "        \n",
    "    # equalizing for all bands together\n",
    "    # goal is to make is human interpretable\n",
    "    img_toshow = exposure.equalize_hist(rawimg, mask = np.isfinite(rawimg))    \n",
    "\n",
    "    # displaying the results and formatting the axes etc\n",
    "    plt.imshow(img_toshow)\n",
    "    ax = plt.gca()\n",
    "    ax.set_title('True Colour Landsat Scene, taken\\n' + date + ', over ' + study_area)\n",
    "    \n",
    "    # define the current colour map to display the change results raster properly\n",
    "    current_cmap = matplotlib.cm.get_cmap('Reds_r')\n",
    "    current_cmap.set_under('k', alpha=0.0)\n",
    "    current_cmap.set_over('r', alpha=1.0)\n",
    "    current_cmap.set_bad('k', alpha=0.0)  \n",
    "    \n",
    "    # draw the change detection results mask\n",
    "    ax.imshow(change_grid, alpha = alpha, interpolation='none', cmap = current_cmap, clim = [0.5, 0.6])\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## drawAnalysedPlots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def drawAnalysedPlots(data, left_scene_num, left_alpha, left_change_grid,\n",
    "                      right_scene_num, right_alpha, right_change_grid):\n",
    "    \n",
    "    ax1 = plt.subplot2grid([2,4],[0,0], rowspan = 2, colspan = 2)\n",
    "    ax1.clear()\n",
    "    ax1 = drawAnalysedScene(data, left_scene_num, left_alpha, left_change_grid)\n",
    "    ax2 = plt.subplot2grid([2,4],[0,2], rowspan = 2, colspan = 2)\n",
    "    ax2.clear()\n",
    "    ax2 = drawAnalysedScene(data, right_scene_num, right_alpha, right_change_grid)\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## function: results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def results(data, left_scene_num, left_alpha, left_change_grid,\n",
    "                      right_scene_num, right_alpha, right_change_grid):\n",
    " \n",
    "    # control the figure size\n",
    "    fig = plt.figure(figsize=[12,7])\n",
    "#     axs = fig.axes\n",
    "#     plt.subplots_adjust(hspace = 0.6)\n",
    "    \n",
    "    # draw the figure\n",
    "    drawAnalysedPlots(data, left_scene_num, left_alpha, left_change_grid,\n",
    "                      right_scene_num, right_alpha, right_change_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Existing Change Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the results ready for comparison\n",
    "# no change should be np.NaN\n",
    "# chanage = 1\n",
    "change = gdal.Open('../' + study_area + '/change_time.img').ReadAsArray()\n",
    "change[change == 0] = np.nan\n",
    "\n",
    "peterchange = change.copy()\n",
    "peterchange[np.isfinite(peterchange)] = 1\n",
    "\n",
    "# mikechange = changedates_arr.copy()\n",
    "# mikechange[np.isfinite(mikechange)] = 1\n",
    "results_save_location = '../' + study_area + '/changeresults_svm2.pkl'\n",
    "mike_results = np.load(results_save_location)\n",
    "mikechange = mike_results.copy()\n",
    "mikechange[np.isfinite(mikechange)] = 1\n",
    "\n",
    "num_scenes = data.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw Results Analysis Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    interact(results,\n",
    "             data = fixed(data),\n",
    "             left_scene_num = IntSlider(value = 1, min = 0, max = num_scenes -1 ,description = \"Scene Number\"),\n",
    "             left_alpha= FloatSlider(value = 0.6, min = 0, max = 1, description = \"Left Alpha\"),\n",
    "             left_change_grid = fixed(peterchange),\n",
    "             right_scene_num = IntSlider(value = num_scenes - 1, min = 0, max = num_scenes - 1,description = \"Scene Number\"),\n",
    "             right_alpha= FloatSlider(value = 0.6, min = 0, max = 1, description = \"Right Alpha\"),\n",
    "             right_change_grid = fixed(mikechange))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "print(np.nanmean(mike_results - change))\n",
    "imshow(mike_results - change)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "898px",
    "left": "0px",
    "right": "1496px",
    "top": "111px",
    "width": "229px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "widgets": {
   "state": {
    "c52feb267d464c0681dc5b8825029c6a": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
