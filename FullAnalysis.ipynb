{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 13,
        "hidden": false,
        "row": 0,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Supervised Classification Urban Change Detection\n",
    "The markdown cells have been designed to work with the 'Table Of Contents(2)' Jupyter notebook extension.\n",
    "This is highly recommended, if you don't have it yet (and are working on the VDI on the 'agdc-py3-prod module'\n",
    "select \"Edit\" on the menu bar above, click the \"nbextension config\" button at the bottom of the menu, and enable\n",
    "the extension. The 'Collapsible Headings' extension is also highly recommended.\n",
    "\n",
    "This notebook identifies urban change within the DEA Landsat archive.\n",
    "It loads the data for the area of interest\n",
    "Loads the landcover training data set that has been generated from TrainingDataPicker.ipynb\n",
    "Trains a classifier of the users choice (either Support Vector Classifer or Random Forest Classifier) with the training data\n",
    "Classifies each pixel from every scene in the study area as being one of the trained landcover classes\n",
    "Runs a custom modal filtering process over the times series of classifications, decides if and when the pixel has changed, and returns that as a number as part of a raster over the study area.\n",
    "\n",
    "This was written Mike Barnes as part of his third graduate rotation, during January 2018.\n",
    "Any questions, please contact me at michael.barnes@ga.gov.au"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 13,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## Python Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import datacube\n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.storage import masking\n",
    "from datacube.storage.masking import mask_to_dict\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import gdal\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider, FloatSlider, Dropdown\n",
    "from IPython.display import display\n",
    "\n",
    "from skimage import exposure\n",
    "from scipy.signal import lfilter\n",
    "\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 5,
        "hidden": false,
        "row": 17,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## Functions for Loading Data and Building the Xarray\n",
    "This project built on some existing work by Peter Tan. An output from Peter's urban change detection algorithm is raster files with all the relevant NBAR (analysis ready satellite derived surface reflectance readings) data saved to the output directory. To speed the loading and analysis during this script, this notebook will use those exisitng files if they are available. Otherwise it will load the data from the Digital Earth Australia archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 13,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### function: checkForLocalFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def checkForLocalFiles(study_area):\n",
    "    \"\"\"A quick boolean function to test if the requested named study area has a local directory\"\"\"\n",
    "    rootdir = os.listdir('../')\n",
    "    if study_area in rootdir:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 13,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### function: getData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def getData(study_area):\n",
    "    \"\"\"This is the main function to retrieve the NBAR Landsat data and return it as an Xarray.\"\"\"\n",
    "    # if the study area is a string, and is accessible locally, load it\n",
    "    if isinstance(study_area, str):\n",
    "        if checkForLocalFiles(study_area):\n",
    "            data = getLocalData(study_area) \n",
    "    # if the study area is a string and is on the list, load it\n",
    "        else:\n",
    "            data = DCLoadName(study_area)\n",
    "            data = transformXarrayToCustomStyle(data)\n",
    "        return data\n",
    "\n",
    "    # if the study area is a list of coordinates, use them to load the data\n",
    "    elif isinstance(study_area, list) and len(study_area) == 4:\n",
    "        data = DCLoad(study_area)\n",
    "        data = transformXarrayToCustomStyle(data)\n",
    "        return data        \n",
    "\n",
    "    else:\n",
    "        print('Data Loading Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 22,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### function: DCLoadName\n",
    "This function is a wrapper for the DCLoad function, that allows previously used study areas to be easily restudied\n",
    "by easily loading exactly the same area of interest (AOI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def DCLoadName(study_area):\n",
    "    \"\"\"\n",
    "    Quick way to load the study areas used for the associated project report if the local files are not available.\n",
    "    \n",
    "    This funciton is a wrapper on the DCLoad function.\n",
    "    \"\"\"\n",
    "    if study_area == 'mtbarker':\n",
    "        lat_min = -35.05\n",
    "        lat_max = -35.08\n",
    "        lon_min = 138.85\n",
    "        lon_max = 138.895  \n",
    "    elif study_area == 'swmelb':\n",
    "        lat_min = -37.879\n",
    "        lat_max = -37.91\n",
    "        lon_min = 144.705\n",
    "        lon_max = 144.76  \n",
    "    elif study_area == 'gunghalin':\n",
    "        lat_min = -35.18\n",
    "        lat_max = -35.21\n",
    "        lon_min = 149.14\n",
    "        lon_max = 149.17\n",
    "    elif study_area == 'goldengrove': \n",
    "        lat_min = -34.77\n",
    "        lat_max = -34.8\n",
    "        lon_min = 138.66\n",
    "        lon_max = 138.73\n",
    "    elif study_area == 'molonglo':\n",
    "        lat_min = -35.3\n",
    "        lat_max = -35.33\n",
    "        lon_min = 149.015\n",
    "        lon_max = 149.06\n",
    "    elif study_area == 'nperth':\n",
    "        lat_min = -31.686\n",
    "        lat_max = -31.73\n",
    "        lon_min = 115.79\n",
    "        lon_max = 115.813\n",
    "    elif study_area == 'swbris':\n",
    "        lat_min = -27.66\n",
    "        lat_max = -27.7 \n",
    "        lon_min = 152.877\n",
    "        lon_max = 152.93\n",
    "    elif study_area == 'swsyd':\n",
    "        lat_min = -33.993\n",
    "        lat_max = -34.04\n",
    "        lon_min = 150.715 \n",
    "        lon_max = 150.78\n",
    "    elif study_area == 'goolwa':\n",
    "        lat_min = -35.49\n",
    "        lat_max = -35.522\n",
    "        lon_min = 138.761\n",
    "        lon_max = 138.83\n",
    "    elif study_area == 'gladstone':\n",
    "        lat_min = -23.868\n",
    "        lat_max = -23.903\n",
    "        lon_min = 152.22\n",
    "        lon_max = 152.265\n",
    "    elif study_area == 'goldcoast':\n",
    "        lat_min = -28.08\n",
    "        lat_max = -28.125\n",
    "        lon_min = 153.360\n",
    "        lon_max = 153.4\n",
    "    elif study_area == 'newcastle':\n",
    "        lat_min = -32.895\n",
    "        lat_max = -32.918\n",
    "        lon_min = 151.59\n",
    "        lon_max = 151.62\n",
    "    \n",
    "    return DCLoad([lat_min, lat_max, lon_min, lon_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 26,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### function: DCLoad\n",
    "This function is a variation of a datacube query supplied by Erin Telfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def DCLoad(study_area):\n",
    "    \"\"\"This function is a variation of a datacube query supplied by Erin Telfer.\n",
    "    This function takes the 4 coordinates as an input, queries the AGDC, and concatenates the NBAR data from\n",
    "    all three Landsat sensors into a single output file.\"\"\"\n",
    "    \n",
    "    # to time how long the load takes\n",
    "    start = datetime.datetime.now()\n",
    "    print('Loading data') \n",
    "    print('Load Started At: ' + str(start))\n",
    "    \n",
    "    # define temporal range \n",
    "    start_of_epoch = '1987-01-01'\n",
    "    end_of_epoch =  '2017-10-31'\n",
    "\n",
    "    # define bands of interest\n",
    "    bands_of_interest = ['blue', 'green', 'red', \n",
    "                         'nir', 'swir1', 'swir2']\n",
    "\n",
    "    # Landsat sensors of interest are defined\n",
    "    sensors = ['ls8', 'ls7', 'ls5'] \n",
    "\n",
    "    # unpack input parameter\n",
    "    lat_min, lat_max, lon_min, lon_max = study_area    \n",
    "\n",
    "    print('Bounding box: ' + str(lat_min) + ' S, ' + str(lon_min) +\n",
    "          ' E to ' + str(lat_max) + ' S, ' + str(lon_max) + ' E' )\n",
    "    print('Epoch: ' + start_of_epoch + ' to ' + end_of_epoch)\n",
    "    print('Sensors: ' + str(sensors))\n",
    "    print('Bands of Interest: ' + str(bands_of_interest))\n",
    "\n",
    "    # create query\n",
    "    query = {'time': (start_of_epoch, end_of_epoch),}\n",
    "    query['x'] = (lon_min, lon_max)\n",
    "    query['y'] = (lat_max, lat_min)\n",
    "    query['crs'] = 'EPSG:4326'\n",
    "\n",
    "    #Create cloud mask. This will define which pixel quality (PQ) artefacts are removed from the results.\n",
    "    # It should be noted the \"land_sea\" code will remove all ocean/sea pixels.\n",
    "    mask_components = {'cloud_acca':'no_cloud',\n",
    "    'cloud_shadow_acca' :'no_cloud_shadow',\n",
    "    'cloud_shadow_fmask' : 'no_cloud_shadow',\n",
    "    'cloud_fmask' :'no_cloud',\n",
    "    'blue_saturated' : False,\n",
    "    'green_saturated' : False,\n",
    "    'red_saturated' : False,\n",
    "    'nir_saturated' : False,\n",
    "    'swir1_saturated' : False,\n",
    "    'swir2_saturated' : False,\n",
    "    'contiguous':True,\n",
    "    'land_sea': 'land'}\n",
    "\n",
    "    # Connect to DataCube\n",
    "    dc = datacube.Datacube(app='Urban Change Detection')\n",
    "    \n",
    "    # Data for each Landsat sensor is retrieved and saved in a dict for concatenation\n",
    "    sensor_clean = {}\n",
    "    \n",
    "    for sensor in sensors:\n",
    "        # Load the NBAR and corresponding PQ\n",
    "        sensor_nbar = dc.load(product= sensor+'_nbar_albers', group_by='solar_day', \n",
    "                              measurements = bands_of_interest,  **query)\n",
    "        sensor_pq = dc.load(product= sensor+'_pq_albers', group_by='solar_day', \n",
    "                            fuse_func=ga_pq_fuser, **query)\n",
    "\n",
    "        # Retrieve the projection information before masking/sorting\n",
    "        crs = sensor_nbar.crs\n",
    "        crswkt = sensor_nbar.crs.wkt\n",
    "        affine = sensor_nbar.affine        \n",
    "\n",
    "        # Combing the pq so it is a single \n",
    "        sensor_all = xr.auto_combine([sensor_pq,sensor_nbar])\n",
    "        sensor_clean[sensor] = sensor_all\n",
    "\n",
    "        print('Loaded %s' % sensor) \n",
    "\n",
    "    print('Concatenating')\n",
    "    nbar_clean = xr.concat(sensor_clean.values(), 'time')\n",
    "    nbar_clean = nbar_clean.sortby('time')\n",
    "    nbar_clean.attrs['crs'] = crs\n",
    "    nbar_clean.attrs['affin|e'] = affine    \n",
    "\n",
    "    print ('Load and Xarray build complete')\n",
    "    print('Process took ' + str(datetime.datetime.now() - start))\n",
    "    \n",
    "    # return xarray\n",
    "    return nbar_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 26,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### function: getLocalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def getLocalData(study_area):\n",
    "    \"\"\"A quick helper function to load the output files from Peter's code for the given location.\n",
    "    It returns and Xarray of the landsat data for that study area.\"\"\"\n",
    "    # build a list of all files in the directory (ie the folder for that location)\n",
    "    location = '../' + study_area + '/'\n",
    "    files = os.listdir(location)\n",
    "\n",
    "    print('Loading data from: ' + location)\n",
    "    \n",
    "    # build a list of all the NBAR*.img file names and which bands they represent\n",
    "    NBARfiles = []\n",
    "    bands = []\n",
    "    for file in files:\n",
    "        if file[-4::] == '.img' and file[0:4] == 'NBAR':\n",
    "            NBARfiles.append(file)\n",
    "            bands.append(file.split('NBAR_')[1].split('.img')[0])\n",
    "\n",
    "    # just catching the random case of when an appropriately named directory exists, but there are no\n",
    "    # relevant NBAR .img files\n",
    "    if len(NBARfiles) == 0:\n",
    "        return DCLoadName(study_area)\n",
    "\n",
    "    # open all the .img files with NBAR in the name, convert to numpy array, swap axes so order is (x, y, t)\n",
    "    # and save to dict\n",
    "    raw_data = {}\n",
    "    for i in range(len(NBARfiles)):\n",
    "        raw_data[bands[i]] = gdal.Open(location + NBARfiles[i]).ReadAsArray().swapaxes(0,2)\n",
    "#     num_scenes = len(raw_data['red'][0][0])   # delete this?\n",
    "\n",
    "    # build a list of all the dates represented by each band in the NBAR files\n",
    "    # reuse the list of NBAR file names, but this time access the .hdr file\n",
    "    in_dates = False\n",
    "    dates = []\n",
    "    for line in open(location + NBARfiles[0].split('.img')[0] + '.hdr'):\n",
    "        if line[0] == '}':\n",
    "            continue\n",
    "        if in_dates:\n",
    "            dates.append(line.split(',')[0].strip())\n",
    "        if line[0:10] == 'band names':\n",
    "            in_dates = True\n",
    "\n",
    "    # save list of satellite originated bands\n",
    "    sat_bands = bands.copy()\n",
    "\n",
    "    # add the yet to be calculated derivative bands to the overall bands list\n",
    "    bands += ['cloud_mask']\n",
    "\n",
    "    # building the Xarray\n",
    "    # define the size for the numpy array that will hold all the data for conversion into XArray\n",
    "    x = len(raw_data['red'])\n",
    "    y = len(raw_data['red'][0])\n",
    "    t = len(raw_data['red'][0][0])\n",
    "    n = len(bands)\n",
    "\n",
    "    # create an empty numpy array of the correct size\n",
    "    alldata = np.zeros((x, y, t, n), dtype=np.float32)\n",
    "\n",
    "    # populate the numpy array with the satellite data\n",
    "    # turn all no data NBAR values to NaNs\n",
    "    for i in range(len(sat_bands)):\n",
    "        alldata[:,:,:,i] = raw_data[sat_bands[i]]\n",
    "        alldata[:,:,:,i][alldata[:,:,:,i] == -999] = np.nan\n",
    "\n",
    "    # convert the numpy array into an xarray, with appropriate lables, and axes names\n",
    "    data = xr.DataArray(alldata, coords = {'x':range(x), 'y':range(y), 'date':dates, 'band':bands},\n",
    "                 dims=['x', 'y', 'date', 'band'])\n",
    "    \n",
    "    # import cloudmask and add to xarray\n",
    "    cloudmask = gdal.Open(location + '/tsmask.img').ReadAsArray().swapaxes(0,2)\n",
    "    data.loc[:,:,:,'cloud_mask'] = cloudmask\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 26,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### function: transformXarrayToCustomStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def transformXarrayToCustomStyle(data_new):\n",
    "    \"\"\"Function to convert the standard format Xarray from DCLoad into the proprietary format Xarray that this\n",
    "    Notebook was written to work with\"\"\"\n",
    "    \n",
    "    # downscale the dataset to a dataarray, and transpose so the variable numbers are right\n",
    "    datafixed = data_new.to_array().transpose('x','y','time','variable')\n",
    "    \n",
    "    # rename the variables into 'band'\n",
    "    datafixed = datafixed.rename({'variable':'band','time':'date'})\n",
    "    \n",
    "    # pull out the current list of bands, find the index number of \"pixelquality\"\n",
    "    # replace with 'cloud_mask', and reassign\n",
    "    new_bands = list(datafixed.band.values)\n",
    "    cm = new_bands.index('pixelquality')\n",
    "    new_bands[cm] = 'cloud_mask'\n",
    "    datafixed.band.values = new_bands\n",
    "    \n",
    "    # changing the full datetime stamp to a simple date only stamp\n",
    "    datafixed['date'] = pd.to_datetime(pd.DataFrame(datafixed.date.to_pandas()).index.date)\n",
    "    \n",
    "    # change pixel quality values to mask, 0 = good, 3 = bad\n",
    "    # see https://www.sciencedirect.com/science/article/pii/S0034425717301086 for PQ value description\n",
    "    cm_vals = datafixed[:,:,:].sel(band='cloud_mask').values\n",
    "    cm_vals[cm_vals == 0] = 1\n",
    "    cm_vals[cm_vals == 16383] = 0\n",
    "    cm_vals[cm_vals != 0] = 3\n",
    "    datafixed[:,:,:].sel(band='cloud_mask').values = cm_vals\n",
    "    \n",
    "    return datafixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: customStyleXarrayToStandard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customStyleXarrayToStandard(data):\n",
    "    return data.to_dataset(dim='band')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 30,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Setting up broad scope variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 30,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## Load Previous Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 12,
        "hidden": false,
        "row": 34,
        "width": 7
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>landcover</th>\n",
       "      <th>blue</th>\n",
       "      <th>green</th>\n",
       "      <th>red</th>\n",
       "      <th>nir</th>\n",
       "      <th>swir1</th>\n",
       "      <th>swir2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>study_area</th>\n",
       "      <th>scene_num</th>\n",
       "      <th>row</th>\n",
       "      <th>column</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">mtbarker</th>\n",
       "      <th rowspan=\"30\" valign=\"top\">1</th>\n",
       "      <th>38</th>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "      <td>247.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>4094.0</td>\n",
       "      <td>1305.0</td>\n",
       "      <td>558.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <th>104</th>\n",
       "      <td>1</td>\n",
       "      <td>420.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>3877.0</td>\n",
       "      <td>2284.0</td>\n",
       "      <td>1088.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "      <td>420.0</td>\n",
       "      <td>677.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>3833.0</td>\n",
       "      <td>2284.0</td>\n",
       "      <td>1044.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <th>106</th>\n",
       "      <td>1</td>\n",
       "      <td>362.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>3877.0</td>\n",
       "      <td>2192.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <th>102</th>\n",
       "      <td>1</td>\n",
       "      <td>324.0</td>\n",
       "      <td>636.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>4483.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "      <td>305.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>4093.0</td>\n",
       "      <td>1886.0</td>\n",
       "      <td>823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">25</th>\n",
       "      <th>109</th>\n",
       "      <td>1</td>\n",
       "      <td>324.0</td>\n",
       "      <td>596.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>4527.0</td>\n",
       "      <td>1948.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>677.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>3746.0</td>\n",
       "      <td>2131.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">29</th>\n",
       "      <th>148</th>\n",
       "      <td>1</td>\n",
       "      <td>439.0</td>\n",
       "      <td>958.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>5734.0</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "      <td>362.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>5390.0</td>\n",
       "      <td>1917.0</td>\n",
       "      <td>823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <th>152</th>\n",
       "      <td>1</td>\n",
       "      <td>458.0</td>\n",
       "      <td>1038.0</td>\n",
       "      <td>847.0</td>\n",
       "      <td>5691.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>911.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <th>151</th>\n",
       "      <td>1</td>\n",
       "      <td>382.0</td>\n",
       "      <td>838.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>5820.0</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "      <td>458.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>5906.0</td>\n",
       "      <td>2070.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <th>136</th>\n",
       "      <td>1</td>\n",
       "      <td>477.0</td>\n",
       "      <td>958.0</td>\n",
       "      <td>811.0</td>\n",
       "      <td>5305.0</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <th>144</th>\n",
       "      <td>1</td>\n",
       "      <td>477.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>811.0</td>\n",
       "      <td>4874.0</td>\n",
       "      <td>2315.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <th>145</th>\n",
       "      <td>1</td>\n",
       "      <td>515.0</td>\n",
       "      <td>1038.0</td>\n",
       "      <td>883.0</td>\n",
       "      <td>5219.0</td>\n",
       "      <td>2254.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <th>147</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <th>140</th>\n",
       "      <td>1</td>\n",
       "      <td>477.0</td>\n",
       "      <td>838.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>4571.0</td>\n",
       "      <td>2346.0</td>\n",
       "      <td>1044.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <th>137</th>\n",
       "      <td>1</td>\n",
       "      <td>477.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>811.0</td>\n",
       "      <td>4917.0</td>\n",
       "      <td>2223.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "      <td>382.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>488.0</td>\n",
       "      <td>5563.0</td>\n",
       "      <td>1704.0</td>\n",
       "      <td>647.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <th>167</th>\n",
       "      <td>1</td>\n",
       "      <td>420.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>5606.0</td>\n",
       "      <td>1704.0</td>\n",
       "      <td>647.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <th>168</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>677.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>5348.0</td>\n",
       "      <td>1734.0</td>\n",
       "      <td>647.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <th>167</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>5262.0</td>\n",
       "      <td>1673.0</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>5348.0</td>\n",
       "      <td>1642.0</td>\n",
       "      <td>602.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "      <td>401.0</td>\n",
       "      <td>798.0</td>\n",
       "      <td>596.0</td>\n",
       "      <td>5348.0</td>\n",
       "      <td>1704.0</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <th>166</th>\n",
       "      <td>1</td>\n",
       "      <td>382.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>5391.0</td>\n",
       "      <td>1765.0</td>\n",
       "      <td>779.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <th>169</th>\n",
       "      <td>1</td>\n",
       "      <td>382.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>5262.0</td>\n",
       "      <td>1856.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <th>169</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>5606.0</td>\n",
       "      <td>1734.0</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <th>166</th>\n",
       "      <td>1</td>\n",
       "      <td>305.0</td>\n",
       "      <td>596.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>5434.0</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>647.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>556.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>5348.0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>558.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">swsyd</th>\n",
       "      <th rowspan=\"30\" valign=\"top\">830</th>\n",
       "      <th>117</th>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>170.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>2402.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>170.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2402.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>637.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>170.0</td>\n",
       "      <td>421.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>2402.0</td>\n",
       "      <td>1376.0</td>\n",
       "      <td>563.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>133.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2262.0</td>\n",
       "      <td>1128.0</td>\n",
       "      <td>416.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>243.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>654.0</td>\n",
       "      <td>3306.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>316.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>2890.0</td>\n",
       "      <td>2322.0</td>\n",
       "      <td>1300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>352.0</td>\n",
       "      <td>858.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>3374.0</td>\n",
       "      <td>2322.0</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>424.0</td>\n",
       "      <td>858.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>3651.0</td>\n",
       "      <td>2421.0</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>496.0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>1061.0</td>\n",
       "      <td>3029.0</td>\n",
       "      <td>3416.0</td>\n",
       "      <td>1963.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>352.0</td>\n",
       "      <td>858.0</td>\n",
       "      <td>829.0</td>\n",
       "      <td>3859.0</td>\n",
       "      <td>2820.0</td>\n",
       "      <td>1300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>243.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>479.0</td>\n",
       "      <td>2891.0</td>\n",
       "      <td>1378.0</td>\n",
       "      <td>711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <th>262</th>\n",
       "      <td>1</td>\n",
       "      <td>281.0</td>\n",
       "      <td>641.0</td>\n",
       "      <td>363.0</td>\n",
       "      <td>4821.0</td>\n",
       "      <td>1924.0</td>\n",
       "      <td>784.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <th>258</th>\n",
       "      <td>1</td>\n",
       "      <td>281.0</td>\n",
       "      <td>714.0</td>\n",
       "      <td>363.0</td>\n",
       "      <td>5369.0</td>\n",
       "      <td>2122.0</td>\n",
       "      <td>784.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <th>253</th>\n",
       "      <td>1</td>\n",
       "      <td>317.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>479.0</td>\n",
       "      <td>5573.0</td>\n",
       "      <td>2271.0</td>\n",
       "      <td>711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <th>250</th>\n",
       "      <td>1</td>\n",
       "      <td>317.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>6254.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>784.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <th>177</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <th>180</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <th>129</th>\n",
       "      <td>2</td>\n",
       "      <td>1245.0</td>\n",
       "      <td>1722.0</td>\n",
       "      <td>1869.0</td>\n",
       "      <td>2334.0</td>\n",
       "      <td>2819.0</td>\n",
       "      <td>2699.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <th>123</th>\n",
       "      <td>2</td>\n",
       "      <td>2386.0</td>\n",
       "      <td>3131.0</td>\n",
       "      <td>3350.0</td>\n",
       "      <td>3514.0</td>\n",
       "      <td>3961.0</td>\n",
       "      <td>3508.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <th>125</th>\n",
       "      <td>2</td>\n",
       "      <td>3489.0</td>\n",
       "      <td>4232.0</td>\n",
       "      <td>4193.0</td>\n",
       "      <td>4134.0</td>\n",
       "      <td>5450.0</td>\n",
       "      <td>5274.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <th>130</th>\n",
       "      <td>2</td>\n",
       "      <td>1174.0</td>\n",
       "      <td>1722.0</td>\n",
       "      <td>1581.0</td>\n",
       "      <td>1707.0</td>\n",
       "      <td>2968.0</td>\n",
       "      <td>3066.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <th>55</th>\n",
       "      <td>2</td>\n",
       "      <td>961.0</td>\n",
       "      <td>1364.0</td>\n",
       "      <td>1465.0</td>\n",
       "      <td>1777.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>2110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <th>53</th>\n",
       "      <td>2</td>\n",
       "      <td>961.0</td>\n",
       "      <td>1435.0</td>\n",
       "      <td>1523.0</td>\n",
       "      <td>3029.0</td>\n",
       "      <td>2620.0</td>\n",
       "      <td>2110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <th>76</th>\n",
       "      <td>2</td>\n",
       "      <td>640.0</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>1407.0</td>\n",
       "      <td>2890.0</td>\n",
       "      <td>2520.0</td>\n",
       "      <td>1594.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <th>71</th>\n",
       "      <td>2</td>\n",
       "      <td>460.0</td>\n",
       "      <td>785.0</td>\n",
       "      <td>944.0</td>\n",
       "      <td>2612.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>1447.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <th>77</th>\n",
       "      <td>2</td>\n",
       "      <td>604.0</td>\n",
       "      <td>930.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>2403.0</td>\n",
       "      <td>2122.0</td>\n",
       "      <td>1594.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <th>79</th>\n",
       "      <td>2</td>\n",
       "      <td>640.0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>2612.0</td>\n",
       "      <td>2073.0</td>\n",
       "      <td>1520.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <th>84</th>\n",
       "      <td>2</td>\n",
       "      <td>496.0</td>\n",
       "      <td>930.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>2542.0</td>\n",
       "      <td>2122.0</td>\n",
       "      <td>1594.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <th>89</th>\n",
       "      <td>2</td>\n",
       "      <td>568.0</td>\n",
       "      <td>1219.0</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>3028.0</td>\n",
       "      <td>2520.0</td>\n",
       "      <td>1741.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <th>90</th>\n",
       "      <td>2</td>\n",
       "      <td>604.0</td>\n",
       "      <td>931.0</td>\n",
       "      <td>944.0</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>1924.0</td>\n",
       "      <td>1300.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1322 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 landcover    blue   green     red     nir  \\\n",
       "study_area scene_num row column                                              \n",
       "mtbarker   1         38  105             1   247.0   434.0   344.0  4094.0   \n",
       "                     33  104             1   420.0   717.0   560.0  3877.0   \n",
       "                     32  105             1   420.0   677.0   524.0  3833.0   \n",
       "                     28  106             1   362.0   717.0   524.0  3877.0   \n",
       "                     26  102             1   324.0   636.0   452.0  4483.0   \n",
       "                     24  100             1   305.0   555.0   380.0  4093.0   \n",
       "                     25  109             1   324.0   596.0   416.0  4527.0   \n",
       "                         116             1   363.0   677.0   524.0  3746.0   \n",
       "                     29  148             1   439.0   958.0   739.0  5734.0   \n",
       "                         150             1   362.0   757.0   560.0  5390.0   \n",
       "                     31  152             1   458.0  1038.0   847.0  5691.0   \n",
       "                     35  151             1   382.0   838.0   632.0  5820.0   \n",
       "                     37  150             1   458.0   918.0   739.0  5906.0   \n",
       "                     68  136             1   477.0   958.0   811.0  5305.0   \n",
       "                     61  144             1   477.0   918.0   811.0  4874.0   \n",
       "                     58  145             1   515.0  1038.0   883.0  5219.0   \n",
       "                     55  147             1     NaN     NaN     NaN     NaN   \n",
       "                     56  140             1   477.0   838.0   739.0  4571.0   \n",
       "                     59  137             1   477.0   998.0   811.0  4917.0   \n",
       "                     54  165             1   382.0   758.0   488.0  5563.0   \n",
       "                     55  167             1   420.0   758.0   524.0  5606.0   \n",
       "                     53  168             1   363.0   677.0   524.0  5348.0   \n",
       "                     52  167             1   363.0   758.0   560.0  5262.0   \n",
       "                     51  165             1   363.0   757.0   524.0  5348.0   \n",
       "                     49  165             1   401.0   798.0   596.0  5348.0   \n",
       "                     47  166             1   382.0   757.0   632.0  5391.0   \n",
       "                     46  169             1   382.0   717.0   632.0  5262.0   \n",
       "                     50  169             1   363.0   758.0   560.0  5606.0   \n",
       "                     60  166             1   305.0   596.0   416.0  5434.0   \n",
       "                     59  165             1   363.0   556.0   416.0  5348.0   \n",
       "...                                    ...     ...     ...     ...     ...   \n",
       "swsyd      830       117 10              1   170.0   347.0   362.0  2402.0   \n",
       "                     124 7               1   170.0   347.0   303.0  2402.0   \n",
       "                     105 56              1   170.0   421.0   245.0  2402.0   \n",
       "                     97  56              1   133.0   347.0   303.0  2262.0   \n",
       "                     210 18              1   243.0   713.0   654.0  3306.0   \n",
       "                     204 21              1   316.0   713.0   712.0  2890.0   \n",
       "                     192 27              1   352.0   858.0   770.0  3374.0   \n",
       "                     191 25              1   424.0   858.0   770.0  3651.0   \n",
       "                     238 21              1   496.0  1003.0  1061.0  3029.0   \n",
       "                     239 33              1   352.0   858.0   829.0  3859.0   \n",
       "                     233 41              1   243.0   494.0   479.0  2891.0   \n",
       "                     201 262             1   281.0   641.0   363.0  4821.0   \n",
       "                     194 258             1   281.0   714.0   363.0  5369.0   \n",
       "                     189 253             1   317.0   713.0   479.0  5573.0   \n",
       "                     175 250             1   317.0   713.0   362.0  6254.0   \n",
       "                     234 177             2     NaN     NaN     NaN     NaN   \n",
       "                     235 180             2     NaN     NaN     NaN     NaN   \n",
       "                     230 129             2  1245.0  1722.0  1869.0  2334.0   \n",
       "                     225 123             2  2386.0  3131.0  3350.0  3514.0   \n",
       "                     214 125             2  3489.0  4232.0  4193.0  4134.0   \n",
       "                     213 130             2  1174.0  1722.0  1581.0  1707.0   \n",
       "                     215 55              2   961.0  1364.0  1465.0  1777.0   \n",
       "                     213 53              2   961.0  1435.0  1523.0  3029.0   \n",
       "                     192 76              2   640.0  1075.0  1407.0  2890.0   \n",
       "                     188 71              2   460.0   785.0   944.0  2612.0   \n",
       "                     178 77              2   604.0   930.0  1118.0  2403.0   \n",
       "                     175 79              2   640.0  1003.0  1234.0  2612.0   \n",
       "                     177 84              2   496.0   930.0  1118.0  2542.0   \n",
       "                     182 89              2   568.0  1219.0  1350.0  3028.0   \n",
       "                     188 90              2   604.0   931.0   944.0  1985.0   \n",
       "\n",
       "                                  swir1   swir2  \n",
       "study_area scene_num row column                  \n",
       "mtbarker   1         38  105     1305.0   558.0  \n",
       "                     33  104     2284.0  1088.0  \n",
       "                     32  105     2284.0  1044.0  \n",
       "                     28  106     2192.0   999.0  \n",
       "                     26  102     2009.0   823.0  \n",
       "                     24  100     1886.0   823.0  \n",
       "                     25  109     1948.0   867.0  \n",
       "                         116     2131.0   999.0  \n",
       "                     29  148     2040.0   867.0  \n",
       "                         150     1917.0   823.0  \n",
       "                     31  152     2009.0   911.0  \n",
       "                     35  151     2040.0   867.0  \n",
       "                     37  150     2070.0   867.0  \n",
       "                     68  136     2040.0   867.0  \n",
       "                     61  144     2315.0  1000.0  \n",
       "                     58  145     2254.0  1000.0  \n",
       "                     55  147        NaN     NaN  \n",
       "                     56  140     2346.0  1044.0  \n",
       "                     59  137     2223.0  1000.0  \n",
       "                     54  165     1704.0   647.0  \n",
       "                     55  167     1704.0   647.0  \n",
       "                     53  168     1734.0   647.0  \n",
       "                     52  167     1673.0   735.0  \n",
       "                     51  165     1642.0   602.0  \n",
       "                     49  165     1704.0   735.0  \n",
       "                     47  166     1765.0   779.0  \n",
       "                     46  169     1856.0   867.0  \n",
       "                     50  169     1734.0   735.0  \n",
       "                     60  166     1551.0   647.0  \n",
       "                     59  165     1489.0   558.0  \n",
       "...                                 ...     ...  \n",
       "swsyd      830       117 10      1028.0   343.0  \n",
       "                     124 7       1028.0   637.0  \n",
       "                     105 56      1376.0   563.0  \n",
       "                     97  56      1128.0   416.0  \n",
       "                     210 18      2024.0  1226.0  \n",
       "                     204 21      2322.0  1300.0  \n",
       "                     192 27      2322.0  1226.0  \n",
       "                     191 25      2421.0  1226.0  \n",
       "                     238 21      3416.0  1963.0  \n",
       "                     239 33      2820.0  1300.0  \n",
       "                     233 41      1378.0   711.0  \n",
       "                     201 262     1924.0   784.0  \n",
       "                     194 258     2122.0   784.0  \n",
       "                     189 253     2271.0   711.0  \n",
       "                     175 250     2023.0   784.0  \n",
       "                     234 177        NaN     NaN  \n",
       "                     235 180        NaN     NaN  \n",
       "                     230 129     2819.0  2699.0  \n",
       "                     225 123     3961.0  3508.0  \n",
       "                     214 125     5450.0  5274.0  \n",
       "                     213 130     2968.0  3066.0  \n",
       "                     215 55      2024.0  2110.0  \n",
       "                     213 53      2620.0  2110.0  \n",
       "                     192 76      2520.0  1594.0  \n",
       "                     188 71      2023.0  1447.0  \n",
       "                     178 77      2122.0  1594.0  \n",
       "                     175 79      2073.0  1520.0  \n",
       "                     177 84      2122.0  1594.0  \n",
       "                     182 89      2520.0  1741.0  \n",
       "                     188 90      1924.0  1300.0  \n",
       "\n",
       "[1322 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load previous training data\n",
    "# by taking the last (ie most recent if the standard date is attached to the file) .pkl file\n",
    "files = os.listdir('../')\n",
    "pickles = []\n",
    "for file in files:\n",
    "    if file[-3::] == 'pkl':\n",
    "        pickles.append(file)\n",
    "trainingdata = pd.read_pickle('../' + pickles[-1])\n",
    "\n",
    "# view the current status\n",
    "trainingdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 30,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## Other broad scope variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# easier to work with integers than strings, so map the planned training classes to integers\n",
    "landcover = {'vegetation':1,'urban':2,'earth':3,'water':4}\n",
    "# range of pretermined study areas to use as sources for training data\n",
    "study_areas = ['mtbarker', 'swmelb', 'gunghalin', 'goldengrove', 'molonglo',\n",
    "               'nperth', 'swbris', 'swsyd', 'goolwa', 'gladstone', 'goldcoast','newcastle','custom']\n",
    "\n",
    "# not in broad scope yet\n",
    "sat_bands = ['blue','green','red','nir','swir1','swir2']\n",
    "dc_bands = sat_bands.copy() + ['cloud_mask']\n",
    "\n",
    "colours = ['r', 'b', 'm', 'c'] # red, blue, magenta, cyan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 50,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## function: makeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def makeClassifier(model):\n",
    "    \"\"\"\n",
    "    This function initialises the classifier requested by the user, scales the data, trains the classifier,\n",
    "    and returns both the classifier and the scaler for use with the unknown data.\n",
    "    \"\"\"\n",
    "    if model == 'svc':   # support vector classifier\n",
    "        clf = svm.SVC()\n",
    "    if model == 'rfc':   # random forest classifier\n",
    "        clf = RandomForestClassifier()\n",
    "    # scale and normalize the data\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler().fit(trainingdata.dropna(axis=0, how = 'any')[sat_bands].values)\n",
    "    X_fortraining = scaler.transform(trainingdata.dropna(axis=0, how = 'any')[sat_bands].values)\n",
    "    X_fortraining = preprocessing.normalize(X_fortraining)\n",
    "\n",
    "    # train the model\n",
    "    clf.fit(X_fortraining,trainingdata.dropna(axis=0, how = 'any')['landcover'].values)\n",
    "    return clf, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 59,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Formatting Remaining Data for Classification & Classifying It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def reformatAndClassify(study_area, clf, scaler):\n",
    "    \"\"\"\n",
    "    This function reformats the data in the Xarray into a format for easy classifiying, applies the scaler\n",
    "    from the makeClassifier function (so that the training and predicted data are comparable), and then\n",
    "    passes the data to the classifer, and appends the results to the input data structure.\n",
    "    \"\"\"\n",
    "    # get the data\n",
    "    data = getData(study_area)\n",
    "    \n",
    "    #setting up the xarray to store the results for easy plotting later\n",
    "    for newband in ['landcover','predicted_landcover']:\n",
    "        temp = data[:,:,:].sel(band='red').copy()\n",
    "        temp.band.values = newband\n",
    "        temp.values[:] = np.nan\n",
    "        data = xr.concat([data, temp], dim='band')\n",
    "\n",
    "    # useful variable for down the track\n",
    "    shape = data.values.shape\n",
    "\n",
    "    # record the start time of the process\n",
    "    start = datetime.datetime.now()\n",
    "    print('Classifying ' + study_area + ' at ' + str(start))\n",
    "    \n",
    "    # classify one scene at a time, save the results to the xarray\n",
    "    # # rewrite this to work pixel by pixel on timeseries, will tie in better to change detection\n",
    "    for scene in range(shape[2]):\n",
    "\n",
    "        # setting up dataframe multilevel indexes\n",
    "        col_idx = list(range(shape[0])) * shape[1]\n",
    "        row_idx = []\n",
    "        for i in range(shape[1]):\n",
    "            row_idx += [i] * shape[0]\n",
    "        scene_idx = [scene] * (shape[0] * shape[1])\n",
    "\n",
    "        # reshape the data into a 2D flat array for scikit learn\n",
    "        flattened = data[:,:,scene].sel(band=dc_bands).values.reshape(shape[0] * shape[1], len(dc_bands))\n",
    "\n",
    "        # add the data to a new DataFrame, set up the columns and index\n",
    "        alldata = pd.DataFrame(flattened)\n",
    "        alldata.columns = dc_bands\n",
    "        alldata['row'] = row_idx\n",
    "        alldata['column'] = col_idx\n",
    "        alldata['scene_num'] = scene_idx\n",
    "        alldata['study_area'] = study_area\n",
    "        alldata = alldata.set_index(['study_area','scene_num','row','column'])\n",
    "\n",
    "        # join in the training data. This is a SQL left join, so only adds data to current study area\n",
    "        alldata = alldata.reset_index().join(trainingdata[['landcover']], on=trainingdata.index.names).set_index(alldata.index.names)\n",
    "\n",
    "        # reduce alldata down to valid pixels (ie cloudmask), and non-training pixels (ie landcover is still NaN)\n",
    "        datatoclassify = alldata[alldata['cloud_mask'] == 0 & np.isnan(alldata['landcover'])].copy()\n",
    "        # remove pixels with a np.nan as scikit-learn doesn't like them. Only keep spectral bands\n",
    "        datatoclassify = datatoclassify[np.isnan(datatoclassify['landcover'])][sat_bands]\n",
    "        # cast these relevant columns into a numpy array\n",
    "        datatoclassify_np = np.array(datatoclassify)\n",
    "\n",
    "        # to deal with an entirely clouded scene\n",
    "        if len(datatoclassify_np) == 0:\n",
    "            continue\n",
    "\n",
    "        # scale and normalize the data so it resembles the training data.\n",
    "        datatoclassify_np = scaler.transform(datatoclassify_np)\n",
    "        datatoclassify_np = preprocessing.normalize(datatoclassify_np)\n",
    "\n",
    "        # results of predict() are a 1 dimensional numpy array of the same length as the input data\n",
    "        # assign these results to a new column in the dataframe\n",
    "        datatoclassify['predicted_landcover'] = clf.predict(datatoclassify_np)\n",
    "\n",
    "        # SQL left join the results back onto the original data\n",
    "        alldata = alldata.reset_index().join(datatoclassify[['predicted_landcover']], on=trainingdata.index.names).set_index(alldata.index.names)\n",
    "\n",
    "        #save the training data and classification results into the Xarray\n",
    "        data[:,:,scene].loc[dict(band='landcover')] = alldata['landcover'].values.reshape(shape[0],shape[1])\n",
    "        data[:,:,scene].loc[dict(band='predicted_landcover')] = alldata['predicted_landcover'].values.reshape(shape[0],shape[1])\n",
    "\n",
    "    \n",
    "    print('Time taken to classify ' + study_area + ' was ' + str(datetime.datetime.now() - start))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 63,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Time Series of Classifications into Change Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 63,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## function: dateStringToFloat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def dateStringToFloat(s):\n",
    "    \"\"\"\n",
    "    A quick funciton to convert from a datetime object to the decimal year format used from Peter's algorithm.\n",
    "    \"\"\"\n",
    "    year = int(s[0:4])\n",
    "    month = int(s[5:7])\n",
    "    \n",
    "    if month in [1,2,3]:\n",
    "        year += 0.125\n",
    "    elif month in [4,5,6]:\n",
    "        year += 0.375\n",
    "    elif month in [7,8,9]:\n",
    "        year += 0.625\n",
    "    else:\n",
    "        year += 0.875\n",
    "    \n",
    "    return year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: modalFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modalFilter(df, column, index, span = 10):\n",
    "    \"\"\"\n",
    "    The custom modal filter function.\n",
    "    It returns the mode of the specified column in the requested dataframe over groups of the number of rows specified\n",
    "    as the span.\n",
    "    \n",
    "    In the cast of a tie, a hierarchy of landcover types is used that is biased against urban.\n",
    "    \"\"\"\n",
    "    # if there aren't enough rows left to do a full size span, just do what is left.\n",
    "    if index > (len(df) - span):\n",
    "        mode_arr = df[column][index::].mode().values\n",
    "    else:\n",
    "        mode_arr = df[column][index : index + span].mode().values\n",
    "    # if there is more than 1 mode, apply the hierachry and return the relevant landcover from that\n",
    "    if len(mode_arr) > 1:\n",
    "        heirarchy = [1, 3, 4, 2] # vegetation, earth, water, urban\n",
    "        max_priority = 4\n",
    "        i_max = 0\n",
    "        for i in mode_arr:\n",
    "            if heirarchy.index(i) < max_priority:\n",
    "                max_priority = heirarchy.index(i)\n",
    "                i_max = i\n",
    "        return i_max \n",
    "    # otherwise, just return the mode\n",
    "    else:\n",
    "        return mode_arr[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: changeDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeDetector(study_area, data, clftype):\n",
    "    \"\"\"\n",
    "    Description of current decision rule for assigning urban change:\n",
    "     calculate the mode over each 10 scenes, and assign that value to the first scene of the 10\n",
    "        if the first two modal values are both urban, assume already developed and move to next pixel\n",
    "        if not, calculate the mode of the modes, over each 3 modes, assigning the value to the first\n",
    "     find the first mode of modes that is urban for that pixel\n",
    "        use that data (MOM_change date) to find the group of 3 modes that contributed to the mode of modes\n",
    "        within the individual classifications for that pixel within that group of 30 classifications\n",
    "           if there is an instance of 2 classifications in a row being urban, use the date of the first of those\n",
    "           if not, simply use the first instance of an urban pixel in that range of 30\n",
    "\n",
    "    Limitations of Method:\n",
    "       - Many of them!\n",
    "       - Algorithm needs first 20 to establish baseline - so it can't detect early change\n",
    "       - Algorithm needs last 30 to build mode of modes - so it can't detect most recent change\n",
    "       - It's very slow - mulitple nested loops. I vectorized the process, but it actually ended up slower?!\n",
    "    \"\"\"\n",
    "    # setting up the results raster\n",
    "    shape = data.shape\n",
    "    changedates_arr = np.zeros((shape[1], shape[0]), dtype=np.float32)\n",
    "    changedates_arr[changedates_arr == 0] = np.nan\n",
    "\n",
    "    # variables for the modal filtering\n",
    "    mode_span = 10\n",
    "    MoM_span = 3\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    print('Change detection for ' + study_area + ' start time: ' + str(start))\n",
    "\n",
    "    # a very slow nested loop, keen to remove if possible\n",
    "    for x in range(shape[0]):\n",
    "        for y in range(shape[1]):\n",
    "#     for x in range(100,131,1):\n",
    "#         for y in range(100,131,1):\n",
    "#     for x in range(90,141,1):\n",
    "#         for y in range(90,141,1):\n",
    "            \n",
    "            # make a dataframe of the time-series of the predicted classifications\n",
    "            pixeldata = pd.DataFrame(data[x, y, :].sel(band='predicted_landcover').values, index = data.date.values, columns = ['predicted_landcover'])\n",
    "    \n",
    "            # make a new column to store mode in\n",
    "            pixeldata['mode1'] = np.nan\n",
    "            pixeldata['mode_of_modes'] = np.nan\n",
    "            mode_loc = pixeldata.columns.get_loc('mode1')\n",
    "\n",
    "            # remove any rows with NaNs\n",
    "            pixeldata = pixeldata[~np.isnan(pixeldata['predicted_landcover'])]\n",
    "            \n",
    "            # remove any possibility of duplicate dates (based on an issue with swmelbourne study area)\n",
    "            pixeldata = pixeldata[~pixeldata.index.duplicated(keep = 'first')]\n",
    "\n",
    "            # third level of nested of loops :(\n",
    "            # find mode of each 10 scenes, and store at first index of range\n",
    "            # for example the mode of scenes 10-19 will be stored in row 10.\n",
    "            for row in range(0, len(pixeldata), mode_span):\n",
    "                pixeldata.iloc[row, mode_loc] = modalFilter(pixeldata, 'predicted_landcover', row, span = mode_span)\n",
    "\n",
    "            # view or slice of data with modes\n",
    "            modes = pixeldata[~np.isnan(pixeldata['mode1'])]\n",
    "            \n",
    "            # if either of the first two modes are urban, assume pixel is already urban at start of landsat archive \n",
    "            if (modes['mode1'].iloc[0:2].values ==  2).any():\n",
    "                continue\n",
    "            # if the first two aren't urban, and any of the last 3 are, then assume change has happened and\n",
    "            # test against decision criteria\n",
    "            if (modes['mode1'].iloc[0:2].values != 2).all() and (modes['mode1'].iloc[-3::] == 2).any():\n",
    "                # applying modal filter to the modes, to create mode of modes\n",
    "                # save the result in the pixeldata dataframe\n",
    "                for row in range(0,len(modes),MoM_span):\n",
    "                    pixeldata.loc[modes.iloc[row].name, 'mode_of_modes'] = modalFilter(modes, 'mode1', row, span = MoM_span)\n",
    "\n",
    "                # decision criteria\n",
    "                if len(pixeldata[pixeldata['mode_of_modes'] == 2]) > 0:\n",
    "                    MoM_changedate = pixeldata[pixeldata['mode_of_modes'] == 2].iloc[0].name\n",
    "                    M_ss = pixeldata.loc[MoM_changedate::]\n",
    "                    M_changedate = M_ss[M_ss['mode1'] == 2].iloc[0].name\n",
    "                    M_changedate_loc = pixeldata.index.get_loc(M_changedate)\n",
    "                    pix_ss = pixeldata.iloc[M_changedate_loc - mode_span : M_changedate_loc + mode_span]\n",
    "                    twoinarow = pix_ss[(pix_ss['predicted_landcover'] == pix_ss['predicted_landcover'].shift(-1)) & \n",
    "                                        (pix_ss['predicted_landcover'][pix_ss['predicted_landcover'] == 2])]\n",
    "                    if len(twoinarow) > 0:\n",
    "                        changedate = twoinarow.iloc[0].name\n",
    "                    else:\n",
    "                        changedate = pix_ss[pix_ss['predicted_landcover'] == 2].iloc[0].name\n",
    "                    changedates_arr[y, x] = dateStringToFloat(changedate)\n",
    "\n",
    "    # print how long the modal filtering and change detection took                \n",
    "    print(study_area + ' processing time: ' + str(datetime.datetime.now() - start))           \n",
    "\n",
    "    # save the results to a .pkl for future access\n",
    "    results_save_location = '../' + study_area + '/changeresults_' + clftype + '_finalRun.pkl'\n",
    "    changedates_arr.dump(results_save_location)  \n",
    "    print('Results have been saved to', results_save_location, '\\n')\n",
    "    return changedates_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 67,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Running It All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 12,
        "hidden": false,
        "row": 67,
        "width": 4
       },
       "report_default": {}
      }
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bigrunstart = datetime.datetime.now()\n",
    "print('All Study Areas Loop Commenced at: ' + str(bigrunstart) + '\\n')\n",
    "clftype = 'svc'\n",
    "for study_area in ['gladstone']:    \n",
    "    clf, scaler = makeClassifier(clftype)  # valid options are 'rfc' and 'svc'\n",
    "    classified_data = reformatAndClassify(study_area, clf, scaler)\n",
    "    changeDetector(study_area, classified_data, clftype)\n",
    "    \n",
    "print('\\nAll study areas loop finished at: ' + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 71,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Viewing the Classification Results\n",
    "This feature exists if you want to dig down into the details of why a pixel recieved the change time that it did.\n",
    "It allows you to view the scene as a classified raster of variable transparency overlain on top of a true colour image of that scene.\n",
    "\n",
    "It relies on data being the Xarray with the classification attached, so it should be run after the previous cells.\n",
    "Though for speeds sake, the modal filtering process could be skipped if you already have the results or if you are more interested in the classification process, rather than the change detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 75,
        "width": 4
       },
       "report_default": {}
      }
     }
    },
    "heading_collapsed": true
   },
   "source": [
    "## function: drawClassifiedScene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def drawClassifiedScene(data, scene_num, alpha):\n",
    "    \n",
    "    # colour map included incase of need to display false colour or other in the future\n",
    "    # could change this to an ordereddict and remove the RGB list created below...?\n",
    "    colourmap = {'R':'red', 'G':'green', 'B':'blue'}\n",
    "    \n",
    "    # combine the data for the 3 bands to be displayed into a single numpy array\n",
    "    h = data.shape[1]\n",
    "    w = data.shape[0]\n",
    "    t = data.shape[2]\n",
    "    \n",
    "    if scene_num > (t -1):\n",
    "        scene_num = t - 1\n",
    "    RGB = ['R','G','B']\n",
    "    date = str(data[:,:,scene_num].date.values)\n",
    "    \n",
    "    # create array to store the RGB info in, and fill by looping through the colourmap variable\n",
    "    # note the .T at the end, because the data array is setup as a (x,y,t), but imshow works (y,x)\n",
    "    rawimg = np.zeros((h, w, 3), dtype=np.float32)\n",
    "    for i in range(len(RGB)):     \n",
    "        rawimg[:,:,i] = data[:,:,scene_num].sel(band=colourmap[RGB[i]]).T\n",
    "        \n",
    "    # equalizing for all bands together\n",
    "    # goal is to make is human interpretable\n",
    "    img_toshow = exposure.equalize_hist(rawimg, mask = np.isfinite(rawimg))    \n",
    "\n",
    "    # displaying the results and formatting the axes etc\n",
    "    plt.imshow(img_toshow)\n",
    "    ax = plt.gca()\n",
    "    ax.set_title('True Colour Landsat Scene, taken\\n' + date + ', over ' + study_area)\n",
    "    \n",
    "    # make the colour map for the cover classes\n",
    "    cmap = colors.ListedColormap(colours)\n",
    "    \n",
    "    # draw the classification results and the training data results\n",
    "    ax.imshow(data[:,:,scene_num].sel(band='predicted_landcover').values.T, cmap = cmap, alpha = alpha)\n",
    "    ax.imshow(data[:,:,scene_num].sel(band='landcover').values.T, cmap = cmap, alpha = 1)\n",
    "    \n",
    "    #draw a legend for the classification colours\n",
    "    legend_patches = []\n",
    "    for cover in landcover.keys():\n",
    "        legend_patches.append(mpatches.Patch(color = colours[landcover[cover]-1], label = cover))\n",
    "    ax.legend(handles = legend_patches)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 75,
        "width": 4
       },
       "report_default": {}
      }
     }
    },
    "heading_collapsed": true
   },
   "source": [
    "## function: drawClassifiedPlots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def drawClassifiedPlots(data, scene_num, alpha):\n",
    "    \n",
    "    ax1 = plt.subplot2grid([2,4],[0,0], rowspan = 2, colspan = 2)\n",
    "    ax1.clear()\n",
    "    ax1 = drawClassifiedScene(data, scene_num, alpha = 0)\n",
    "    ax2 = plt.subplot2grid([2,4],[0,2], rowspan = 2, colspan = 2)\n",
    "    ax2.clear()\n",
    "    ax2 = drawClassifiedScene(data, scene_num, alpha)\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 79,
        "width": 4
       },
       "report_default": {}
      }
     }
    },
    "heading_collapsed": true
   },
   "source": [
    "## function: check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def check(data, scene_num, alpha):\n",
    " \n",
    "    # control the figure size\n",
    "    fig = plt.figure(figsize=[12,7])\n",
    "    axs = fig.axes\n",
    "    plt.subplots_adjust(hspace = 0.6)\n",
    "    \n",
    "    # draw the figure\n",
    "    drawClassifiedPlots(data, scene_num, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 79,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## Drawing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "num_scenes = data.shape[2]\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    interact(check,\n",
    "             data = fixed(data),\n",
    "             scene_num = IntSlider(value = 1, min = 0, max = num_scenes -1 ,description = \"Scene Number\"),\n",
    "             alpha= FloatSlider(value = 0.6, min = 0, max = 1, description = \"Classification Transparency\"))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "938px",
    "left": "0px",
    "right": "1464px",
    "top": "111px",
    "width": "270px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "widgets": {
   "state": {
    "c52feb267d464c0681dc5b8825029c6a": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
