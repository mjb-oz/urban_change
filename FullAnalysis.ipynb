{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 13,
        "hidden": false,
        "row": 0,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Supervised Classification Urban Change Detection\n",
    "This notebook is the complete workflow for urban change detection algorithm.\n",
    "The goal of this process is to be able to identigfy pixels that have been urbanised (changed from vegetation to the built environment) during the operation of the LandSat Earth Observation Satellites (since 1987).\n",
    "\n",
    "This notebook lets you:\n",
    "- create training data to train the classifier on\n",
    "- classify the data according to 4 broad landcover classes\n",
    "- view the results of your classification process\n",
    "- identify if and when a pixel that was previously not urban becomes dominantly urban (change detection)\n",
    "- view the results of the change detection\n",
    "\n",
    "The markdown cells have been designed to work with the 'Table Of Contents(2)' Jupyter notebook extension.\n",
    "This is highly recommended, if you don't have it yet (and are working on the VDI on the 'agdc-py3-prod module'\n",
    "select \"Edit\" on the menu bar above, click the \"nbextension config\" button at the bottom of the menu, and enable\n",
    "the extension. The 'Collapsible Headings' extension is also highly recommended.\n",
    "\n",
    "This was written Mike Barnes as part of his third graduate rotation, during January 2018.\n",
    "Any questions, please contact me at michael.barnes@ga.gov.au"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 13,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## Python Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import datacube\n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.storage import masking\n",
    "from datacube.storage.masking import mask_to_dict\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import gdal\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider, FloatSlider, Dropdown\n",
    "from IPython.display import display\n",
    "\n",
    "from skimage import exposure\n",
    "from scipy.signal import lfilter\n",
    "\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 5,
        "hidden": false,
        "row": 17,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## Functions for Loading Data and Building the Xarray\n",
    "This project built on some existing work by Peter Tan. An output from Peter's urban change detection algorithm is raster files with all the relevant NBAR (analysis ready satellite derived surface reflectance readings) data saved to the output directory. To speed the loading and analysis during this script, this notebook will use those exisitng files if they are available. Otherwise it will load the data from the Digital Earth Australia archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 13,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### function: checkForLocalFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def checkForLocalFiles(study_area):\n",
    "    rootdir = os.listdir('../')\n",
    "    if study_area in rootdir:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 13,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### function: getData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def getData(study_area):\n",
    "    # if the study area is a string, and is accessible locally, load it\n",
    "    if isinstance(study_area, str):\n",
    "        if checkForLocalFiles(study_area):\n",
    "            data = getLocalData(study_area)\n",
    "            return data\n",
    "    # if the study area is a string and is on the list, load it\n",
    "        else:\n",
    "            data = DCLoadName(study_area)\n",
    "    # if the study area is a list of coordinates, use them to load the data\n",
    "    elif isinstance(study_area, list) and len(study_area) == 4:\n",
    "        data = DCLoad(study_area)\n",
    "        \n",
    "    # if the study area isn't loaded locally, transfrom the DC originated xarray into \"my\" format  \n",
    "    if not checkForLocalFiles(study_area):\n",
    "        data = transformXarrayToCustomStyle(data)\n",
    "        return data\n",
    "    else:\n",
    "        print('Data Loading Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 22,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### function: DCLoadName\n",
    "This function is a wrapper for the DCLoad function, that allows previously used study areas to be easily restudied\n",
    "by easily loading exactly the same area of interest (AOI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def DCLoadName(study_area):   \n",
    "    if study_area == 'mtbarker':\n",
    "        lat_min = -35.05\n",
    "        lat_max = -35.08\n",
    "        lon_min = 138.85\n",
    "        lon_max = 138.895  \n",
    "    elif study_area == 'swmelb':\n",
    "        lat_min = -37.879\n",
    "        lat_max = -37.91\n",
    "        lon_min = 144.705\n",
    "        lon_max = 144.76  \n",
    "    elif study_area == 'gunghalin':\n",
    "        lat_min = -35.18\n",
    "        lat_max = -35.21\n",
    "        lon_min = 149.14\n",
    "        lon_max = 149.17\n",
    "    elif study_area == 'goldengrove': \n",
    "        lat_min = -34.77\n",
    "        lat_max = -34.8\n",
    "        lon_min = 138.66\n",
    "        lon_max = 138.73\n",
    "    elif study_area == 'molonglo':\n",
    "        lat_min = -35.3\n",
    "        lat_max = -35.33\n",
    "        lon_min = 149.015\n",
    "        lon_max = 149.06\n",
    "    elif study_area == 'nperth':\n",
    "        lat_min = -31.686\n",
    "        lat_max = -31.73\n",
    "        lon_min = 115.79\n",
    "        lon_max = 115.813\n",
    "    elif study_area == 'swbris':\n",
    "        lat_min = -27.66\n",
    "        lat_max = -27.7 \n",
    "        lon_min = 152.877\n",
    "        lon_max = 152.93\n",
    "    elif study_area == 'swsyd':\n",
    "        lat_min = -33.993\n",
    "        lat_max = -34.04\n",
    "        lon_min = 150.715 \n",
    "        lon_max = 150.78\n",
    "    \n",
    "    return DCLoad([lat_min, lat_max, lon_min, lon_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 26,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### function: DCLoad\n",
    "This function is a variation of a datacube query supplied by Erin Telfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def DCLoad(study_area):\n",
    "    # to time how long the load takes\n",
    "    start = datetime.datetime.now()\n",
    "    print('Loading data') \n",
    "    print('Load Started At: ' + str(start))\n",
    "    \n",
    "    # define temporal range \n",
    "    start_of_epoch = '1987-01-01'\n",
    "    end_of_epoch =  '2017-10-31'\n",
    "\n",
    "    # define bands of interest\n",
    "    bands_of_interest = ['blue', 'green', 'red', \n",
    "                         'nir', 'swir1', 'swir2']\n",
    "\n",
    "    # Landsat sensors of interest are defined\n",
    "    sensors = ['ls8', 'ls7', 'ls5'] \n",
    "\n",
    "    # unpack input parameter\n",
    "    lat_min, lat_max, lon_min, lon_max = study_area    \n",
    "\n",
    "    print('Bounding box: ' + str(lat_min) + ' S, ' + str(lon_min) +\n",
    "          ' E to ' + str(lat_max) + ' S, ' + str(lon_max) + ' E' )\n",
    "    print('Epoch: ' + start_of_epoch + ' to ' + end_of_epoch)\n",
    "    print('Sensors: ' + str(sensors))\n",
    "    print('Bands of Interest: ' + str(bands_of_interest))\n",
    "\n",
    "    # create query\n",
    "    query = {'time': (start_of_epoch, end_of_epoch),}\n",
    "    query['x'] = (lon_min, lon_max)\n",
    "    query['y'] = (lat_max, lat_min)\n",
    "    query['crs'] = 'EPSG:4326'\n",
    "\n",
    "    #Create cloud mask. This will define which pixel quality (PQ) artefacts are removed from the results.\n",
    "    # It should be noted the \"land_sea\" code will remove all ocean/sea pixels.\n",
    "    mask_components = {'cloud_acca':'no_cloud',\n",
    "    'cloud_shadow_acca' :'no_cloud_shadow',\n",
    "    'cloud_shadow_fmask' : 'no_cloud_shadow',\n",
    "    'cloud_fmask' :'no_cloud',\n",
    "    'blue_saturated' : False,\n",
    "    'green_saturated' : False,\n",
    "    'red_saturated' : False,\n",
    "    'nir_saturated' : False,\n",
    "    'swir1_saturated' : False,\n",
    "    'swir2_saturated' : False,\n",
    "    'contiguous':True,\n",
    "    'land_sea': 'land'}\n",
    "\n",
    "    # Connect to DataCube\n",
    "    dc = datacube.Datacube(app='Urban Change Detection')\n",
    "    \n",
    "    # Data for each Landsat sensor is retrieved and saved in a dict for concatenation\n",
    "    sensor_clean = {}\n",
    "    \n",
    "    for sensor in sensors:\n",
    "        # Load the NBAR and corresponding PQ\n",
    "        sensor_nbar = dc.load(product= sensor+'_nbar_albers', group_by='solar_day', \n",
    "                              measurements = bands_of_interest,  **query)\n",
    "        sensor_pq = dc.load(product= sensor+'_pq_albers', group_by='solar_day', \n",
    "                            fuse_func=ga_pq_fuser, **query)\n",
    "\n",
    "        # Retrieve the projection information before masking/sorting\n",
    "        crs = sensor_nbar.crs\n",
    "        crswkt = sensor_nbar.crs.wkt\n",
    "        affine = sensor_nbar.affine        \n",
    "\n",
    "        # Combing the pq so it is a single \n",
    "        sensor_all = xr.auto_combine([sensor_pq,sensor_nbar])\n",
    "        sensor_clean[sensor] = sensor_all\n",
    "\n",
    "        print('Loaded %s' % sensor) \n",
    "\n",
    "    print('Concatenating')\n",
    "    nbar_clean = xr.concat(sensor_clean.values(), 'time')\n",
    "    nbar_clean = nbar_clean.sortby('time')\n",
    "    nbar_clean.attrs['crs'] = crs\n",
    "    nbar_clean.attrs['affin|e'] = affine    \n",
    "\n",
    "    print ('Load and Xarray build complete')\n",
    "    print('Process took ' + str(datetime.datetime.now() - start))\n",
    "    \n",
    "    # return xarray\n",
    "    return nbar_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 26,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### function: getLocalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def getLocalData(study_area):\n",
    "    \"\"\"A quick helper function to load the output files from Peter's code for the given location.\n",
    "    It returns and Xarray of the landsat data for that study area.\"\"\"\n",
    "    # build a list of all files in the directory (ie the folder for that location)\n",
    "    location = '../' + study_area + '/'\n",
    "    files = os.listdir(location)\n",
    "\n",
    "    print('Loading data from: ' + location)\n",
    "    \n",
    "    # build a list of all the NBAR*.img file names and which bands they represent\n",
    "    NBARfiles = []\n",
    "    bands = []\n",
    "    for file in files:\n",
    "        if file[-4::] == '.img' and file[0:4] == 'NBAR':\n",
    "            NBARfiles.append(file)\n",
    "            bands.append(file.split('NBAR_')[1].split('.img')[0])\n",
    "\n",
    "    # open all the .img files with NBAR in the name, convert to numpy array, swap axes so order is (x, y, t)\n",
    "    # and save to dict\n",
    "    raw_data = {}\n",
    "    for i in range(len(NBARfiles)):\n",
    "        raw_data[bands[i]] = gdal.Open(location + NBARfiles[i]).ReadAsArray().swapaxes(0,2)\n",
    "#     num_scenes = len(raw_data['red'][0][0])   # delete this?\n",
    "\n",
    "    # build a list of all the dates represented by each band in the NBAR files\n",
    "    # reuse the list of NBAR file names, but this time access the .hdr file\n",
    "    in_dates = False\n",
    "    dates = []\n",
    "    for line in open(location + NBARfiles[0].split('.img')[0] + '.hdr'):\n",
    "        if line[0] == '}':\n",
    "            continue\n",
    "        if in_dates:\n",
    "            dates.append(line.split(',')[0].strip())\n",
    "        if line[0:10] == 'band names':\n",
    "            in_dates = True\n",
    "\n",
    "    # save list of satellite originated bands\n",
    "    sat_bands = bands.copy()\n",
    "\n",
    "    # add the yet to be calculated derivative bands to the overall bands list\n",
    "    bands += ['cloud_mask']\n",
    "\n",
    "    # building the Xarray\n",
    "    # define the size for the numpy array that will hold all the data for conversion into XArray\n",
    "    x = len(raw_data['red'])\n",
    "    y = len(raw_data['red'][0])\n",
    "    t = len(raw_data['red'][0][0])\n",
    "    n = len(bands)\n",
    "\n",
    "    # create an empty numpy array of the correct size\n",
    "    alldata = np.zeros((x, y, t, n), dtype=np.float32)\n",
    "\n",
    "    # populate the numpy array with the satellite data\n",
    "    # turn all no data NBAR values to NaNs\n",
    "    for i in range(len(sat_bands)):\n",
    "        alldata[:,:,:,i] = raw_data[sat_bands[i]]\n",
    "        alldata[:,:,:,i][alldata[:,:,:,i] == -999] = np.nan\n",
    "\n",
    "    # convert the numpy array into an xarray, with appropriate lables, and axes names\n",
    "    data = xr.DataArray(alldata, coords = {'x':range(x), 'y':range(y), 'date':dates, 'band':bands},\n",
    "                 dims=['x', 'y', 'date', 'band'])\n",
    "    \n",
    "    # import cloudmask and add to xarray\n",
    "    cloudmask = gdal.Open(location + '/tsmask.img').ReadAsArray().swapaxes(0,2)\n",
    "    data.loc[:,:,:,'cloud_mask'] = cloudmask\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 26,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### function: transformXarrayToCustomStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def transformXarrayToCustomStyle(data_new):\n",
    "    # downscale the dataset to the dataarray, and transpose so the variable numbers are right\n",
    "    datafixed = data_new.to_array().transpose('x','y','time','variable')\n",
    "    \n",
    "    # rename the variables into 'band'\n",
    "    datafixed = datafixed.rename({'variable':'band',})\n",
    "    \n",
    "    # pull out the current list of bands, find the index number of \"pixelquality\"\n",
    "    # replace with 'cloud_mask', and reassign\n",
    "    new_bands = list(datafixed.band.values)\n",
    "    cm = new_bands.index('pixelquality')\n",
    "    new_bands[cm] = 'cloud_mask'\n",
    "    datafixed.band.values = new_bands\n",
    "    \n",
    "    # change pixel quality values to mask, 0 = good, 3 = bad\n",
    "    cm_vals = data[:,:,:].sel(band='cloud_mask').values\n",
    "    cm_vals[cm_vals == 0] = 1\n",
    "    cm_vals[cm_vals == 16383] = 0\n",
    "    cm_vals[cm_vals != 0] = 3\n",
    "    \n",
    "    return datafixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: customStyleXarrayToStandard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customStyleXarrayToStandard(data):\n",
    "    return data.to_dataset(dim='band')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 30,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Setting up broad scope variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 30,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## Load Previous Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 12,
        "hidden": false,
        "row": 34,
        "width": 7
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>landcover</th>\n",
       "      <th>blue</th>\n",
       "      <th>green</th>\n",
       "      <th>red</th>\n",
       "      <th>nir</th>\n",
       "      <th>swir1</th>\n",
       "      <th>swir2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>study_area</th>\n",
       "      <th>scene_num</th>\n",
       "      <th>row</th>\n",
       "      <th>column</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">mtbarker</th>\n",
       "      <th rowspan=\"30\" valign=\"top\">1</th>\n",
       "      <th>38</th>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "      <td>247.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>4094.0</td>\n",
       "      <td>1305.0</td>\n",
       "      <td>558.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <th>104</th>\n",
       "      <td>1</td>\n",
       "      <td>420.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>3877.0</td>\n",
       "      <td>2284.0</td>\n",
       "      <td>1088.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "      <td>420.0</td>\n",
       "      <td>677.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>3833.0</td>\n",
       "      <td>2284.0</td>\n",
       "      <td>1044.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <th>106</th>\n",
       "      <td>1</td>\n",
       "      <td>362.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>3877.0</td>\n",
       "      <td>2192.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <th>102</th>\n",
       "      <td>1</td>\n",
       "      <td>324.0</td>\n",
       "      <td>636.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>4483.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "      <td>305.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>4093.0</td>\n",
       "      <td>1886.0</td>\n",
       "      <td>823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">25</th>\n",
       "      <th>109</th>\n",
       "      <td>1</td>\n",
       "      <td>324.0</td>\n",
       "      <td>596.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>4527.0</td>\n",
       "      <td>1948.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>677.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>3746.0</td>\n",
       "      <td>2131.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">29</th>\n",
       "      <th>148</th>\n",
       "      <td>1</td>\n",
       "      <td>439.0</td>\n",
       "      <td>958.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>5734.0</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "      <td>362.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>5390.0</td>\n",
       "      <td>1917.0</td>\n",
       "      <td>823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <th>152</th>\n",
       "      <td>1</td>\n",
       "      <td>458.0</td>\n",
       "      <td>1038.0</td>\n",
       "      <td>847.0</td>\n",
       "      <td>5691.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>911.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <th>151</th>\n",
       "      <td>1</td>\n",
       "      <td>382.0</td>\n",
       "      <td>838.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>5820.0</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "      <td>458.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>5906.0</td>\n",
       "      <td>2070.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <th>136</th>\n",
       "      <td>1</td>\n",
       "      <td>477.0</td>\n",
       "      <td>958.0</td>\n",
       "      <td>811.0</td>\n",
       "      <td>5305.0</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <th>144</th>\n",
       "      <td>1</td>\n",
       "      <td>477.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>811.0</td>\n",
       "      <td>4874.0</td>\n",
       "      <td>2315.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <th>145</th>\n",
       "      <td>1</td>\n",
       "      <td>515.0</td>\n",
       "      <td>1038.0</td>\n",
       "      <td>883.0</td>\n",
       "      <td>5219.0</td>\n",
       "      <td>2254.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <th>147</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <th>140</th>\n",
       "      <td>1</td>\n",
       "      <td>477.0</td>\n",
       "      <td>838.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>4571.0</td>\n",
       "      <td>2346.0</td>\n",
       "      <td>1044.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <th>137</th>\n",
       "      <td>1</td>\n",
       "      <td>477.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>811.0</td>\n",
       "      <td>4917.0</td>\n",
       "      <td>2223.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "      <td>382.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>488.0</td>\n",
       "      <td>5563.0</td>\n",
       "      <td>1704.0</td>\n",
       "      <td>647.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <th>167</th>\n",
       "      <td>1</td>\n",
       "      <td>420.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>5606.0</td>\n",
       "      <td>1704.0</td>\n",
       "      <td>647.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <th>168</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>677.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>5348.0</td>\n",
       "      <td>1734.0</td>\n",
       "      <td>647.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <th>167</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>5262.0</td>\n",
       "      <td>1673.0</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>5348.0</td>\n",
       "      <td>1642.0</td>\n",
       "      <td>602.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "      <td>401.0</td>\n",
       "      <td>798.0</td>\n",
       "      <td>596.0</td>\n",
       "      <td>5348.0</td>\n",
       "      <td>1704.0</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <th>166</th>\n",
       "      <td>1</td>\n",
       "      <td>382.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>5391.0</td>\n",
       "      <td>1765.0</td>\n",
       "      <td>779.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <th>169</th>\n",
       "      <td>1</td>\n",
       "      <td>382.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>5262.0</td>\n",
       "      <td>1856.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <th>169</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>5606.0</td>\n",
       "      <td>1734.0</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <th>166</th>\n",
       "      <td>1</td>\n",
       "      <td>305.0</td>\n",
       "      <td>596.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>5434.0</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>647.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>556.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>5348.0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>558.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">swsyd</th>\n",
       "      <th rowspan=\"30\" valign=\"top\">830</th>\n",
       "      <th>117</th>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>170.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>2402.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>170.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2402.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>637.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>170.0</td>\n",
       "      <td>421.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>2402.0</td>\n",
       "      <td>1376.0</td>\n",
       "      <td>563.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>133.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2262.0</td>\n",
       "      <td>1128.0</td>\n",
       "      <td>416.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>243.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>654.0</td>\n",
       "      <td>3306.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>316.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>2890.0</td>\n",
       "      <td>2322.0</td>\n",
       "      <td>1300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>352.0</td>\n",
       "      <td>858.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>3374.0</td>\n",
       "      <td>2322.0</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>424.0</td>\n",
       "      <td>858.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>3651.0</td>\n",
       "      <td>2421.0</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>496.0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>1061.0</td>\n",
       "      <td>3029.0</td>\n",
       "      <td>3416.0</td>\n",
       "      <td>1963.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>352.0</td>\n",
       "      <td>858.0</td>\n",
       "      <td>829.0</td>\n",
       "      <td>3859.0</td>\n",
       "      <td>2820.0</td>\n",
       "      <td>1300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>243.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>479.0</td>\n",
       "      <td>2891.0</td>\n",
       "      <td>1378.0</td>\n",
       "      <td>711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <th>262</th>\n",
       "      <td>1</td>\n",
       "      <td>281.0</td>\n",
       "      <td>641.0</td>\n",
       "      <td>363.0</td>\n",
       "      <td>4821.0</td>\n",
       "      <td>1924.0</td>\n",
       "      <td>784.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <th>258</th>\n",
       "      <td>1</td>\n",
       "      <td>281.0</td>\n",
       "      <td>714.0</td>\n",
       "      <td>363.0</td>\n",
       "      <td>5369.0</td>\n",
       "      <td>2122.0</td>\n",
       "      <td>784.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <th>253</th>\n",
       "      <td>1</td>\n",
       "      <td>317.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>479.0</td>\n",
       "      <td>5573.0</td>\n",
       "      <td>2271.0</td>\n",
       "      <td>711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <th>250</th>\n",
       "      <td>1</td>\n",
       "      <td>317.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>6254.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>784.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <th>177</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <th>180</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <th>129</th>\n",
       "      <td>2</td>\n",
       "      <td>1245.0</td>\n",
       "      <td>1722.0</td>\n",
       "      <td>1869.0</td>\n",
       "      <td>2334.0</td>\n",
       "      <td>2819.0</td>\n",
       "      <td>2699.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <th>123</th>\n",
       "      <td>2</td>\n",
       "      <td>2386.0</td>\n",
       "      <td>3131.0</td>\n",
       "      <td>3350.0</td>\n",
       "      <td>3514.0</td>\n",
       "      <td>3961.0</td>\n",
       "      <td>3508.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <th>125</th>\n",
       "      <td>2</td>\n",
       "      <td>3489.0</td>\n",
       "      <td>4232.0</td>\n",
       "      <td>4193.0</td>\n",
       "      <td>4134.0</td>\n",
       "      <td>5450.0</td>\n",
       "      <td>5274.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <th>130</th>\n",
       "      <td>2</td>\n",
       "      <td>1174.0</td>\n",
       "      <td>1722.0</td>\n",
       "      <td>1581.0</td>\n",
       "      <td>1707.0</td>\n",
       "      <td>2968.0</td>\n",
       "      <td>3066.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <th>55</th>\n",
       "      <td>2</td>\n",
       "      <td>961.0</td>\n",
       "      <td>1364.0</td>\n",
       "      <td>1465.0</td>\n",
       "      <td>1777.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>2110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <th>53</th>\n",
       "      <td>2</td>\n",
       "      <td>961.0</td>\n",
       "      <td>1435.0</td>\n",
       "      <td>1523.0</td>\n",
       "      <td>3029.0</td>\n",
       "      <td>2620.0</td>\n",
       "      <td>2110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <th>76</th>\n",
       "      <td>2</td>\n",
       "      <td>640.0</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>1407.0</td>\n",
       "      <td>2890.0</td>\n",
       "      <td>2520.0</td>\n",
       "      <td>1594.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <th>71</th>\n",
       "      <td>2</td>\n",
       "      <td>460.0</td>\n",
       "      <td>785.0</td>\n",
       "      <td>944.0</td>\n",
       "      <td>2612.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>1447.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <th>77</th>\n",
       "      <td>2</td>\n",
       "      <td>604.0</td>\n",
       "      <td>930.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>2403.0</td>\n",
       "      <td>2122.0</td>\n",
       "      <td>1594.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <th>79</th>\n",
       "      <td>2</td>\n",
       "      <td>640.0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>2612.0</td>\n",
       "      <td>2073.0</td>\n",
       "      <td>1520.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <th>84</th>\n",
       "      <td>2</td>\n",
       "      <td>496.0</td>\n",
       "      <td>930.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>2542.0</td>\n",
       "      <td>2122.0</td>\n",
       "      <td>1594.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <th>89</th>\n",
       "      <td>2</td>\n",
       "      <td>568.0</td>\n",
       "      <td>1219.0</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>3028.0</td>\n",
       "      <td>2520.0</td>\n",
       "      <td>1741.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <th>90</th>\n",
       "      <td>2</td>\n",
       "      <td>604.0</td>\n",
       "      <td>931.0</td>\n",
       "      <td>944.0</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>1924.0</td>\n",
       "      <td>1300.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1322 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 landcover    blue   green     red     nir  \\\n",
       "study_area scene_num row column                                              \n",
       "mtbarker   1         38  105             1   247.0   434.0   344.0  4094.0   \n",
       "                     33  104             1   420.0   717.0   560.0  3877.0   \n",
       "                     32  105             1   420.0   677.0   524.0  3833.0   \n",
       "                     28  106             1   362.0   717.0   524.0  3877.0   \n",
       "                     26  102             1   324.0   636.0   452.0  4483.0   \n",
       "                     24  100             1   305.0   555.0   380.0  4093.0   \n",
       "                     25  109             1   324.0   596.0   416.0  4527.0   \n",
       "                         116             1   363.0   677.0   524.0  3746.0   \n",
       "                     29  148             1   439.0   958.0   739.0  5734.0   \n",
       "                         150             1   362.0   757.0   560.0  5390.0   \n",
       "                     31  152             1   458.0  1038.0   847.0  5691.0   \n",
       "                     35  151             1   382.0   838.0   632.0  5820.0   \n",
       "                     37  150             1   458.0   918.0   739.0  5906.0   \n",
       "                     68  136             1   477.0   958.0   811.0  5305.0   \n",
       "                     61  144             1   477.0   918.0   811.0  4874.0   \n",
       "                     58  145             1   515.0  1038.0   883.0  5219.0   \n",
       "                     55  147             1     NaN     NaN     NaN     NaN   \n",
       "                     56  140             1   477.0   838.0   739.0  4571.0   \n",
       "                     59  137             1   477.0   998.0   811.0  4917.0   \n",
       "                     54  165             1   382.0   758.0   488.0  5563.0   \n",
       "                     55  167             1   420.0   758.0   524.0  5606.0   \n",
       "                     53  168             1   363.0   677.0   524.0  5348.0   \n",
       "                     52  167             1   363.0   758.0   560.0  5262.0   \n",
       "                     51  165             1   363.0   757.0   524.0  5348.0   \n",
       "                     49  165             1   401.0   798.0   596.0  5348.0   \n",
       "                     47  166             1   382.0   757.0   632.0  5391.0   \n",
       "                     46  169             1   382.0   717.0   632.0  5262.0   \n",
       "                     50  169             1   363.0   758.0   560.0  5606.0   \n",
       "                     60  166             1   305.0   596.0   416.0  5434.0   \n",
       "                     59  165             1   363.0   556.0   416.0  5348.0   \n",
       "...                                    ...     ...     ...     ...     ...   \n",
       "swsyd      830       117 10              1   170.0   347.0   362.0  2402.0   \n",
       "                     124 7               1   170.0   347.0   303.0  2402.0   \n",
       "                     105 56              1   170.0   421.0   245.0  2402.0   \n",
       "                     97  56              1   133.0   347.0   303.0  2262.0   \n",
       "                     210 18              1   243.0   713.0   654.0  3306.0   \n",
       "                     204 21              1   316.0   713.0   712.0  2890.0   \n",
       "                     192 27              1   352.0   858.0   770.0  3374.0   \n",
       "                     191 25              1   424.0   858.0   770.0  3651.0   \n",
       "                     238 21              1   496.0  1003.0  1061.0  3029.0   \n",
       "                     239 33              1   352.0   858.0   829.0  3859.0   \n",
       "                     233 41              1   243.0   494.0   479.0  2891.0   \n",
       "                     201 262             1   281.0   641.0   363.0  4821.0   \n",
       "                     194 258             1   281.0   714.0   363.0  5369.0   \n",
       "                     189 253             1   317.0   713.0   479.0  5573.0   \n",
       "                     175 250             1   317.0   713.0   362.0  6254.0   \n",
       "                     234 177             2     NaN     NaN     NaN     NaN   \n",
       "                     235 180             2     NaN     NaN     NaN     NaN   \n",
       "                     230 129             2  1245.0  1722.0  1869.0  2334.0   \n",
       "                     225 123             2  2386.0  3131.0  3350.0  3514.0   \n",
       "                     214 125             2  3489.0  4232.0  4193.0  4134.0   \n",
       "                     213 130             2  1174.0  1722.0  1581.0  1707.0   \n",
       "                     215 55              2   961.0  1364.0  1465.0  1777.0   \n",
       "                     213 53              2   961.0  1435.0  1523.0  3029.0   \n",
       "                     192 76              2   640.0  1075.0  1407.0  2890.0   \n",
       "                     188 71              2   460.0   785.0   944.0  2612.0   \n",
       "                     178 77              2   604.0   930.0  1118.0  2403.0   \n",
       "                     175 79              2   640.0  1003.0  1234.0  2612.0   \n",
       "                     177 84              2   496.0   930.0  1118.0  2542.0   \n",
       "                     182 89              2   568.0  1219.0  1350.0  3028.0   \n",
       "                     188 90              2   604.0   931.0   944.0  1985.0   \n",
       "\n",
       "                                  swir1   swir2  \n",
       "study_area scene_num row column                  \n",
       "mtbarker   1         38  105     1305.0   558.0  \n",
       "                     33  104     2284.0  1088.0  \n",
       "                     32  105     2284.0  1044.0  \n",
       "                     28  106     2192.0   999.0  \n",
       "                     26  102     2009.0   823.0  \n",
       "                     24  100     1886.0   823.0  \n",
       "                     25  109     1948.0   867.0  \n",
       "                         116     2131.0   999.0  \n",
       "                     29  148     2040.0   867.0  \n",
       "                         150     1917.0   823.0  \n",
       "                     31  152     2009.0   911.0  \n",
       "                     35  151     2040.0   867.0  \n",
       "                     37  150     2070.0   867.0  \n",
       "                     68  136     2040.0   867.0  \n",
       "                     61  144     2315.0  1000.0  \n",
       "                     58  145     2254.0  1000.0  \n",
       "                     55  147        NaN     NaN  \n",
       "                     56  140     2346.0  1044.0  \n",
       "                     59  137     2223.0  1000.0  \n",
       "                     54  165     1704.0   647.0  \n",
       "                     55  167     1704.0   647.0  \n",
       "                     53  168     1734.0   647.0  \n",
       "                     52  167     1673.0   735.0  \n",
       "                     51  165     1642.0   602.0  \n",
       "                     49  165     1704.0   735.0  \n",
       "                     47  166     1765.0   779.0  \n",
       "                     46  169     1856.0   867.0  \n",
       "                     50  169     1734.0   735.0  \n",
       "                     60  166     1551.0   647.0  \n",
       "                     59  165     1489.0   558.0  \n",
       "...                                 ...     ...  \n",
       "swsyd      830       117 10      1028.0   343.0  \n",
       "                     124 7       1028.0   637.0  \n",
       "                     105 56      1376.0   563.0  \n",
       "                     97  56      1128.0   416.0  \n",
       "                     210 18      2024.0  1226.0  \n",
       "                     204 21      2322.0  1300.0  \n",
       "                     192 27      2322.0  1226.0  \n",
       "                     191 25      2421.0  1226.0  \n",
       "                     238 21      3416.0  1963.0  \n",
       "                     239 33      2820.0  1300.0  \n",
       "                     233 41      1378.0   711.0  \n",
       "                     201 262     1924.0   784.0  \n",
       "                     194 258     2122.0   784.0  \n",
       "                     189 253     2271.0   711.0  \n",
       "                     175 250     2023.0   784.0  \n",
       "                     234 177        NaN     NaN  \n",
       "                     235 180        NaN     NaN  \n",
       "                     230 129     2819.0  2699.0  \n",
       "                     225 123     3961.0  3508.0  \n",
       "                     214 125     5450.0  5274.0  \n",
       "                     213 130     2968.0  3066.0  \n",
       "                     215 55      2024.0  2110.0  \n",
       "                     213 53      2620.0  2110.0  \n",
       "                     192 76      2520.0  1594.0  \n",
       "                     188 71      2023.0  1447.0  \n",
       "                     178 77      2122.0  1594.0  \n",
       "                     175 79      2073.0  1520.0  \n",
       "                     177 84      2122.0  1594.0  \n",
       "                     182 89      2520.0  1741.0  \n",
       "                     188 90      1924.0  1300.0  \n",
       "\n",
       "[1322 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load previous training data\n",
    "# by taking the last (ie most recent if the standard date is attached to the file) .pkl file\n",
    "files = os.listdir('../')\n",
    "pickles = []\n",
    "for file in files:\n",
    "    if file[-3::] == 'pkl':\n",
    "        pickles.append(file)\n",
    "trainingdata = pd.read_pickle('../' + pickles[-1])\n",
    "\n",
    "# view the current status\n",
    "trainingdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 30,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## Other broad scope variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# easier to work with integers than strings, so map the planned training classes to integers\n",
    "landcover = {'vegetation':1,'urban':2,'earth':3,'water':4}\n",
    "# range of pretermined study areas to use as sources for training data\n",
    "study_areas = ['mtbarker', 'swmelb', 'gunghalin', 'goldengrove', 'molonglo', 'nperth', 'swbris', 'swsyd', 'custom']\n",
    "\n",
    "# not in broad scope yet\n",
    "sat_bands = ['blue','green','red','nir','swir1','swir2']\n",
    "dc_bands = sat_bands.copy() + ['cloud_mask']\n",
    "\n",
    "colours = ['r', 'b', 'm', 'c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 50,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## function: makeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def makeClassifier(model):\n",
    "    if model == 'svc':   # support vector classifier\n",
    "        clf = svm.SVC()\n",
    "    if model == 'rfc':   # random forest classifier\n",
    "        clf = RandomForestClassifier()\n",
    "    # scale and normalize the data\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler().fit(trainingdata.dropna(axis=0, how = 'any')[sat_bands].values)\n",
    "    X_fortraining = scaler.transform(trainingdata.dropna(axis=0, how = 'any')[sat_bands].values)\n",
    "    X_fortraining = preprocessing.normalize(X_fortraining)\n",
    "\n",
    "    # train the model\n",
    "    clf.fit(X_fortraining,trainingdata.dropna(axis=0, how = 'any')['landcover'].values)\n",
    "    return clf, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 59,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Formatting Remaining Data for Classification & Classifying It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def reformatAndClassify(study_area, clf, scaler):\n",
    "    # get the data\n",
    "    data = getData(study_area)\n",
    "    \n",
    "    #setting up the xarray to store the results for easy plotting later\n",
    "    for newband in ['landcover','predicted_landcover']:\n",
    "        temp = data[:,:,:].sel(band='red').copy()\n",
    "        temp.band.values = newband\n",
    "        temp.values[:] = np.nan\n",
    "        data = xr.concat([data, temp], dim='band')\n",
    "\n",
    "    # useful variable for down the track\n",
    "    shape = data.values.shape\n",
    "\n",
    "    # record the start time of the process\n",
    "    start = datetime.datetime.now()\n",
    "    print('Classifying ' + study_area + ' at ' + str(start))\n",
    "    \n",
    "    # classify one scene at a time, save the results to the xarray\n",
    "    # # rewrite this to work pixel by pixel on timeseries, will tie in better to change detection\n",
    "    for scene in range(shape[2]):\n",
    "\n",
    "        # setting up dataframe multilevel indexes\n",
    "        col_idx = list(range(shape[0])) * shape[1]\n",
    "        row_idx = []\n",
    "        for i in range(shape[1]):\n",
    "            row_idx += [i] * shape[0]\n",
    "        scene_idx = [scene] * (shape[0] * shape[1])\n",
    "\n",
    "        # reshape the data into a 2D flat array for scikit learn\n",
    "        flattened = data[:,:,scene].sel(band=dc_bands).values.reshape(shape[0] * shape[1], len(dc_bands))\n",
    "\n",
    "        # add the data to a new DataFrame, set up the columns and index\n",
    "        alldata = pd.DataFrame(flattened)\n",
    "        alldata.columns = dc_bands\n",
    "        alldata['row'] = row_idx\n",
    "        alldata['column'] = col_idx\n",
    "        alldata['scene_num'] = scene_idx\n",
    "        alldata['study_area'] = study_area\n",
    "        alldata = alldata.set_index(['study_area','scene_num','row','column'])\n",
    "\n",
    "        # join in the training data. This is a SQL left join, so only adds data to current study area\n",
    "        alldata = alldata.reset_index().join(trainingdata[['landcover']], on=trainingdata.index.names).set_index(alldata.index.names)\n",
    "\n",
    "        # reduce alldata down to valid pixels (ie cloudmask), and non-training pixels (ie landcover is still NaN)\n",
    "        datatoclassify = alldata[alldata['cloud_mask'] == 0 & np.isnan(alldata['landcover'])].copy()\n",
    "        # remove pixels with a np.nan as scikit-learn doesn't like them. Only keep spectral bands\n",
    "        datatoclassify = datatoclassify[np.isnan(datatoclassify['landcover'])][sat_bands]\n",
    "        # cast these relevant columns into a numpy array\n",
    "        datatoclassify_np = np.array(datatoclassify)\n",
    "\n",
    "        # to deal with an entirely clouded scene\n",
    "        if len(datatoclassify_np) == 0:\n",
    "            continue\n",
    "\n",
    "        # scale and normalize the data so it resembles the training data.\n",
    "        datatoclassify_np = scaler.transform(datatoclassify_np)\n",
    "        datatoclassify_np = preprocessing.normalize(datatoclassify_np)\n",
    "\n",
    "        # results of predict() are a 1 dimensional numpy array of the same length as the input data\n",
    "        # assign these results to a new column in the dataframe\n",
    "        datatoclassify['predicted_landcover'] = clf.predict(datatoclassify_np)\n",
    "\n",
    "        # SQL left join the results back onto the original data\n",
    "        alldata = alldata.reset_index().join(datatoclassify[['predicted_landcover']], on=trainingdata.index.names).set_index(alldata.index.names)\n",
    "\n",
    "        #save the training data and classification results into the Xarray\n",
    "        data[:,:,scene].loc[dict(band='landcover')] = alldata['landcover'].values.reshape(shape[0],shape[1])\n",
    "        data[:,:,scene].loc[dict(band='predicted_landcover')] = alldata['predicted_landcover'].values.reshape(shape[0],shape[1])\n",
    "\n",
    "    \n",
    "    print('Time taken to classify ' + study_area + ' was ' + str(datetime.datetime.now() - start))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 63,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Time Series of Classifications into Change Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 63,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## function: dateStringToFloat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def dateStringToFloat(s):\n",
    "    year = int(s[0:4])\n",
    "    month = int(s[5:7])\n",
    "    \n",
    "    if month in [1,2,3]:\n",
    "        year += 0.125\n",
    "    elif month in [4,5,6]:\n",
    "        year += 0.375\n",
    "    elif month in [7,8,9]:\n",
    "        year += 0.625\n",
    "    else:\n",
    "        year += 0.875\n",
    "    \n",
    "    return year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: modalFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modalFilter(df, column, index, span = 10):\n",
    "    if index > (len(df) - span):\n",
    "        mode_arr = df[column][index::].mode().values\n",
    "    else:\n",
    "        mode_arr = df[column][index : index + span].mode().values\n",
    "    if len(mode_arr) > 1:\n",
    "        heirarchy = [1, 3, 4, 2] # vegetation, earth, water, urban\n",
    "        max_priority = 4\n",
    "        i_max = 0\n",
    "        for i in mode_arr:\n",
    "            if heirarchy.index(i) < max_priority:\n",
    "                max_priority = heirarchy.index(i)\n",
    "                i_max = i\n",
    "        return i_max \n",
    "    else:\n",
    "        return mode_arr[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: changeDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeDetector(study_area, data, clftype):\n",
    "    \"\"\"\n",
    "    Description of current decision rule for assigning urban change:\n",
    "     calculate the mode over each 10 scenes, and assign that value to the first scene of the 10\n",
    "        if the first two modal values are both urban, assume already developed and move to next pixel\n",
    "        if not, calculate the mode of the modes, over each 3 modes, assigning the value to the first\n",
    "     find the first mode of modes that is urban for that pixel\n",
    "        use that data (MOM_change date) to find the group of 3 modes that contributed to the mode of modes\n",
    "        within the individual classifications for that pixel within that group of 30 classifications\n",
    "           if there is an instance of 2 classifications in a row being urban, use the date of the first of those\n",
    "           if not, simply use the first instance of an urban pixel in that range of 30\n",
    "\n",
    "    Ideas for improvement:\n",
    "       - Deal with water pixels (eg if >25% of classifications are water, ignore)\n",
    "\n",
    "    Limitations of Method:\n",
    "       - Many of them!\n",
    "       - Algorithm needs first 20 to establish baseline - so it can't detect early change\n",
    "       - Algorithm needs last 30 to build mode of modes - so it can't detect most recent change\n",
    "       - It's very slow - mulitple nested loops. Main speed up would come from being able to do\n",
    "          Modal filter as a df.apply(modalFilter) \"\"\"\n",
    "    # setting up the results raster\n",
    "    shape = data.shape\n",
    "    changedates_arr = np.zeros((shape[1], shape[0]), dtype=np.float32)\n",
    "    changedates_arr[changedates_arr == 0] = np.nan\n",
    "\n",
    "    # variables for the modal filtering\n",
    "    mode_span = 10\n",
    "    MoM_span = 3\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    print('Change detection for ' + study_area + ' start time: ' + str(start))\n",
    "\n",
    "    # a very slow nested loop, keen to remove if possible\n",
    "#     for x in range(shape[0]):\n",
    "#         for y in range(shape[1]):\n",
    "    for x in range(100,131,1):\n",
    "        for y in range(100,131,1):\n",
    "            \n",
    "            # make a dataframe of the time-series of the predicted classifications\n",
    "            pixeldata = pd.DataFrame(data[x, y, :].sel(band='predicted_landcover').values, index = data.date.values, columns = ['predicted_landcover'])\n",
    "    \n",
    "            # make a new column to store mode in\n",
    "            pixeldata['mode1'] = np.nan\n",
    "            pixeldata['mode_of_modes'] = np.nan\n",
    "            mode_loc = pixeldata.columns.get_loc('mode1')\n",
    "\n",
    "            # remove any rows with NaNs\n",
    "            pixeldata = pixeldata[~np.isnan(pixeldata['predicted_landcover'])]\n",
    "            \n",
    "            # remove any possibility of duplicate dates (based on an issue with swmelbourne study area)\n",
    "            pixeldata = pixeldata[~pixeldata.index.duplicated(keep = 'first')]\n",
    "\n",
    "            # third level of nested of loops :(\n",
    "            # find mode of each 10 scenes, and store at first index of range\n",
    "            # for example the mode of scenes 10-19 will be stored in row 10.\n",
    "            for row in range(0, len(pixeldata), mode_span):\n",
    "                pixeldata.iloc[row, mode_loc] = modalFilter(pixeldata, 'predicted_landcover', row, span = mode_span)\n",
    "\n",
    "            # if either of the first two modes are urban, assume pixel is already urban at start of landsat archive \n",
    "            if (pixeldata[~np.isnan(pixeldata['mode1'])].iloc[0:2].values ==  2).any():\n",
    "                continue\n",
    "\n",
    "            # view or slice of data with modes\n",
    "            modes = pixeldata[~np.isnan(pixeldata['mode1'])]\n",
    "\n",
    "            # applying modal filter to the modes, to create mode of modes\n",
    "            # save the result in the pixeldata dataframe\n",
    "            for row in range(0,len(modes),MoM_span):\n",
    "                pixeldata.loc[modes.iloc[row].name, 'mode_of_modes'] = modalFilter(modes, 'mode1', row, span = MoM_span)\n",
    "\n",
    "            # decision criteria\n",
    "            if len(pixeldata[pixeldata['mode_of_modes'] == 2]) > 0:\n",
    "                MoM_changedate = pixeldata[pixeldata['mode_of_modes'] == 2].iloc[0].name\n",
    "                M_ss = pixeldata.loc[MoM_changedate::]\n",
    "                M_changedate = M_ss[M_ss['mode1'] == 2].iloc[0].name\n",
    "                M_changedate_loc = pixeldata.index.get_loc(M_changedate)\n",
    "                pix_ss = pixeldata.iloc[M_changedate_loc - mode_span : M_changedate_loc + mode_span]\n",
    "                twoinarow = pix_ss[(pix_ss['predicted_landcover'] == pix_ss['predicted_landcover'].shift(-1)) & \n",
    "                                    (pix_ss['predicted_landcover'][pix_ss['predicted_landcover'] == 2])]\n",
    "                if len(twoinarow) > 0:\n",
    "                    changedate = twoinarow.iloc[0].name\n",
    "                else:\n",
    "                    changedate = pix_ss[pix_ss['predicted_landcover'] == 2].iloc[0].name\n",
    "                changedates_arr[y, x] = dateStringToFloat(changedate)\n",
    "\n",
    "    # print how long the modal filtering and change detection took                \n",
    "    print(study_area + ' processing time: ' + str(datetime.datetime.now() - start))           \n",
    "\n",
    "    # save the results to a .pkl for future access\n",
    "    results_save_location = '../' + study_area + '/changeresults_' + clftype + '_noVector_origMode_allTD.pkl'\n",
    "    changedates_arr.dump(results_save_location)  \n",
    "    print('Results have been saved to', results_save_location, '\\n')\n",
    "    return changedates_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Mt Barker Subset, SVC, All Training Data, Modal Filter2 Function\n",
      "All Study Areas Loop Commenced at: 2018-01-23 09:29:13.921388\n",
      "\n",
      "Loading data from: ../mtbarker/\n",
      "Classifying mtbarker at 2018-01-23 09:29:17.223772\n",
      "Time taken to classify mtbarker was 0:05:43.016567\n",
      "Change detection for mtbarker start time: 2018-01-23 09:35:00.247228\n",
      "mtbarker processing time: 0:01:01.374648\n",
      "Results have been saved to ../mtbarker/changeresults_svc_noVector_allTD.pkl \n",
      "\n",
      "\n",
      "All study areas loop finished at: 2018-01-23 09:36:01.652192\n"
     ]
    }
   ],
   "source": [
    "bigrunstart = datetime.datetime.now()\n",
    "print('Testing Mt Barker Subset, SVC, All Training Data, Modal Filter2 Function')\n",
    "print('All Study Areas Loop Commenced at: ' + str(bigrunstart) + '\\n')\n",
    "clftype = 'svc'\n",
    "study_area = 'mtbarker'\n",
    "\n",
    "clf, scaler = makeClassifier(clftype)  # valid options are 'rfc' and 'svc'\n",
    "classified_data = reformatAndClassify(study_area, clf, scaler)\n",
    "changeDetector(study_area, classified_data, clftype)\n",
    "    \n",
    "print('\\nAll study areas loop finished at: ' + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Mt Barker Subset, SVC, All Training Data, Modal Filter Function\n",
      "All Study Areas Loop Commenced at: 2018-01-23 09:45:10.362538\n",
      "\n",
      "Change detection for mtbarker start time: 2018-01-23 09:45:10.367161\n",
      "mtbarker processing time: 0:01:41.717646\n",
      "Results have been saved to ../mtbarker/changeresults_svc_noVector_origMode_allTD.pkl \n",
      "\n",
      "\n",
      "All study areas loop finished at: 2018-01-23 09:46:52.102286\n"
     ]
    }
   ],
   "source": [
    "bigrunstart = datetime.datetime.now()\n",
    "print('Testing Mt Barker Subset, SVC, All Training Data, Modal Filter Function')\n",
    "print('All Study Areas Loop Commenced at: ' + str(bigrunstart) + '\\n')\n",
    "clftype = 'svc'\n",
    "study_area = 'mtbarker'\n",
    "\n",
    "# clf, scaler = makeClassifier(clftype)  # valid options are 'rfc' and 'svc'\n",
    "# classified_data = reformatAndClassify(study_area, clf, scaler)\n",
    "changeDetector(study_area, classified_data, clftype)\n",
    "    \n",
    "print('\\nAll study areas loop finished at: ' + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Mt Barker Subset, SVC, All Training Data, vectorModal Filter Function\n",
      "All Study Areas Loop Commenced at: 2018-01-23 10:36:26.114457\n",
      "\n",
      "Change detection for mtbarker start time: 2018-01-23 10:36:26.122324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mtbarker processing time: 0:06:58.388537\n",
      "Results have been saved to ../mtbarker/changeresults_svc_Vector_allTD.pkl \n",
      "\n",
      "\n",
      "All study areas loop finished at: 2018-01-23 10:43:25.461238\n"
     ]
    }
   ],
   "source": [
    "bigrunstart = datetime.datetime.now()\n",
    "print('Testing Mt Barker Subset, SVC, All Training Data, vectorModal Filter Function')\n",
    "print('All Study Areas Loop Commenced at: ' + str(bigrunstart) + '\\n')\n",
    "clftype = 'svc'\n",
    "study_area = 'mtbarker'\n",
    "\n",
    "# clf, scaler = makeClassifier(clftype)  # valid options are 'rfc' and 'svc'\n",
    "# classified_data = reformatAndClassify(study_area, clf, scaler)\n",
    "changeDetector(study_area, classified_data, clftype)\n",
    "    \n",
    "print('\\nAll study areas loop finished at: ' + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 67,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## Looping Through All Study Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 12,
        "hidden": false,
        "row": 67,
        "width": 4
       },
       "report_default": {}
      }
     }
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Study Areas Loop Commenced at: 2018-01-22 17:07:15.649458\n",
      "\n",
      "Loading data from: ../mtbarker/\n",
      "Classifying mtbarker at 2018-01-22 17:07:26.512225\n",
      "Time taken to classify mtbarker was 0:04:42.075674\n",
      "Change detection for mtbarker start time: 2018-01-22 17:12:08.594760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mtbarker processing time: 3:09:10.697211\n",
      "Results have been saved to ../mtbarker/changeresults_svc_allTD.pkl \n",
      "\n",
      "Loading data from: ../mtbarker/\n",
      "Classifying mtbarker at 2018-01-22 20:21:25.088647\n",
      "Time taken to classify mtbarker was 0:01:50.547736\n",
      "Change detection for mtbarker start time: 2018-01-22 20:23:15.705677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mtbarker processing time: 3:17:50.248462\n",
      "Results have been saved to ../mtbarker/changeresults_rfc_allTD.pkl \n",
      "\n",
      "Loading data from: ../swmelb/\n",
      "Classifying swmelb at 2018-01-22 23:41:18.944259\n",
      "Time taken to classify swmelb was 0:09:54.414793\n",
      "Change detection for swmelb start time: 2018-01-22 23:51:13.378810\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'slice' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-17c3ef8e386a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmakeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclftype\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# valid options are 'rfc' and 'svc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mclassified_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreformatAndClassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy_area\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mchangeDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy_area\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassified_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclftype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nAll study areas loop finished at: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-eb787fe6a0e4>\u001b[0m in \u001b[0;36mchangeDetector\u001b[0;34m(study_area, data, clftype)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mM_changedate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM_ss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mM_ss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mode1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mM_changedate_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixeldata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM_changedate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mpix_ss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixeldata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mM_changedate_loc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmode_span\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mM_changedate_loc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmode_span\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 twoinarow = pix_ss[(pix_ss['predicted_landcover'] == pix_ss['predicted_landcover'].shift(-1)) & \n\u001b[1;32m     84\u001b[0m                                     (pix_ss['predicted_landcover'][pix_ss['predicted_landcover'] == 2])]\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'slice' and 'int'"
     ]
    }
   ],
   "source": [
    "bigrunstart = datetime.datetime.now()\n",
    "print('All Study Areas Loop Commenced at: ' + str(bigrunstart) + '\\n')\n",
    "clftype = 'svc'\n",
    "for study_area in ['mtbarker','swmelb','molonglo']:    # study_areas[0:-1]:\n",
    "    for clftype in ['svc','rfc']:\n",
    "        clf, scaler = makeClassifier(clftype)  # valid options are 'rfc' and 'svc'\n",
    "        classified_data = reformatAndClassify(study_area, clf, scaler)\n",
    "        changeDetector(study_area, classified_data, clftype)\n",
    "    \n",
    "print('\\nAll study areas loop finished at: ' + str(datetime.datetime.now()))\n",
    "\n",
    "# bigrunstart = datetime.datetime.now()\n",
    "# print('All Study Areas Loop Commenced at: ' + str(bigrunstart) + '\\n')\n",
    "\n",
    "# for clftype in ['rfc','svc']:\n",
    "#     for study_area in  ['mtbarker','swmelb','gunghalin']:    #study_areas[0:-1]:\n",
    "#         clf = makeClassifier(clftype)  #valid options are 'rfc' and 'svc'\n",
    "#         classified_data = reformatAndClassify(study_area, clf)\n",
    "#         changeDetector(study_area, classified_data, clftype)\n",
    "    \n",
    "# print('\\nAll study areas loop finished at: ' + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrunstart = datetime.datetime.now()\n",
    "print('All Study Areas Loop Commenced at: ' + str(bigrunstart) + '\\n')\n",
    "clftype = 'svc'\n",
    "study_area = 'mtbarker'\n",
    "\n",
    "clf, scaler = makeClassifier(clftype)  # valid options are 'rfc' and 'svc'\n",
    "classified_data = reformatAndClassify(study_area, clf, scaler)\n",
    "changeDetector(study_area, classified_data, clftype)\n",
    "    \n",
    "print('\\nAll study areas loop finished at: ' + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mtbarker',\n",
       " 'swmelb',\n",
       " 'gunghalin',\n",
       " 'goldengrove',\n",
       " 'molonglo',\n",
       " 'nperth',\n",
       " 'swbris',\n",
       " 'swsyd',\n",
       " 'custom']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 71,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Modal Filter Testing and Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Classify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (x: 171, y: 143, date: 672, band: 9)>\n",
       "array([[[[  3.561000e+03, ...,   1.000000e+00],\n",
       "         ..., \n",
       "         [  4.786000e+03, ...,   1.000000e+00]],\n",
       "\n",
       "        ..., \n",
       "        [[  3.421000e+03, ...,   1.000000e+00],\n",
       "         ..., \n",
       "         [  2.513000e+03, ...,   2.000000e+00]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[  5.215000e+03, ...,   1.000000e+00],\n",
       "         ..., \n",
       "         [  2.075000e+03, ...,   2.000000e+00]],\n",
       "\n",
       "        ..., \n",
       "        [[  9.481000e+03, ...,            nan],\n",
       "         ..., \n",
       "         [  3.785000e+03, ...,   1.000000e+00]]]], dtype=float32)\n",
       "Coordinates:\n",
       "  * x        (x) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ...\n",
       "  * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ...\n",
       "  * date     (date) <U10 '1987-09-20' '1987-10-06' '1987-11-07' '1987-12-25' ...\n",
       "  * band     (band) <U19 'nir' 'swir2' 'red' 'blue' 'swir1' 'green' ..."
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# study_area = 'mtbarker'\n",
    "\n",
    "# clf = makeClassifier('svc')  #valid options are 'rfc' and 'svc'\n",
    "# data = reformatAndClassify(study_area, clf)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Extract Pixel of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_landcover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1987-09-20</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987-10-06</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987-11-07</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987-12-25</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-01-10</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-01-26</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-02-11</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-03-30</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-04-15</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-05-17</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-06-02</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-07-20</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-08-05</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-08-21</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-09-06</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-09-22</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-10-08</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-10-24</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-11-09</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-12-27</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-01-28</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-02-13</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-03-01</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-03-17</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-04-02</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-05-20</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-06-05</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-08-23</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-09-08</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-10-10</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-07</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-15</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-23</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-31</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-08</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-24</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-03</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-11</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-19</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-27</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-04</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-12</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-20</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-28</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-14</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-30</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-07</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-15</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-01</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-17</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-25</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-02</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-18</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-03</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-19</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-27</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-05</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-13</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-21</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-29</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>672 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            predicted_landcover\n",
       "1987-09-20                  1.0\n",
       "1987-10-06                  1.0\n",
       "1987-11-07                  3.0\n",
       "1987-12-25                  2.0\n",
       "1988-01-10                  2.0\n",
       "1988-01-26                  3.0\n",
       "1988-02-11                  3.0\n",
       "1988-03-30                  3.0\n",
       "1988-04-15                  NaN\n",
       "1988-05-17                  NaN\n",
       "1988-06-02                  3.0\n",
       "1988-07-20                  NaN\n",
       "1988-08-05                  1.0\n",
       "1988-08-21                  NaN\n",
       "1988-09-06                  1.0\n",
       "1988-09-22                  1.0\n",
       "1988-10-08                  1.0\n",
       "1988-10-24                  1.0\n",
       "1988-11-09                  NaN\n",
       "1988-12-27                  NaN\n",
       "1989-01-28                  3.0\n",
       "1989-02-13                  NaN\n",
       "1989-03-01                  2.0\n",
       "1989-03-17                  3.0\n",
       "1989-04-02                  1.0\n",
       "1989-05-20                  1.0\n",
       "1989-06-05                  1.0\n",
       "1989-08-23                  1.0\n",
       "1989-09-08                  NaN\n",
       "1989-10-10                  1.0\n",
       "...                         ...\n",
       "2016-01-07                  2.0\n",
       "2016-01-15                  2.0\n",
       "2016-01-23                  NaN\n",
       "2016-01-31                  NaN\n",
       "2016-02-08                  3.0\n",
       "2016-02-24                  NaN\n",
       "2016-03-03                  2.0\n",
       "2016-03-11                  3.0\n",
       "2016-03-19                  NaN\n",
       "2016-03-27                  NaN\n",
       "2016-04-04                  2.0\n",
       "2016-04-12                  1.0\n",
       "2016-04-20                  3.0\n",
       "2016-04-28                  NaN\n",
       "2016-05-14                  NaN\n",
       "2016-05-30                  NaN\n",
       "2016-06-07                  NaN\n",
       "2016-06-15                  2.0\n",
       "2016-07-01                  3.0\n",
       "2016-07-17                  2.0\n",
       "2016-07-25                  3.0\n",
       "2016-08-02                  1.0\n",
       "2016-08-18                  NaN\n",
       "2016-09-03                  NaN\n",
       "2016-09-19                  2.0\n",
       "2016-09-27                  2.0\n",
       "2016-10-05                  3.0\n",
       "2016-10-13                  3.0\n",
       "2016-10-21                  NaN\n",
       "2016-10-29                  2.0\n",
       "\n",
       "[672 rows x 1 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mt barker pixel that definitely changed\n",
    "# pixeldata = pd.DataFrame(data[129, 39, :].sel(band='predicted_landcover').values, index = data.date.values, columns = ['predicted_landcover'])\n",
    "\n",
    "# mt barker pixel that was always urban\n",
    "# pixeldata = pd.DataFrame(data[40, 63, :].sel(band='predicted_landcover').values, index = data.date.values, columns = ['predicted_landcover'])\n",
    "\n",
    "# mt barker pixel that was exception causing\n",
    "pixeldata = pd.DataFrame(data[106, 120, :].sel(band='predicted_landcover').values, index = data.date.values, columns = ['predicted_landcover'])\n",
    "pixeldata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 71,
        "width": 4
       },
       "report_default": {}
      }
     }
    },
    "heading_collapsed": true
   },
   "source": [
    "# Viewing the Classification Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 75,
        "width": 4
       },
       "report_default": {}
      }
     }
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## function: drawClassifiedScene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def drawClassifiedScene(data, scene_num, alpha):\n",
    "    \n",
    "    # colour map included incase of need to display false colour or other in the future\n",
    "    # could change this to an ordereddict and remove the RGB list created below...?\n",
    "    colourmap = {'R':'red', 'G':'green', 'B':'blue'}\n",
    "    \n",
    "    # combine the data for the 3 bands to be displayed into a single numpy array\n",
    "    h = data.shape[1]\n",
    "    w = data.shape[0]\n",
    "    t = data.shape[2]\n",
    "    \n",
    "    if scene_num > (t -1):\n",
    "        scene_num = t - 1\n",
    "    RGB = ['R','G','B']\n",
    "    date = str(data[:,:,scene_num].date.values)\n",
    "    \n",
    "    # create array to store the RGB info in, and fill by looping through the colourmap variable\n",
    "    # note the .T at the end, because the data array is setup as a (x,y,t), but imshow works (y,x)\n",
    "    rawimg = np.zeros((h, w, 3), dtype=np.float32)\n",
    "    for i in range(len(RGB)):     \n",
    "        rawimg[:,:,i] = data[:,:,scene_num].sel(band=colourmap[RGB[i]]).T\n",
    "        \n",
    "    # equalizing for all bands together\n",
    "    # goal is to make is human interpretable\n",
    "    img_toshow = exposure.equalize_hist(rawimg, mask = np.isfinite(rawimg))    \n",
    "\n",
    "    # displaying the results and formatting the axes etc\n",
    "    plt.imshow(img_toshow)\n",
    "    ax = plt.gca()\n",
    "    ax.set_title('True Colour Landsat Scene, taken\\n' + date + ', over ' + study_area)\n",
    "    \n",
    "    # make the colour map for the cover classes\n",
    "    cmap = colors.ListedColormap(colours)\n",
    "    \n",
    "    # draw the classification results and the training data results\n",
    "    ax.imshow(data[:,:,scene_num].sel(band='predicted_landcover').values.T, cmap = cmap, alpha = alpha)\n",
    "    ax.imshow(data[:,:,scene_num].sel(band='landcover').values.T, cmap = cmap, alpha = 1)\n",
    "    \n",
    "    #draw a legend for the classification colours\n",
    "    legend_patches = []\n",
    "    for cover in landcover.keys():\n",
    "        legend_patches.append(mpatches.Patch(color = colours[landcover[cover]-1], label = cover))\n",
    "    ax.legend(handles = legend_patches)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 75,
        "width": 4
       },
       "report_default": {}
      }
     }
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## function: drawClassifiedPlots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def drawClassifiedPlots(data, scene_num, alpha):\n",
    "    \n",
    "    ax1 = plt.subplot2grid([2,4],[0,0], rowspan = 2, colspan = 2)\n",
    "    ax1.clear()\n",
    "    ax1 = drawClassifiedScene(data, scene_num, alpha = 0)\n",
    "    ax2 = plt.subplot2grid([2,4],[0,2], rowspan = 2, colspan = 2)\n",
    "    ax2.clear()\n",
    "    ax2 = drawClassifiedScene(data, scene_num, alpha)\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 79,
        "width": 4
       },
       "report_default": {}
      }
     }
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## function: check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def check(data, scene_num, alpha):\n",
    " \n",
    "    # control the figure size\n",
    "    fig = plt.figure(figsize=[12,7])\n",
    "    axs = fig.axes\n",
    "    plt.subplots_adjust(hspace = 0.6)\n",
    "    \n",
    "    # draw the figure\n",
    "    drawClassifiedPlots(data, scene_num, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 79,
        "width": 4
       },
       "report_default": {}
      }
     }
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Drawing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_scenes = data.shape[2]\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    interact(check,\n",
    "             data = fixed(data),\n",
    "             scene_num = IntSlider(value = 1, min = 0, max = num_scenes -1 ,description = \"Scene Number\"),\n",
    "             alpha= FloatSlider(value = 0.6, min = 0, max = 1, description = \"Classification Transparency\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 79,
        "width": 4
       },
       "report_default": {}
      }
     }
    },
    "heading_collapsed": true
   },
   "source": [
    "# Viewing the Change Detection Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 83,
        "width": 4
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "source": [
    "## drawAnalysedScene()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def drawAnalysedScene(data, scene_num, alpha, change_grid):\n",
    "    \n",
    "    # colour map included incase of need to display false colour or other in the future\n",
    "    # could change this to an ordereddict and remove the RGB list created below...?\n",
    "    colourmap = {'R':'red', 'G':'green', 'B':'blue'}\n",
    "    \n",
    "    # combine the data for the 3 bands to be displayed into a single numpy array\n",
    "    h = data.shape[1]\n",
    "    w = data.shape[0]\n",
    "    t = data.shape[2]\n",
    "    \n",
    "    if scene_num > (t -1):\n",
    "        scene_num = t - 1\n",
    "    RGB = ['R','G','B']\n",
    "    date = str(data[:,:,scene_num].date.values)\n",
    "    \n",
    "    # create array to store the RGB info in, and fill by looping through the colourmap variable\n",
    "    # note the .T at the end, because the data array is setup as a (x,y,t), but imshow works (y,x)\n",
    "    rawimg = np.zeros((h, w, 3), dtype=np.float32)\n",
    "    for i in range(len(RGB)):     \n",
    "        rawimg[:,:,i] = data[:,:,scene_num].sel(band=colourmap[RGB[i]]).T\n",
    "        \n",
    "    # equalizing for all bands together\n",
    "    # goal is to make is human interpretable\n",
    "    img_toshow = exposure.equalize_hist(rawimg, mask = np.isfinite(rawimg))    \n",
    "\n",
    "    # displaying the results and formatting the axes etc\n",
    "    plt.imshow(img_toshow)\n",
    "    ax = plt.gca()\n",
    "    ax.set_title('True Colour Landsat Scene, taken\\n' + date + ', over ' + study_area)\n",
    "    \n",
    "    # define the current colour map to display the change results raster properly\n",
    "    current_cmap = matplotlib.cm.get_cmap('Reds_r')\n",
    "    current_cmap.set_under('k', alpha=0.0)\n",
    "    current_cmap.set_over('r', alpha=1.0)\n",
    "    current_cmap.set_bad('k', alpha=0.0)  \n",
    "    \n",
    "    # draw the change detection results mask\n",
    "    ax.imshow(change_grid, alpha = alpha, interpolation='none', cmap = current_cmap, clim = [0.5, 0.6])\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 83,
        "width": 4
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "source": [
    "## drawAnalysedPlots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def drawAnalysedPlots(data, left_scene_num, left_alpha, left_change_grid,\n",
    "                      right_scene_num, right_alpha, right_change_grid):\n",
    "    \n",
    "    ax1 = plt.subplot2grid([2,4],[0,0], rowspan = 2, colspan = 2)\n",
    "    ax1.clear()\n",
    "    ax1 = drawAnalysedScene(data, left_scene_num, left_alpha, left_change_grid)\n",
    "    ax2 = plt.subplot2grid([2,4],[0,2], rowspan = 2, colspan = 2)\n",
    "    ax2.clear()\n",
    "    ax2 = drawAnalysedScene(data, right_scene_num, right_alpha, right_change_grid)\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 83,
        "width": 4
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "source": [
    "## function: results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def results(data, left_scene_num, left_alpha, left_change_grid,\n",
    "                      right_scene_num, right_alpha, right_change_grid):\n",
    " \n",
    "    # control the figure size\n",
    "    fig = plt.figure(figsize=[12,7])\n",
    "#     axs = fig.axes\n",
    "#     plt.subplots_adjust(hspace = 0.6)\n",
    "    \n",
    "    # draw the figure\n",
    "    drawAnalysedPlots(data, left_scene_num, left_alpha, left_change_grid,\n",
    "                      right_scene_num, right_alpha, right_change_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 87,
        "width": 4
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "source": [
    "## Open Existing Change Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get the results ready for comparison\n",
    "# no change should be np.NaN\n",
    "# chanage = 1\n",
    "# change = gdal.Open('../' + study_area + '/change_time.img').ReadAsArray()\n",
    "# change[change == 0] = np.nan\n",
    "\n",
    "# peterchange = change.copy()\n",
    "# peterchange[np.isfinite(peterchange)] = 1\n",
    "\n",
    "# mikechange = changedates_arr.copy()\n",
    "# mikechange[np.isfinite(mikechange)] = 1\n",
    "\n",
    "\n",
    "study_area = 'mtbarker'\n",
    "# data = getData(study_area)\n",
    "results_save_location = '../' + study_area + '/changeresults_svc_Vector_allTD.pkl'\n",
    "sml_training_res = np.load(results_save_location)\n",
    "sml_training_flag = sml_training_res.copy()\n",
    "sml_training_flag[np.isfinite(sml_training_flag)] = 1\n",
    "\n",
    "results_save_location = '../' + study_area + '/changeresults_svc_noVector_allTD.pkl'\n",
    "big_training_res = np.load(results_save_location)\n",
    "big_training_flag = big_training_res.copy()\n",
    "big_training_flag[np.isfinite(big_training_flag)] = 1\n",
    "\n",
    "num_scenes = data.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 87,
        "width": 4
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "source": [
    "## Draw Results Analysis Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8b4db7e21242ae830ec980edc4e84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='Scene Number', max=671), FloatSlider(value=0.6, description='Left Alpha', max=1.0), IntSlider(value=671, description='Scene Number', max=671), FloatSlider(value=0.6, description='Right Alpha', max=1.0), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    interact(results,\n",
    "             data = fixed(data),\n",
    "             left_scene_num = IntSlider(value = 1, min = 0, max = num_scenes -1 ,description = \"Scene Number\"),\n",
    "             left_alpha= FloatSlider(value = 0.6, min = 0, max = 1, description = \"Left Alpha\"),\n",
    "             left_change_grid = fixed(sml_training_flag),\n",
    "             right_scene_num = IntSlider(value = num_scenes - 1, min = 0, max = num_scenes - 1,description = \"Scene Number\"),\n",
    "             right_alpha= FloatSlider(value = 0.6, min = 0, max = 1, description = \"Right Alpha\"),\n",
    "             right_change_grid = fixed(big_training_flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.48214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f1c591660b8>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1d4c447278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "print(np.nanmean(big_training_res - sml_training_res))\n",
    "imshow(big_training_res - sml_training_res)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,   1.25,  11.5 ],\n",
       "       [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,   0.  ,   0.  ],\n",
       "       [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,   1.25,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,   0.  ,   0.  ],\n",
       "       [   nan,    nan,   0.  ,   0.  ,   0.  ,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,   0.  ,   0.75,    nan,    nan,    nan,  13.75,    nan,\n",
       "           nan,    nan,    nan,    nan,   0.  ,   0.  ,   0.  ],\n",
       "       [  1.5 ,   1.5 ,   0.  ,   1.  ,   0.  ,   0.  ,   0.  ,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,   0.  ,   0.  ,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan],\n",
       "       [  0.  ,   1.  ,   1.  ,   0.5 ,   0.  ,   7.  ,   8.5 ,   0.  ,\n",
       "         10.  ,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,   1.25,    nan,    nan,\n",
       "           nan,   1.  ,    nan,    nan,    nan,    nan,    nan],\n",
       "       [  0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   3.75,   0.  ,\n",
       "          0.  ,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,   0.  ,   0.  ,    nan,\n",
       "           nan,   0.  ,   0.  ,    nan,    nan,    nan,    nan],\n",
       "       [  0.  ,   1.75,   0.25,   0.  ,   0.  ,   3.75,   0.  ,   0.  ,\n",
       "          0.  ,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,   0.  ,   0.  ,   2.  ,   0.  ,\n",
       "          1.25,   0.  ,   0.  ,    nan,    nan,    nan,    nan],\n",
       "       [  0.  ,   4.25,   0.  ,   0.  ,   7.  ,   0.  ,   0.  ,   2.  ,\n",
       "         16.  ,    nan,    nan,    nan,    nan,    nan,   0.  ,    nan,\n",
       "           nan,    nan,    nan,    nan,   1.75,   1.75,   3.  ,   4.  ,\n",
       "          3.25,   0.  ,   0.  ,   0.  ,    nan,    nan,    nan],\n",
       "       [  0.  ,   0.  ,   0.5 ,   0.  ,   0.  ,   7.  ,   0.  ,   0.  ,\n",
       "          0.  ,   0.  ,    nan,    nan,    nan,   6.25,   1.75,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,   1.75,   2.75,   0.  ,\n",
       "          3.5 ,   0.  ,   1.  ,   9.  ,    nan,    nan,    nan],\n",
       "       [  0.  ,   0.  ,   0.  ,   1.  ,   3.75,   3.75,   0.  ,   0.  ,\n",
       "          0.  ,    nan,    nan,    nan,   0.  ,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,   1.75,   0.  ,   1.  ,\n",
       "          0.  ,   0.  ,   0.  ,    nan,    nan,    nan,    nan],\n",
       "       [  0.  ,   9.75,   0.  ,   4.25,   8.  ,   2.5 ,   0.  ,   0.  ,\n",
       "           nan,    nan,    nan,   1.  ,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,   0.  ,   0.  ,   0.  ,   0.5 ,\n",
       "          0.5 ,   0.  ,   3.5 ,    nan,    nan,    nan,   0.  ],\n",
       "       [  0.  ,  11.75,  11.5 ,   7.75,   2.75,   2.5 ,   0.  ,    nan,\n",
       "           nan,    nan,    nan,   0.  ,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,   0.  ,   0.  ,   0.  ,   0.  ,\n",
       "          0.5 ,   0.  ,   0.  ,   0.  ,   0.  ,    nan,    nan],\n",
       "       [  0.  ,   0.  ,  12.  ,    nan,   0.  ,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,   0.  ,   0.  ,   1.75,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,   2.  ,    nan,    nan,    nan,\n",
       "           nan,    nan,   0.  ,   0.  ,   0.  ,    nan,    nan],\n",
       "       [ 13.25,   0.  ,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,   0.  ,   0.  ,   0.  ,    nan,    nan,\n",
       "          1.75,   0.  ,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,   0.  ,    nan,    nan,    nan,    nan],\n",
       "       [ 16.75,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,   0.  ,   1.5 ,   0.  ,   0.  ,   0.  ,   0.25,\n",
       "          0.  ,   0.  ,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,   0.  ,    nan,    nan,    nan,    nan,    nan],\n",
       "       [  0.  ,   0.  ,    nan,    nan,    nan,    nan,   0.  ,   0.  ,\n",
       "          0.  ,   0.  ,   0.  ,   0.  ,   7.5 ,    nan,    nan,   0.  ,\n",
       "          0.  ,   0.  ,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan],\n",
       "       [  0.  ,   0.  ,    nan,    nan,    nan,   0.  ,   0.  ,   0.75,\n",
       "          0.  ,   0.  ,   0.  ,    nan,    nan,    nan,    nan,    nan,\n",
       "          1.5 ,   0.  ,    nan,    nan,    nan,   0.  ,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan],\n",
       "       [  1.25,   3.  ,    nan,    nan,    nan,   0.  ,   0.  ,   0.  ,\n",
       "          0.  ,   0.  ,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "          0.  ,   0.  ,    nan,    nan,   0.  ,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan],\n",
       "       [  0.  ,    nan,    nan,    nan,   0.  ,   0.75,   0.  ,   0.  ,\n",
       "          0.  ,    nan,    nan,   0.  ,   0.  ,    nan,    nan,    nan,\n",
       "          0.  ,    nan,   0.  ,   0.  ,   0.  ,   0.  ,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan],\n",
       "       [   nan,    nan,    nan,    nan,    nan,    nan,   1.5 ,   0.  ,\n",
       "           nan,    nan,   1.5 ,   1.  ,   0.75,    nan,    nan,    nan,\n",
       "           nan,   0.  ,    nan,    nan,    nan,   0.  ,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan],\n",
       "       [   nan,    nan,    nan,   0.  ,   0.  ,   0.  ,   0.  ,    nan,\n",
       "           nan,    nan,    nan,   1.  ,   2.  ,    nan,    nan,    nan,\n",
       "          0.  ,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan],\n",
       "       [   nan,    nan,    nan,   0.  ,    nan,  19.75,   0.  ,    nan,\n",
       "           nan,    nan,   0.  ,   1.  ,   1.  ,    nan,   1.5 ,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan],\n",
       "       [  9.5 ,  23.  ,    nan,    nan,    nan,   5.5 ,    nan,    nan,\n",
       "           nan,   0.  ,   0.  ,   0.  ,   1.5 ,   0.  ,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan],\n",
       "       [   nan,    nan,    nan,    nan,    nan,    nan,    nan,   0.  ,\n",
       "          0.  ,   0.  ,   0.5 ,   4.5 ,    nan,    nan,   0.  ,    nan,\n",
       "           nan,    nan,    nan,   0.  ,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan],\n",
       "       [   nan,    nan,    nan,   0.  ,    nan,    nan,   0.75,   0.25,\n",
       "          0.  ,   0.  ,   4.25,   0.  ,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,   0.  ,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan],\n",
       "       [   nan,    nan,    nan,   0.  ,   0.  ,   0.  ,    nan,   0.  ,\n",
       "          9.25,   0.  ,   0.  ,   0.  ,   0.  ,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,   0.  ,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan],\n",
       "       [   nan,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,\n",
       "          0.  ,   0.  ,   0.  ,   0.  ,   0.  ,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,   0.  ,    nan,    nan,   0.  ,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan],\n",
       "       [  0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,\n",
       "          0.  ,   2.25,    nan,   0.  ,   0.  ,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,   0.  ,   0.  ,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan],\n",
       "       [  2.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,\n",
       "          0.  ,   0.  ,   0.  ,   0.  ,   0.  ,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan],\n",
       "       [  0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,    nan,   1.5 ,\n",
       "          1.25,   0.  ,   0.  ,   0.  ,   0.  ,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "           nan,    nan,    nan,    nan,    nan,    nan,    nan]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change = (big_training_res - sml_training_res)[100:131,100:131]\n",
    "change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "heading_collapsed": true
   },
   "source": [
    "# Cut Stuff\n",
    "This is left over code from my attempts to speed up the change detection process. I managed to vectorize quite a lot of the process, but it was slower than the original version, so it is saved here in case someone else wants to have a look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## function: vectorMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def vectorMode(pixeldata, column, span):\n",
    "    \n",
    "    heirarchy = {1: 1, 2: 4, 3: 2, 4: 3} # class to priority, vegetation > earth > water > urban\n",
    "    reversedheirarchy = {1: 1, 2: 3, 3: 4, 4:2} # priority to class\n",
    "     \n",
    "    # create sequential group numbers with a group size = span\n",
    "    pixeldata['group'] = np.arange(len(pixeldata)) // span\n",
    "    # create new dataframe with group numbers as index and the mode of each group as the data.\n",
    "    # will make heirarchical index if more than 1 mode\n",
    "    groups = pixeldata.groupby('group').apply(pd.DataFrame.mode)\n",
    "    # make new column with the group modes mapped according to heirarchy\n",
    "    groups['mapped'] = groups[column].map(heirarchy)\n",
    "    # unstack heirarchical index\n",
    "    groups = groups['mapped'].unstack(level=-1)\n",
    "    # find the lowest priority number (highest priority) for each group\n",
    "    groups['lowest_priority'] = groups.min(axis = 1)\n",
    "    # remap priority rankings to landcover classes\n",
    "    groups['mode'] = groups['lowest_priority'].map(reversedheirarchy)\n",
    "    # join the highest priority landcover mode for each group back onto the main data table\n",
    "    pixeldata = pd.merge(pixeldata, groups[['mode']], how = 'left', left_on='group', right_index = True)\n",
    "    # NaNify so that only every span rows have a value\n",
    "    pixeldata.loc[(np.arange(len(pixeldata)) % span != 0),'mode'] = np.nan\n",
    "    \n",
    "    return pixeldata['mode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## changeDetector vectorMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def changeDetector(study_area, data, clftype):\n",
    "    \"\"\"description of current decision rule for assigning urban change:\n",
    "     calculate the mode over each 10 scenes, and assign that value to the first scene of the 10\n",
    "        if the first two modal values are both urban, assume already developed and move to next pixel\n",
    "        if not, calculate the mode of the modes, over each 3 modes, assigning the value to the first\n",
    "     find the first mode of modes that is urban for that pixel\n",
    "        use that data (MOM_change date) to find the group of 3 modes that contributed to the mode of modes\n",
    "        within the individual classifications for that pixel within that group of 30 classifications\n",
    "           if there is an instance of 2 classifications in a row being urban, use the date of the first of those\n",
    "           if not, simply use the first instance of an urban pixel in that range of 30\n",
    "\n",
    "    Ideas for improvement:\n",
    "       - Deal with water pixels (eg if >25% of classifications are water, ignore)\n",
    "\n",
    "    Limitations of Method:\n",
    "       - Many of them!\n",
    "       - Algorithm needs first 20 to establish baseline - so it can't detect early change\n",
    "       - Algorithm needs last 30 to build mode of modes - so it can't detect most recent change \"\"\"\n",
    "\n",
    "    # setting up the results raster\n",
    "    shape = data.shape\n",
    "    changedates_arr = np.zeros((shape[1], shape[0]), dtype=np.float32)\n",
    "    changedates_arr[changedates_arr == 0] = np.nan\n",
    "\n",
    "    # variables for the modal filtering\n",
    "    mode_span = 10\n",
    "    MoM_span = 3\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    print('Change detection for ' + study_area + ' start time: ' + str(start))\n",
    "\n",
    "    # a very slow nested loop, keen to remove if possible\n",
    "#     for x in range(shape[0]):\n",
    "#         for y in range(shape[1]): \n",
    "    for x in range(100,131,1):\n",
    "        for y in range(100,131,1):\n",
    "            \n",
    "            # make a dataframe of the time-series of the predicted classifications\n",
    "            pixeldata = pd.DataFrame(data[x, y, :].sel(band='predicted_landcover').values, index = data.date.values, columns = ['predicted_landcover'])\n",
    "    \n",
    "            # remove any rows with NaNs\n",
    "            pixeldata = pixeldata[~np.isnan(pixeldata['predicted_landcover'])]\n",
    "\n",
    "            # remove any possibility of duplicate dates (based on an issue with swmelbourne study area)\n",
    "            pixeldata = pixeldata[~pixeldata.index.duplicated(keep = 'first')]\n",
    "\n",
    "            # calculate mode of each group of 10 rows\n",
    "            pixeldata['mode1'] = vectorMode(pixeldata, 'predicted_landcover', 10)\n",
    "\n",
    "            # if either of the first two modes are urban, assume pixel is already urban at start of landsat archive \n",
    "            if (pixeldata[~np.isnan(pixeldata['mode1'])].iloc[0:2].values ==  2).any():\n",
    "                continue\n",
    "\n",
    "            # view or slice of data with modes\n",
    "            modes = pixeldata[~np.isnan(pixeldata['mode1'])]\n",
    "\n",
    "            # third level of nested of loops :(\n",
    "            # applying modal filter to the modes, to create mode of modes\n",
    "            # save the result in the pixeldata dataframe\n",
    "            pixeldata['mode_of_modes'] = vectorMode(pixeldata[['mode1']],'mode1', 3)\n",
    "\n",
    "            # decision criteria\n",
    "            if len(pixeldata[pixeldata['mode_of_modes'] == 2]) > 0:\n",
    "                MoM_changedate = pixeldata[pixeldata['mode_of_modes'] == 2].iloc[0].name\n",
    "                M_ss = pixeldata.loc[MoM_changedate::]\n",
    "                M_changedate = M_ss[M_ss['mode1'] == 2].iloc[0].name\n",
    "                M_changedate_loc = pixeldata.index.get_loc(M_changedate)\n",
    "                pix_ss = pixeldata.iloc[M_changedate_loc - mode_span : M_changedate_loc + mode_span]\n",
    "                twoinarow = pix_ss[(pix_ss['predicted_landcover'] == pix_ss['predicted_landcover'].shift(-1)) & \n",
    "                                    (pix_ss['predicted_landcover'][pix_ss['predicted_landcover'] == 2])]\n",
    "                if len(twoinarow) > 0:\n",
    "                    changedate = twoinarow.iloc[0].name\n",
    "                else:\n",
    "                    changedate = pix_ss[pix_ss['predicted_landcover'] == 2].iloc[0].name\n",
    "                changedates_arr[y, x] = dateStringToFloat(changedate)\n",
    "\n",
    "    # print how long the modal filtering and change detection took                \n",
    "    print(study_area + ' processing time: ' + str(datetime.datetime.now() - start))           \n",
    "\n",
    "    # save the results to a .pkl for future access\n",
    "    results_save_location = '../' + study_area + '/changeresults_' + clftype + '_Vector_allTD.pkl'\n",
    "    changedates_arr.dump(results_save_location)  \n",
    "    print('Results have been saved to', results_save_location, '\\n')\n",
    "    return changedates_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## applyChangeDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def applyChangeDetector(obj):\n",
    "    \"\"\"\n",
    "            description of current decision rule for assigning urban change:\n",
    "             calculate the mode over each 10 scenes, and assign that value to the first scene of the 10\n",
    "                if the first two modal values are both urban, assume already developed and move to next pixel\n",
    "                if not, calculate the mode of the modes, over each 3 modes, assigning the value to the first\n",
    "             find the first mode of modes that is urban for that pixel\n",
    "                use that data (MOM_change date) to find the group of 3 modes that contributed to the mode of modes\n",
    "                within the individual classifications for that pixel within that group of 30 classifications\n",
    "                   if there is an instance of 2 classifications in a row being urban, use the date of the first of those\n",
    "                   if not, simply use the first instance of an urban pixel in that range of 30\n",
    "\n",
    "            Ideas for improvement:\n",
    "               - Deal with water pixels (eg if >25% of classifications are water, ignore)\n",
    "\n",
    "            Limitations of Method:\n",
    "               - Many of them!\n",
    "               - Algorithm needs first 20 to establish baseline - so it can't detect early change\n",
    "               - Algorithm needs last 30 to build mode of modes - so it can't detect most recent change\n",
    "               - It's very slow - mulitple nested loops. Main speed up would come from being able to do\n",
    "                  Modal filter as a df.apply(modalFilter)\"\"\"\n",
    "#     print(obj.xy.values)\n",
    "    # make a DataFrame of the data to work with\n",
    "    pixeldata = pd.DataFrame(obj.values, index = obj.date.values, columns = ['predicted_landcover'])\n",
    "\n",
    "    # drop any rows with nulls\n",
    "    pixeldata = pixeldata.dropna(axis = 0, how='any')  \n",
    "    # drop any duplicates\n",
    "    pixeldata = pixeldata[~pixeldata.index.duplicated(keep = 'first')]\n",
    "    \n",
    "    # variables for the modal filtering\n",
    "    mode_span = 10\n",
    "    MoM_span = 3\n",
    "\n",
    "    # calculate mode of each group of 10 rows\n",
    "    pixeldata['mode1'] = vectorMode(pixeldata, 'predicted_landcover', mode_span)\n",
    "\n",
    "    # if either of the first two modes are urban, assume pixel is already urban at start of landsat archive \n",
    "    if (pixeldata[~np.isnan(pixeldata['mode1'])].iloc[0:2].values ==  2).any():\n",
    "        changedate = np.nan\n",
    "#         print(\"branch 1\")\n",
    "        return xr.DataArray([changedate], dims = 'xy')\n",
    "\n",
    "    # view or slice of data with modes\n",
    "    modes = pixeldata[~np.isnan(pixeldata['mode1'])]\n",
    "\n",
    "    pixeldata['mode_of_modes'] = vectorMode(modes[['mode1']],'mode1', MoM_span)\n",
    "    \n",
    "    # decision criteria\n",
    "    if len(pixeldata[pixeldata['mode_of_modes'] == 2]) > 0:\n",
    "        MoM_changedate = pixeldata[pixeldata['mode_of_modes'] == 2].iloc[0].name\n",
    "        M_ss = pixeldata.loc[MoM_changedate::]\n",
    "        M_changedate = M_ss[M_ss['mode1'] == 2].iloc[0].name\n",
    "        M_changedate_loc = pixeldata.index.get_loc(M_changedate)        \n",
    "#         print('MoM change', MoM_changedate)\n",
    "#         print('M_changedate: ' + M_changedate + ' location ' + str(M_changedate_loc))\n",
    "\n",
    "        pix_ss = pixeldata.iloc[M_changedate_loc - mode_span : M_changedate_loc + mode_span]\n",
    "#         print(pix_ss)\n",
    "        twoinarow = pix_ss[(pix_ss['predicted_landcover'] == pix_ss['predicted_landcover'].shift(-1)) & \n",
    "                            (pix_ss['predicted_landcover'][pix_ss['predicted_landcover'] == 2])]\n",
    "#         print('twoinarow')\n",
    "#         print(twoinarow)\n",
    "#         print(pixeldata.iloc[M_changedate_loc - mode_span : M_changedate_loc + mode_span])\n",
    "        if len(twoinarow) > 0:\n",
    "#             print(\"branch 2\")\n",
    "#             changedate = twoinarow.iloc[0].name\n",
    "            changedate = dateStringToFloat(twoinarow.iloc[0].name)\n",
    "            \n",
    "        else:\n",
    "#             print(\"branch 3\")\n",
    "            changedate = dateStringToFloat(pix_ss[pix_ss['predicted_landcover'] == 2].iloc[0].name)\n",
    "    else:\n",
    "#         print(\"branch 4\")\n",
    "        changedate =  np.nan\n",
    "        \n",
    "#     print(changedate)\n",
    "    return xr.DataArray([changedate], dims = 'xy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-d2e6c07eb4ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m105\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m107\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m74\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m76\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mband\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'predicted_landcover'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplyChangeDetector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'xy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# data[105:107,74:76,:].sel(band='predicted_landcover').reduce(applyChangeDetector, axis = 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/xarray/core/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, shortcut, **kwargs)\u001b[0m\n\u001b[1;32m    516\u001b[0m         applied = (maybe_wrap_array(arr, func(arr, **kwargs))\n\u001b[1;32m    517\u001b[0m                    for arr in grouped)\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshortcut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_combine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshortcut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/xarray/core/groupby.py\u001b[0m in \u001b[0;36m_combine\u001b[0;34m(self, applied, shortcut)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_reorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/xarray/core/groupby.py\u001b[0m in \u001b[0;36m_maybe_reorder\u001b[0;34m(xarray_obj, dim, positions)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_maybe_reorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxarray_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m     \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_inverse_permutation_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/xarray/core/groupby.py\u001b[0m in \u001b[0;36m_inverse_permutation_indices\u001b[0;34m(positions)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mno\u001b[0m \u001b[0mpermutation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnecessary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "data[105:107,74:76,:].stack(xy = ('x','y')).sel(band='predicted_landcover').groupby('xy').apply(applyChangeDetector).unstack(dim='xy')\n",
    "\n",
    "# data[105:107,74:76,:].sel(band='predicted_landcover').reduce(applyChangeDetector, axis = 2)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "898px",
    "left": "0px",
    "right": "1448px",
    "top": "111px",
    "width": "277px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "widgets": {
   "state": {
    "c52feb267d464c0681dc5b8825029c6a": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
