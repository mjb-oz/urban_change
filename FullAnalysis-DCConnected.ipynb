{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Classification Urban Change Detection\n",
    "This notebook is the complete workflow for urban change detection algorithm.\n",
    "The goal of this process is to be able to identigfy pixels that have been urbanised (changed from vegetation to the built environment) during the operation of the LandSat Earth Observation Satellites (since 1987).\n",
    "\n",
    "This notebook lets you:\n",
    "- create training data to train the classifier on\n",
    "- classify the data according to 4 broad landcover classes\n",
    "- view the results of your classification process\n",
    "- identify if and when a pixel that was previously not urban becomes dominantly urban (change detection)\n",
    "- view the results of the change detection\n",
    "\n",
    "The markdown cells have been designed to work with the 'Table Of Contents(2)' Jupyter notebook extension.\n",
    "This is highly recommended, if you don't have it yet (and are working on the VDI on the 'agdc-py3-prod module'\n",
    "select \"Edit\" on the menu bar above, click the \"nbextension config\" button at the bottom of the menu, and enable\n",
    "the extension. The 'Collapsible Headings' extension is also highly recommended.\n",
    "\n",
    "This was written Mike Barnes as part of his third graduate rotation, during January 2018.\n",
    "Any questions, please contact me at michael.barnes@ga.gov.au"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import datacube\n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.storage import masking\n",
    "from datacube.storage.masking import mask_to_dict\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import gdal\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider, FloatSlider, Dropdown\n",
    "from IPython.display import display\n",
    "\n",
    "from skimage import exposure\n",
    "from scipy.signal import lfilter\n",
    "\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Loading Data and Building the Xarray\n",
    "This project built on some existing work by Peter Tan. An output from Peter's urban change detection algorithm is raster files with all the relevant NBAR (analysis ready satellite derived surface reflectance readings) data saved to the output directory. To speed the loading and analysis during this script, this notebook will use those exisitng files if they are available. Otherwise it will load the data from the Digital Earth Australia archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: checkForLocalFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkForLocalFiles(study_area):\n",
    "    rootdir = os.listdir('../')\n",
    "    if study_area in rootdir:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: getData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(study_area):\n",
    "    if isinstance(study_area, str):\n",
    "        if checkForLocalFiles(study_area):\n",
    "            data = getLocalData(study_area)\n",
    "        else:\n",
    "            data = DCLoadName(study_area)\n",
    "        return data\n",
    "    elif isinstance(study_area, list) and len(study_area) == 4:\n",
    "        data = DCLoad(study_area)\n",
    "        return data\n",
    "    else:\n",
    "        print('Data Loading Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: DCLoadName\n",
    "This function is a wrapper for the DCLoad function, that allows previously used study areas to be easily restudied\n",
    "by easily loading exactly the same area of interest (AOI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DCLoadName(study_area):   \n",
    "    if study_area == 'mtbarker':\n",
    "        lat_min = -35.05\n",
    "        lat_max = -35.08\n",
    "        lon_min = 138.85\n",
    "        lon_max = 138.895  \n",
    "    elif study_area == 'swmelb':\n",
    "        lat_min = -37.879\n",
    "        lat_max = -37.91\n",
    "        lon_min = 144.705\n",
    "        lon_max = 144.76  \n",
    "    elif study_area == 'gunghalin':\n",
    "        lat_min = -35.18\n",
    "        lat_max = -35.21\n",
    "        lon_min = 149.14\n",
    "        lon_max = 149.17\n",
    "    elif study_area == 'goldengrove': \n",
    "        lat_min = -34.77\n",
    "        lat_max = -34.8\n",
    "        lon_min = 138.66\n",
    "        lon_max = 138.73\n",
    "    elif study_area == 'molonglo':\n",
    "        lat_min = -35.3\n",
    "        lat_max = -35.33\n",
    "        lon_min = 149.015\n",
    "        lon_max = 149.06\n",
    "    elif study_area == 'nperth':\n",
    "        lat_min = -31.686\n",
    "        lat_max = -31.73\n",
    "        lon_min = 115.79\n",
    "        lon_max = 115.813\n",
    "    elif study_area == 'swbris':\n",
    "        lat_min = -27.66\n",
    "        lat_max = -27.7 \n",
    "        lon_min = 152.877\n",
    "        lon_max = 152.93\n",
    "    elif study_area == 'swsyd':\n",
    "        lat_min = -33.993\n",
    "        lat_max = -34.04\n",
    "        lon_min = 150.715 \n",
    "        lon_max = 150.78\n",
    "    \n",
    "    return DCLoad([lat_min, lat_max, lon_min, lon_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: DCLoad\n",
    "This function is a variation of a datacube query supplied by Erin Telfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DCLoad(study_area):\n",
    "    # to time how long the load takes\n",
    "    start = datetime.datetime.now()\n",
    "    print('Loading data') \n",
    "    print('Load Started At: ' + str(start))\n",
    "    \n",
    "    # define temporal range \n",
    "    start_of_epoch = '1987-01-01'\n",
    "    end_of_epoch =  '2017-10-31'\n",
    "\n",
    "    # define bands of interest\n",
    "    bands_of_interest = ['blue', 'green', 'red', \n",
    "                         'nir', 'swir1', 'swir2']\n",
    "\n",
    "    # Landsat sensors of interest are defined\n",
    "    sensors = ['ls8', 'ls7', 'ls5'] \n",
    "\n",
    "    # unpack input parameter\n",
    "    lat_min, lat_max, lon_min, lon_max = study_area    \n",
    "\n",
    "    print('Bounding box: ' + str(lat_min) + ' S, ' + str(lon_min) +\n",
    "          ' E to ' + str(lat_max) + ' S, ' + str(lon_max) + ' E' )\n",
    "    print('Epoch: ' + start_of_epoch + ' to ' + end_of_epoch)\n",
    "    print('Sensors: ' + str(sensors))\n",
    "    print('Bands of Interest: ' + str(bands_of_interest))\n",
    "\n",
    "    # create query\n",
    "    query = {'time': (start_of_epoch, end_of_epoch),}\n",
    "    query['x'] = (lon_min, lon_max)\n",
    "    query['y'] = (lat_max, lat_min)\n",
    "    query['crs'] = 'EPSG:4326'\n",
    "\n",
    "    #Create cloud mask. This will define which pixel quality (PQ) artefacts are removed from the results.\n",
    "    # It should be noted the \"land_sea\" code will remove all ocean/sea pixels.\n",
    "    mask_components = {'cloud_acca':'no_cloud',\n",
    "    'cloud_shadow_acca' :'no_cloud_shadow',\n",
    "    'cloud_shadow_fmask' : 'no_cloud_shadow',\n",
    "    'cloud_fmask' :'no_cloud',\n",
    "    'blue_saturated' : False,\n",
    "    'green_saturated' : False,\n",
    "    'red_saturated' : False,\n",
    "    'nir_saturated' : False,\n",
    "    'swir1_saturated' : False,\n",
    "    'swir2_saturated' : False,\n",
    "    'contiguous':True,\n",
    "    'land_sea': 'land'}\n",
    "\n",
    "    # Connect to DataCube\n",
    "    dc = datacube.Datacube(app='Urban Change Detection')\n",
    "    \n",
    "    # Data for each Landsat sensor is retrieved and saved in a dict for concatenation\n",
    "    sensor_clean = {}\n",
    "    \n",
    "    for sensor in sensors:\n",
    "        # Load the NBAR and corresponding PQ\n",
    "        sensor_nbar = dc.load(product= sensor+'_nbar_albers', group_by='solar_day', \n",
    "                              measurements = bands_of_interest,  **query)\n",
    "        sensor_pq = dc.load(product= sensor+'_pq_albers', group_by='solar_day', \n",
    "                            fuse_func=ga_pq_fuser, **query)\n",
    "\n",
    "        # Retrieve the projection information before masking/sorting\n",
    "        crs = sensor_nbar.crs\n",
    "        crswkt = sensor_nbar.crs.wkt\n",
    "        affine = sensor_nbar.affine        \n",
    "\n",
    "        # Combing the pq so it is a single \n",
    "        sensor_all = xr.auto_combine([sensor_pq,sensor_nbar])\n",
    "        sensor_clean[sensor] = sensor_all\n",
    "\n",
    "        print('Loaded %s' % sensor) \n",
    "\n",
    "    print('Concatenating')\n",
    "    nbar_clean = xr.concat(sensor_clean.values(), 'time')\n",
    "    nbar_clean = nbar_clean.sortby('time')\n",
    "    nbar_clean.attrs['crs'] = crs\n",
    "    nbar_clean.attrs['affin|e'] = affine    \n",
    "\n",
    "    print ('Load and Xarray build complete')\n",
    "    print('Process took ' + str(datetime.datetime.now() - start))\n",
    "    \n",
    "    # return xarray\n",
    "    return nbar_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: getLocalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLocalData(study_area):\n",
    "    \"\"\"A quick helper function to load the output files from Peter's code for the given location.\n",
    "    It returns and Xarray of the landsat data for that study area.\"\"\"\n",
    "    # build a list of all files in the directory (ie the folder for that location)\n",
    "    location = '../' + study_area + '/'\n",
    "    files = os.listdir(location)\n",
    "\n",
    "    print('Loading data from: ' + location)\n",
    "    \n",
    "    # build a list of all the NBAR*.img file names and which bands they represent\n",
    "    NBARfiles = []\n",
    "    bands = []\n",
    "    for file in files:\n",
    "        if file[-4::] == '.img' and file[0:4] == 'NBAR':\n",
    "            NBARfiles.append(file)\n",
    "            bands.append(file.split('NBAR_')[1].split('.img')[0])\n",
    "\n",
    "    # open all the .img files with NBAR in the name, convert to numpy array, swap axes so order is (x, y, t)\n",
    "    # and save to dict\n",
    "    raw_data = {}\n",
    "    for i in range(len(NBARfiles)):\n",
    "        raw_data[bands[i]] = gdal.Open(location + NBARfiles[i]).ReadAsArray().swapaxes(0,2)\n",
    "#     num_scenes = len(raw_data['red'][0][0])   # delete this?\n",
    "\n",
    "    # build a list of all the dates represented by each band in the NBAR files\n",
    "    # reuse the list of NBAR file names, but this time access the .hdr file\n",
    "    in_dates = False\n",
    "    dates = []\n",
    "    for line in open(location + NBARfiles[0].split('.img')[0] + '.hdr'):\n",
    "        if line[0] == '}':\n",
    "            continue\n",
    "        if in_dates:\n",
    "            dates.append(line.split(',')[0].strip())\n",
    "        if line[0:10] == 'band names':\n",
    "            in_dates = True\n",
    "\n",
    "    # save list of satellite originated bands\n",
    "    sat_bands = bands.copy()\n",
    "\n",
    "    # add the yet to be calculated derivative bands to the overall bands list\n",
    "    bands += ['cloud_mask']\n",
    "\n",
    "    # building the Xarray\n",
    "    # define the size for the numpy array that will hold all the data for conversion into XArray\n",
    "    x = len(raw_data['red'])\n",
    "    y = len(raw_data['red'][0])\n",
    "    t = len(raw_data['red'][0][0])\n",
    "    n = len(bands)\n",
    "\n",
    "    # create an empty numpy array of the correct size\n",
    "    alldata = np.zeros((x, y, t, n), dtype=np.float32)\n",
    "\n",
    "    # populate the numpy array with the satellite data\n",
    "    # turn all no data NBAR values to NaNs\n",
    "    for i in range(len(sat_bands)):\n",
    "        alldata[:,:,:,i] = raw_data[sat_bands[i]]\n",
    "        alldata[:,:,:,i][alldata[:,:,:,i] == -999] = np.nan\n",
    "\n",
    "    # convert the numpy array into an xarray, with appropriate lables, and axes names\n",
    "    data = xr.DataArray(alldata, coords = {'x':range(x), 'y':range(y), 'date':dates, 'band':bands},\n",
    "                 dims=['x', 'y', 'date', 'band'])\n",
    "    \n",
    "    # import cloudmask and add to xarray\n",
    "    cloudmask = gdal.Open(location + '/tsmask.img').ReadAsArray().swapaxes(0,2)\n",
    "    data.loc[:,:,:,'cloud_mask'] = cloudmask\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: transformXarrayToCustomStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformXarrayToCustomStyle(data_new):\n",
    "    # loop through and extract all the values into a list of 3D numpy arrays\n",
    "    bands = ['blue','green','red','nir','swir1','swir2','pixelquality']\n",
    "    all_data = []\n",
    "    for band in bands:\n",
    "        all_data.append(data_new.variables[band].values.swapaxes(2,0).astype(np.float32))\n",
    "\n",
    "    # replace 'pixelquality' with 'cloud_mask'  \n",
    "    bands[bands.index('pixelquality')] = 'cloud_mask'\n",
    "\n",
    "    # stack the list of 3D arrays along a 4th dimension\n",
    "    data_flipped = np.stack(all_data, axis = -1)\n",
    "    # set all bad NBAR values to np.nan\n",
    "    data_flipped[data_flipped == -999] = np.nan\n",
    "\n",
    "    # save the new shape for use in defining the Xarray\n",
    "    new_shape = data_flipped.shape\n",
    "\n",
    "    # build a list of all the dates (as strings)\n",
    "    dates = []\n",
    "    for time in range(len(data_new.time)):\n",
    "        dates.append(np.datetime_as_string(data_new.time[time].values)[0:10])\n",
    "\n",
    "    # assemble the new Xarray\n",
    "    newdatafixed = xr.DataArray(data_flipped, coords = {'x': range(new_shape[0]), 'y': range(new_shape[1]),\n",
    "                                                        'date': dates, 'band': bands}, dims=['x','y','date','band'])\n",
    "    return newdatafixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Drawing the Traing Data Generator Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: drawTrainingPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawTrainingPlot(study_area, scene_num, covertype, scene_picks_arr):\n",
    "    \"\"\"Allows easy extension to extra subplots in the training plot figure\"\"\"\n",
    "    ax1, scene_picks_arr = drawTrainingScene(study_area, scene_num, covertype, scene_picks_arr)\n",
    "    plt.draw()\n",
    "    \n",
    "    return scene_picks_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: drawTrainingScene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawTrainingScene(study_area, scene_num, covertype, scene_picks_arr):\n",
    "    \"\"\"This function draws the desired scene (study area and scene number).\n",
    "    It also presents the existing training data for that scene if any exists.\n",
    "    It returns the axes object for the image, along with a numpy array which is\n",
    "    the existing picks for that scene\"\"\"\n",
    "    \n",
    "    # get data for selected study area\n",
    "#     data = getData(study_area)\n",
    "    \n",
    "    # colour map included incase of need to display false colour or other in the future\n",
    "    # could change this to an ordereddict and remove the RGB list created below...?\n",
    "    # or have RGB a list, and use that in data.sel(band=RGB).values\n",
    "    colourmap = {'R':'red', 'G':'green', 'B':'blue'}\n",
    "    \n",
    "    # combine the data for the 3 bands to be displayed into a single numpy array\n",
    "    h = data.shape[1]\n",
    "    w = data.shape[0]\n",
    "    t = data.shape[2]\n",
    "    if scene_num > (t -1):\n",
    "        scene_num = t - 1\n",
    "    RGB = ['R','G','B']\n",
    "    date = str(data[:,:,scene_num].date.values)\n",
    "    \n",
    "    # create array to store the RGB info in, and fill by looping through the colourmap variable\n",
    "    # note the .T at the end, because the data array is setup as a (x,y,t), but imshow works (y,x)\n",
    "    rawimg = np.zeros((h, w, 3), dtype=np.float32)\n",
    "    for i in range(len(RGB)):     \n",
    "        rawimg[:,:,i] = data[:,:,scene_num].sel(band=colourmap[RGB[i]]).T\n",
    "        \n",
    "    # equalizing for all bands together\n",
    "    # goal is to make is human interpretable\n",
    "    img_toshow = exposure.equalize_hist(rawimg, mask = np.isfinite(rawimg))    \n",
    "\n",
    "    # displaying the results and formatting the axes etc\n",
    "    plt.imshow(img_toshow)\n",
    "    ax = plt.gca()\n",
    "    ax.set_title('True Colour Landsat Scene, taken\\n' + date + ', over ' + study_area)\n",
    "    \n",
    "   \n",
    "    if scene_picks_arr is None:\n",
    "        \n",
    "        if study_area in trainingdata.index and scene_num in trainingdata.loc[study_area].index:\n",
    "            # if there aren't any picks yet, make the array\n",
    "            scene_picks_arr = np.zeros((h,w), dtype=np.float32)\n",
    "            # fill it with np.nan\n",
    "            scene_picks_arr[scene_picks_arr == 0] = np.nan\n",
    "            # make a dict with the key (study_area, scene_num)\n",
    "            scene_picks_arr = {(study_area, scene_num): scene_picks_arr}\n",
    "            temp = trainingdata.loc[(study_area, scene_num)]\n",
    "            # loop through all relevant training points, and populate the array\n",
    "            for i in range(len(temp)):\n",
    "                position = temp.iloc[i].name\n",
    "                scene_picks_arr[(study_area, scene_num)][position[0], position[1]] = temp['landcover'].iloc[i]\n",
    "        else:\n",
    "            # if not the right scene/location combination, set to None\n",
    "            scene_picks_arr = None\n",
    "    \n",
    "    # if there are picks, then plot them up, coloured as per the environment level variable colours\n",
    "    # should I better tie cmap colours to colours\n",
    "    if scene_picks_arr is not None and (study_area,scene_num) in (scene_picks_arr.keys()):\n",
    "        cmap = colors.ListedColormap(colours)\n",
    "        # plot the training pixels\n",
    "        ax.imshow(scene_picks_arr[(study_area, scene_num)], cmap)\n",
    "        legend_patches = []\n",
    "        # build the legend\n",
    "        for cover in landcover.keys():\n",
    "            legend_patches.append(mpatches.Patch(color = colours[landcover[cover]-1], label = cover))\n",
    "        ax.legend(handles = legend_patches)\n",
    "    else:\n",
    "        # if not the right scene/location combination, set to None\n",
    "        scene_picks_arr = None\n",
    "    \n",
    "    return ax, scene_picks_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some broad scope variables specific to the plotting that need setting up and seem to very fragile\n",
    "# so I'm too scared to move them in case something breaks!\n",
    "global xpos\n",
    "global ypos\n",
    "xpos = 0\n",
    "ypos = 0\n",
    "global scene_picks_arr\n",
    "scene_picks_arr = None\n",
    "colours = ['r', 'b', 'm', 'c']\n",
    "\n",
    "def train(study_area, scene_num, covertype):\n",
    "\n",
    "    def onclick(event):\n",
    "        # defining what to do on a click event\n",
    "        \n",
    "        # I don't understand why this need to be declared global again, but it breaks without these lines\n",
    "        global xpos\n",
    "        global ypos\n",
    "        # need to cast to int as result is a float, and can't index a list with a float\n",
    "        xpos = int(event.xdata)\n",
    "        ypos = int(event.ydata)\n",
    "        # save the results of the click to the training data dataframe\n",
    "        trainingdata.loc[(study_area, scene_num, ypos, xpos)] = landcover[covertype]\n",
    "        # add the results to the current scenes overlay\n",
    "        scene_picks_arr[(study_area,scene_num)][ypos, xpos] = landcover[covertype]\n",
    "        # redraw with the trained pixels updated on the image\n",
    "        drawTrainingPlot(study_area, scene_num, covertype, scene_picks_arr)\n",
    "    \n",
    "    # control the figure size\n",
    "    fig = plt.figure(figsize=[10,10])\n",
    "    axs = fig.axes\n",
    "    plt.subplots_adjust(hspace = 0.6)\n",
    "    \n",
    "    # draw the figure\n",
    "    global scene_picks_arr\n",
    "    scene_picks_arr = drawTrainingPlot(study_area, scene_num, covertype, scene_picks_arr)\n",
    "    #connect the click event action to the figure\n",
    "    cid = fig.canvas.mpl_connect('button_press_event', onclick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up broad scope variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Previous Training Data (or make blank dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>landcover</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>study_area</th>\n",
       "      <th>scene_num</th>\n",
       "      <th>row</th>\n",
       "      <th>column</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"61\" valign=\"top\">mtbarker</th>\n",
       "      <th rowspan=\"61\" valign=\"top\">1</th>\n",
       "      <th>38</th>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <th>104</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <th>106</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <th>102</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">25</th>\n",
       "      <th>109</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">29</th>\n",
       "      <th>148</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <th>152</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <th>151</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <th>136</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <th>144</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <th>145</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <th>147</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <th>140</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <th>137</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <th>167</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <th>168</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <th>167</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <th>166</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <th>169</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <th>169</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <th>166</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>122</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <th>122</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <th>123</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <th>122</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <th>120</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <th>119</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <th>118</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <th>117</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <th>121</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <th>90</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <th>90</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <th>88</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <th>88</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <th>88</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <th>140</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <th>139</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <th>138</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <th>139</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <th>141</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <th>142</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>144</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <th>145</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <th>143</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <th>154</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <th>157</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <th>158</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <th>159</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <th>153</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <th>153</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <th>156</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 landcover\n",
       "study_area scene_num row column           \n",
       "mtbarker   1         38  105             1\n",
       "                     33  104             1\n",
       "                     32  105             1\n",
       "                     28  106             1\n",
       "                     26  102             1\n",
       "                     24  100             1\n",
       "                     25  109             1\n",
       "                         116             1\n",
       "                     29  148             1\n",
       "                         150             1\n",
       "                     31  152             1\n",
       "                     35  151             1\n",
       "                     37  150             1\n",
       "                     68  136             1\n",
       "                     61  144             1\n",
       "                     58  145             1\n",
       "                     55  147             1\n",
       "                     56  140             1\n",
       "                     59  137             1\n",
       "                     54  165             1\n",
       "                     55  167             1\n",
       "                     53  168             1\n",
       "                     52  167             1\n",
       "                     51  165             1\n",
       "                     49  165             1\n",
       "                     47  166             1\n",
       "                     46  169             1\n",
       "                     50  169             1\n",
       "                     60  166             1\n",
       "                     59  165             1\n",
       "...                                    ...\n",
       "                     32  122             3\n",
       "                     30  122             3\n",
       "                     29  123             3\n",
       "                     28  122             3\n",
       "                     27  120             3\n",
       "                     30  119             3\n",
       "                     33  118             3\n",
       "                     35  117             3\n",
       "                     34  121             3\n",
       "                     33  90              3\n",
       "                     30  90              3\n",
       "                     28  88              3\n",
       "                     31  88              3\n",
       "                     33  88              3\n",
       "                     120 140             3\n",
       "                     118 139             3\n",
       "                     115 138             3\n",
       "                     114 139             3\n",
       "                     118 141             3\n",
       "                     120 142             3\n",
       "                     122 144             3\n",
       "                     124 145             3\n",
       "                     125 143             3\n",
       "                     96  154             3\n",
       "                     97  157             3\n",
       "                     96  158             3\n",
       "                     95  159             3\n",
       "                     93  153             3\n",
       "                     89  153             3\n",
       "                     90  156             3\n",
       "\n",
       "[126 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load previous training data\n",
    "# by taking the last (ie most recent if the standard date is attached to the file) .pkl file\n",
    "files = os.listdir('../')\n",
    "pickles = []\n",
    "for file in files:\n",
    "    if file[-3::] == 'pkl':\n",
    "        pickles.append(file)\n",
    "trainingdata = pd.read_pickle('../' + pickles[-1])\n",
    "trainingdata = trainingdata.drop(columns = trainingdata.columns[1::])\n",
    "\n",
    "# # setup a multilevel heirachrical index dataframe to store the results\n",
    "# # storing the training data in this format is way more memory efficient than in an Xarray of same size as data\n",
    "# # but it takes a lot of processing and manipulation to get it into a more useable form\n",
    "\n",
    "# trainidx = pd.MultiIndex(levels = [[]]*4, labels = [[]]*4, names=['study_area', 'scene_num', 'row','column'])\n",
    "# traincols = ['landcover']\n",
    "# trainingdata = pd.DataFrame(index = trainidx, columns = traincols)\n",
    "\n",
    "# view the current status\n",
    "trainingdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other broad scope variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier to work with integers than strings, so map the planned training classes to integers\n",
    "landcover = {'vegetation':1,'urban':2,'earth':3,'water':4}\n",
    "# range of pretermined study areas to use as sources for training data\n",
    "study_areas = ['mtbarker', 'swmelb', 'gunghalin', 'goldengrove', 'molonglo', 'nperth', 'swbris', 'swsyd', 'custom']\n",
    "\n",
    "colours = ['r', 'b', 'm', 'c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b3794c1a714f6f98053023d47dbcfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>Dropdown</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Dropdown(description='Study Area', options=('mtbarker', 'swmelb', 'gunghalin', 'goldengrove', 'molonglo', 'nperth', 'swbris', 'swsyd', 'custom'), value='mtbarker')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "study_area_dd = Dropdown(options=study_areas, value = study_areas[0], description='Study Area', disabled = False)\n",
    "display(study_area_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: ../mtbarker/\n",
      "\n",
      "Study area data loaded.\n"
     ]
    }
   ],
   "source": [
    "study_area = study_area_dd.value\n",
    "if study_area == 'custom':\n",
    "    coords = ['lat_min', 'lat_max', 'lon_min', 'lon_max']\n",
    "    spatial_query = collections.OrderedDict()\n",
    "    for coord in coords:\n",
    "        spatial_query[coord] = input(coord + ': ')\n",
    "    data = getData(list(spatial_query.values()))\n",
    "else:\n",
    "    data = getData(study_area)\n",
    "if not checkForLocalFiles(study_area):\n",
    "    data = transformXarrayToCustomStyle(data)\n",
    "print('\\nStudy area data loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9cb85471334db89eecbcb04ffd2782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='Scene Number', max=671), Dropdown(description='Landcover', options=('vegetation', 'urban', 'earth', 'water'), value='vegetation'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    interact(train,\n",
    "             study_area = fixed(study_area),\n",
    "             scene_num = IntSlider(value = 1, min = 0, max = data.shape[2]-1,description = \"Scene Number\"),\n",
    "             covertype = Dropdown(options=list(landcover.keys()), value=list(landcover.keys())[0], description='Landcover', disabled = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>landcover</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>study_area</th>\n",
       "      <th>scene_num</th>\n",
       "      <th>row</th>\n",
       "      <th>column</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"61\" valign=\"top\">mtbarker</th>\n",
       "      <th rowspan=\"56\" valign=\"top\">1</th>\n",
       "      <th>38</th>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <th>104</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <th>106</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <th>102</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">25</th>\n",
       "      <th>109</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">29</th>\n",
       "      <th>148</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <th>152</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <th>151</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <th>136</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <th>144</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <th>145</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <th>147</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <th>140</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <th>137</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <th>167</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <th>168</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <th>167</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <th>166</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <th>169</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <th>169</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <th>166</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <th>119</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <th>118</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <th>117</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <th>121</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <th>90</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <th>90</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <th>88</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <th>88</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <th>88</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <th>140</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <th>139</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <th>138</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <th>139</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <th>141</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <th>142</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>144</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <th>145</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <th>143</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <th>154</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <th>157</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <th>158</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <th>159</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <th>153</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <th>153</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <th>156</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2</th>\n",
       "      <th>20</th>\n",
       "      <th>109</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <th>117</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 landcover\n",
       "study_area scene_num row column           \n",
       "mtbarker   1         38  105             1\n",
       "                     33  104             1\n",
       "                     32  105             1\n",
       "                     28  106             1\n",
       "                     26  102             1\n",
       "                     24  100             1\n",
       "                     25  109             1\n",
       "                         116             1\n",
       "                     29  148             1\n",
       "                         150             1\n",
       "                     31  152             1\n",
       "                     35  151             1\n",
       "                     37  150             1\n",
       "                     68  136             1\n",
       "                     61  144             1\n",
       "                     58  145             1\n",
       "                     55  147             1\n",
       "                     56  140             1\n",
       "                     59  137             1\n",
       "                     54  165             1\n",
       "                     55  167             1\n",
       "                     53  168             1\n",
       "                     52  167             1\n",
       "                     51  165             1\n",
       "                     49  165             1\n",
       "                     47  166             1\n",
       "                     46  169             1\n",
       "                     50  169             1\n",
       "                     60  166             1\n",
       "                     59  165             1\n",
       "...                                    ...\n",
       "                     30  119             3\n",
       "                     33  118             3\n",
       "                     35  117             3\n",
       "                     34  121             3\n",
       "                     33  90              3\n",
       "                     30  90              3\n",
       "                     28  88              3\n",
       "                     31  88              3\n",
       "                     33  88              3\n",
       "                     120 140             3\n",
       "                     118 139             3\n",
       "                     115 138             3\n",
       "                     114 139             3\n",
       "                     118 141             3\n",
       "                     120 142             3\n",
       "                     122 144             3\n",
       "                     124 145             3\n",
       "                     125 143             3\n",
       "                     96  154             3\n",
       "                     97  157             3\n",
       "                     96  158             3\n",
       "                     95  159             3\n",
       "                     93  153             3\n",
       "                     89  153             3\n",
       "                     90  156             3\n",
       "           2         20  109             2\n",
       "                     21  117             2\n",
       "                     37  17              2\n",
       "                     33  29              1\n",
       "                     27  51              1\n",
       "\n",
       "[131 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the outputs of the training data generation process\n",
    "trainingdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Results Manipulation and Classifier Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: ../mtbarker/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>landcover</th>\n",
       "      <th>blue</th>\n",
       "      <th>green</th>\n",
       "      <th>red</th>\n",
       "      <th>nir</th>\n",
       "      <th>swir1</th>\n",
       "      <th>swir2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>study_area</th>\n",
       "      <th>scene_num</th>\n",
       "      <th>row</th>\n",
       "      <th>column</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"61\" valign=\"top\">mtbarker</th>\n",
       "      <th rowspan=\"56\" valign=\"top\">1</th>\n",
       "      <th>38</th>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "      <td>247.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>4094.0</td>\n",
       "      <td>1305.0</td>\n",
       "      <td>558.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <th>104</th>\n",
       "      <td>1</td>\n",
       "      <td>420.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>3877.0</td>\n",
       "      <td>2284.0</td>\n",
       "      <td>1088.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "      <td>420.0</td>\n",
       "      <td>677.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>3833.0</td>\n",
       "      <td>2284.0</td>\n",
       "      <td>1044.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <th>106</th>\n",
       "      <td>1</td>\n",
       "      <td>362.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>3877.0</td>\n",
       "      <td>2192.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <th>102</th>\n",
       "      <td>1</td>\n",
       "      <td>324.0</td>\n",
       "      <td>636.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>4483.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "      <td>305.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>4093.0</td>\n",
       "      <td>1886.0</td>\n",
       "      <td>823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">25</th>\n",
       "      <th>109</th>\n",
       "      <td>1</td>\n",
       "      <td>324.0</td>\n",
       "      <td>596.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>4527.0</td>\n",
       "      <td>1948.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>677.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>3746.0</td>\n",
       "      <td>2131.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">29</th>\n",
       "      <th>148</th>\n",
       "      <td>1</td>\n",
       "      <td>439.0</td>\n",
       "      <td>958.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>5734.0</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "      <td>362.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>5390.0</td>\n",
       "      <td>1917.0</td>\n",
       "      <td>823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <th>152</th>\n",
       "      <td>1</td>\n",
       "      <td>458.0</td>\n",
       "      <td>1038.0</td>\n",
       "      <td>847.0</td>\n",
       "      <td>5691.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>911.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <th>151</th>\n",
       "      <td>1</td>\n",
       "      <td>382.0</td>\n",
       "      <td>838.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>5820.0</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "      <td>458.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>5906.0</td>\n",
       "      <td>2070.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <th>136</th>\n",
       "      <td>1</td>\n",
       "      <td>477.0</td>\n",
       "      <td>958.0</td>\n",
       "      <td>811.0</td>\n",
       "      <td>5305.0</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <th>144</th>\n",
       "      <td>1</td>\n",
       "      <td>477.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>811.0</td>\n",
       "      <td>4874.0</td>\n",
       "      <td>2315.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <th>145</th>\n",
       "      <td>1</td>\n",
       "      <td>515.0</td>\n",
       "      <td>1038.0</td>\n",
       "      <td>883.0</td>\n",
       "      <td>5219.0</td>\n",
       "      <td>2254.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <th>147</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <th>140</th>\n",
       "      <td>1</td>\n",
       "      <td>477.0</td>\n",
       "      <td>838.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>4571.0</td>\n",
       "      <td>2346.0</td>\n",
       "      <td>1044.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <th>137</th>\n",
       "      <td>1</td>\n",
       "      <td>477.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>811.0</td>\n",
       "      <td>4917.0</td>\n",
       "      <td>2223.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "      <td>382.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>488.0</td>\n",
       "      <td>5563.0</td>\n",
       "      <td>1704.0</td>\n",
       "      <td>647.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <th>167</th>\n",
       "      <td>1</td>\n",
       "      <td>420.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>5606.0</td>\n",
       "      <td>1704.0</td>\n",
       "      <td>647.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <th>168</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>677.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>5348.0</td>\n",
       "      <td>1734.0</td>\n",
       "      <td>647.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <th>167</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>5262.0</td>\n",
       "      <td>1673.0</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>5348.0</td>\n",
       "      <td>1642.0</td>\n",
       "      <td>602.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "      <td>401.0</td>\n",
       "      <td>798.0</td>\n",
       "      <td>596.0</td>\n",
       "      <td>5348.0</td>\n",
       "      <td>1704.0</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <th>166</th>\n",
       "      <td>1</td>\n",
       "      <td>382.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>5391.0</td>\n",
       "      <td>1765.0</td>\n",
       "      <td>779.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <th>169</th>\n",
       "      <td>1</td>\n",
       "      <td>382.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>5262.0</td>\n",
       "      <td>1856.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <th>169</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>5606.0</td>\n",
       "      <td>1734.0</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <th>166</th>\n",
       "      <td>1</td>\n",
       "      <td>305.0</td>\n",
       "      <td>596.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>5434.0</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>647.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "      <td>363.0</td>\n",
       "      <td>556.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>5348.0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>558.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <th>119</th>\n",
       "      <td>3</td>\n",
       "      <td>591.0</td>\n",
       "      <td>958.0</td>\n",
       "      <td>1133.0</td>\n",
       "      <td>2436.0</td>\n",
       "      <td>2926.0</td>\n",
       "      <td>2279.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <th>118</th>\n",
       "      <td>3</td>\n",
       "      <td>743.0</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>1418.0</td>\n",
       "      <td>2393.0</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>2941.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <th>117</th>\n",
       "      <td>3</td>\n",
       "      <td>819.0</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>2480.0</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>2941.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <th>121</th>\n",
       "      <td>3</td>\n",
       "      <td>800.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>2524.0</td>\n",
       "      <td>3323.0</td>\n",
       "      <td>2897.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <th>90</th>\n",
       "      <td>3</td>\n",
       "      <td>781.0</td>\n",
       "      <td>1198.0</td>\n",
       "      <td>1666.0</td>\n",
       "      <td>2611.0</td>\n",
       "      <td>3781.0</td>\n",
       "      <td>3426.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <th>90</th>\n",
       "      <td>3</td>\n",
       "      <td>857.0</td>\n",
       "      <td>1277.0</td>\n",
       "      <td>1631.0</td>\n",
       "      <td>2611.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>3206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <th>88</th>\n",
       "      <td>3</td>\n",
       "      <td>534.0</td>\n",
       "      <td>878.0</td>\n",
       "      <td>954.0</td>\n",
       "      <td>3441.0</td>\n",
       "      <td>2559.0</td>\n",
       "      <td>1705.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <th>88</th>\n",
       "      <td>3</td>\n",
       "      <td>857.0</td>\n",
       "      <td>1277.0</td>\n",
       "      <td>1773.0</td>\n",
       "      <td>2786.0</td>\n",
       "      <td>3720.0</td>\n",
       "      <td>3206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <th>88</th>\n",
       "      <td>3</td>\n",
       "      <td>724.0</td>\n",
       "      <td>1198.0</td>\n",
       "      <td>1631.0</td>\n",
       "      <td>2480.0</td>\n",
       "      <td>3537.0</td>\n",
       "      <td>3029.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <th>140</th>\n",
       "      <td>3</td>\n",
       "      <td>401.0</td>\n",
       "      <td>637.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>1425.0</td>\n",
       "      <td>1765.0</td>\n",
       "      <td>1618.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <th>139</th>\n",
       "      <td>3</td>\n",
       "      <td>478.0</td>\n",
       "      <td>677.0</td>\n",
       "      <td>776.0</td>\n",
       "      <td>1425.0</td>\n",
       "      <td>1918.0</td>\n",
       "      <td>1663.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <th>138</th>\n",
       "      <td>3</td>\n",
       "      <td>516.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>1734.0</td>\n",
       "      <td>1949.0</td>\n",
       "      <td>1530.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <th>139</th>\n",
       "      <td>3</td>\n",
       "      <td>516.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>955.0</td>\n",
       "      <td>1822.0</td>\n",
       "      <td>2102.0</td>\n",
       "      <td>1795.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <th>141</th>\n",
       "      <td>3</td>\n",
       "      <td>420.0</td>\n",
       "      <td>637.0</td>\n",
       "      <td>812.0</td>\n",
       "      <td>1469.0</td>\n",
       "      <td>1949.0</td>\n",
       "      <td>1707.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <th>142</th>\n",
       "      <td>3</td>\n",
       "      <td>420.0</td>\n",
       "      <td>637.0</td>\n",
       "      <td>776.0</td>\n",
       "      <td>1469.0</td>\n",
       "      <td>1918.0</td>\n",
       "      <td>1618.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>144</th>\n",
       "      <td>3</td>\n",
       "      <td>497.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>1602.0</td>\n",
       "      <td>2194.0</td>\n",
       "      <td>1839.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <th>145</th>\n",
       "      <td>3</td>\n",
       "      <td>516.0</td>\n",
       "      <td>798.0</td>\n",
       "      <td>991.0</td>\n",
       "      <td>1734.0</td>\n",
       "      <td>2347.0</td>\n",
       "      <td>1972.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <th>143</th>\n",
       "      <td>3</td>\n",
       "      <td>497.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>883.0</td>\n",
       "      <td>1513.0</td>\n",
       "      <td>2163.0</td>\n",
       "      <td>1751.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <th>154</th>\n",
       "      <td>3</td>\n",
       "      <td>687.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>1312.0</td>\n",
       "      <td>2218.0</td>\n",
       "      <td>2775.0</td>\n",
       "      <td>2501.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <th>157</th>\n",
       "      <td>3</td>\n",
       "      <td>516.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>1026.0</td>\n",
       "      <td>1822.0</td>\n",
       "      <td>2316.0</td>\n",
       "      <td>1972.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <th>158</th>\n",
       "      <td>3</td>\n",
       "      <td>554.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>1026.0</td>\n",
       "      <td>1866.0</td>\n",
       "      <td>2377.0</td>\n",
       "      <td>2060.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <th>159</th>\n",
       "      <td>3</td>\n",
       "      <td>478.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>1646.0</td>\n",
       "      <td>2102.0</td>\n",
       "      <td>1883.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <th>153</th>\n",
       "      <td>3</td>\n",
       "      <td>649.0</td>\n",
       "      <td>1039.0</td>\n",
       "      <td>1312.0</td>\n",
       "      <td>2394.0</td>\n",
       "      <td>2836.0</td>\n",
       "      <td>2501.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <th>153</th>\n",
       "      <td>3</td>\n",
       "      <td>592.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>1205.0</td>\n",
       "      <td>2218.0</td>\n",
       "      <td>2560.0</td>\n",
       "      <td>2281.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <th>156</th>\n",
       "      <td>3</td>\n",
       "      <td>535.0</td>\n",
       "      <td>798.0</td>\n",
       "      <td>1026.0</td>\n",
       "      <td>1954.0</td>\n",
       "      <td>2499.0</td>\n",
       "      <td>2148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2</th>\n",
       "      <th>20</th>\n",
       "      <th>109</th>\n",
       "      <td>2</td>\n",
       "      <td>638.0</td>\n",
       "      <td>904.0</td>\n",
       "      <td>983.0</td>\n",
       "      <td>2425.0</td>\n",
       "      <td>2146.0</td>\n",
       "      <td>1486.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <th>117</th>\n",
       "      <td>2</td>\n",
       "      <td>638.0</td>\n",
       "      <td>937.0</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>2425.0</td>\n",
       "      <td>2367.0</td>\n",
       "      <td>1556.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>761.0</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>2531.0</td>\n",
       "      <td>2977.0</td>\n",
       "      <td>2007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>792.0</td>\n",
       "      <td>1098.0</td>\n",
       "      <td>1298.0</td>\n",
       "      <td>2244.0</td>\n",
       "      <td>2439.0</td>\n",
       "      <td>1694.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>621.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>982.0</td>\n",
       "      <td>2460.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>1451.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 landcover   blue   green     red     nir  \\\n",
       "study_area scene_num row column                                             \n",
       "mtbarker   1         38  105             1  247.0   434.0   344.0  4094.0   \n",
       "                     33  104             1  420.0   717.0   560.0  3877.0   \n",
       "                     32  105             1  420.0   677.0   524.0  3833.0   \n",
       "                     28  106             1  362.0   717.0   524.0  3877.0   \n",
       "                     26  102             1  324.0   636.0   452.0  4483.0   \n",
       "                     24  100             1  305.0   555.0   380.0  4093.0   \n",
       "                     25  109             1  324.0   596.0   416.0  4527.0   \n",
       "                         116             1  363.0   677.0   524.0  3746.0   \n",
       "                     29  148             1  439.0   958.0   739.0  5734.0   \n",
       "                         150             1  362.0   757.0   560.0  5390.0   \n",
       "                     31  152             1  458.0  1038.0   847.0  5691.0   \n",
       "                     35  151             1  382.0   838.0   632.0  5820.0   \n",
       "                     37  150             1  458.0   918.0   739.0  5906.0   \n",
       "                     68  136             1  477.0   958.0   811.0  5305.0   \n",
       "                     61  144             1  477.0   918.0   811.0  4874.0   \n",
       "                     58  145             1  515.0  1038.0   883.0  5219.0   \n",
       "                     55  147             1    NaN     NaN     NaN     NaN   \n",
       "                     56  140             1  477.0   838.0   739.0  4571.0   \n",
       "                     59  137             1  477.0   998.0   811.0  4917.0   \n",
       "                     54  165             1  382.0   758.0   488.0  5563.0   \n",
       "                     55  167             1  420.0   758.0   524.0  5606.0   \n",
       "                     53  168             1  363.0   677.0   524.0  5348.0   \n",
       "                     52  167             1  363.0   758.0   560.0  5262.0   \n",
       "                     51  165             1  363.0   757.0   524.0  5348.0   \n",
       "                     49  165             1  401.0   798.0   596.0  5348.0   \n",
       "                     47  166             1  382.0   757.0   632.0  5391.0   \n",
       "                     46  169             1  382.0   717.0   632.0  5262.0   \n",
       "                     50  169             1  363.0   758.0   560.0  5606.0   \n",
       "                     60  166             1  305.0   596.0   416.0  5434.0   \n",
       "                     59  165             1  363.0   556.0   416.0  5348.0   \n",
       "...                                    ...    ...     ...     ...     ...   \n",
       "                     30  119             3  591.0   958.0  1133.0  2436.0   \n",
       "                     33  118             3  743.0  1078.0  1418.0  2393.0   \n",
       "                     35  117             3  819.0  1158.0  1489.0  2480.0   \n",
       "                     34  121             3  800.0  1118.0  1489.0  2524.0   \n",
       "                     33  90              3  781.0  1198.0  1666.0  2611.0   \n",
       "                     30  90              3  857.0  1277.0  1631.0  2611.0   \n",
       "                     28  88              3  534.0   878.0   954.0  3441.0   \n",
       "                     31  88              3  857.0  1277.0  1773.0  2786.0   \n",
       "                     33  88              3  724.0  1198.0  1631.0  2480.0   \n",
       "                     120 140             3  401.0   637.0   740.0  1425.0   \n",
       "                     118 139             3  478.0   677.0   776.0  1425.0   \n",
       "                     115 138             3  516.0   717.0   919.0  1734.0   \n",
       "                     114 139             3  516.0   717.0   955.0  1822.0   \n",
       "                     118 141             3  420.0   637.0   812.0  1469.0   \n",
       "                     120 142             3  420.0   637.0   776.0  1469.0   \n",
       "                     122 144             3  497.0   717.0   919.0  1602.0   \n",
       "                     124 145             3  516.0   798.0   991.0  1734.0   \n",
       "                     125 143             3  497.0   717.0   883.0  1513.0   \n",
       "                     96  154             3  687.0   999.0  1312.0  2218.0   \n",
       "                     97  157             3  516.0   758.0  1026.0  1822.0   \n",
       "                     96  158             3  554.0   758.0  1026.0  1866.0   \n",
       "                     95  159             3  478.0   717.0   919.0  1646.0   \n",
       "                     93  153             3  649.0  1039.0  1312.0  2394.0   \n",
       "                     89  153             3  592.0   918.0  1205.0  2218.0   \n",
       "                     90  156             3  535.0   798.0  1026.0  1954.0   \n",
       "           2         20  109             2  638.0   904.0   983.0  2425.0   \n",
       "                     21  117             2  638.0   937.0  1040.0  2425.0   \n",
       "                     37  17              2  761.0  1130.0  1297.0  2531.0   \n",
       "                     33  29              1  792.0  1098.0  1298.0  2244.0   \n",
       "                     27  51              1  621.0   936.0   982.0  2460.0   \n",
       "\n",
       "                                  swir1   swir2  \n",
       "study_area scene_num row column                  \n",
       "mtbarker   1         38  105     1305.0   558.0  \n",
       "                     33  104     2284.0  1088.0  \n",
       "                     32  105     2284.0  1044.0  \n",
       "                     28  106     2192.0   999.0  \n",
       "                     26  102     2009.0   823.0  \n",
       "                     24  100     1886.0   823.0  \n",
       "                     25  109     1948.0   867.0  \n",
       "                         116     2131.0   999.0  \n",
       "                     29  148     2040.0   867.0  \n",
       "                         150     1917.0   823.0  \n",
       "                     31  152     2009.0   911.0  \n",
       "                     35  151     2040.0   867.0  \n",
       "                     37  150     2070.0   867.0  \n",
       "                     68  136     2040.0   867.0  \n",
       "                     61  144     2315.0  1000.0  \n",
       "                     58  145     2254.0  1000.0  \n",
       "                     55  147        NaN     NaN  \n",
       "                     56  140     2346.0  1044.0  \n",
       "                     59  137     2223.0  1000.0  \n",
       "                     54  165     1704.0   647.0  \n",
       "                     55  167     1704.0   647.0  \n",
       "                     53  168     1734.0   647.0  \n",
       "                     52  167     1673.0   735.0  \n",
       "                     51  165     1642.0   602.0  \n",
       "                     49  165     1704.0   735.0  \n",
       "                     47  166     1765.0   779.0  \n",
       "                     46  169     1856.0   867.0  \n",
       "                     50  169     1734.0   735.0  \n",
       "                     60  166     1551.0   647.0  \n",
       "                     59  165     1489.0   558.0  \n",
       "...                                 ...     ...  \n",
       "                     30  119     2926.0  2279.0  \n",
       "                     33  118     3384.0  2941.0  \n",
       "                     35  117     3384.0  2941.0  \n",
       "                     34  121     3323.0  2897.0  \n",
       "                     33  90      3781.0  3426.0  \n",
       "                     30  90      3750.0  3206.0  \n",
       "                     28  88      2559.0  1705.0  \n",
       "                     31  88      3720.0  3206.0  \n",
       "                     33  88      3537.0  3029.0  \n",
       "                     120 140     1765.0  1618.0  \n",
       "                     118 139     1918.0  1663.0  \n",
       "                     115 138     1949.0  1530.0  \n",
       "                     114 139     2102.0  1795.0  \n",
       "                     118 141     1949.0  1707.0  \n",
       "                     120 142     1918.0  1618.0  \n",
       "                     122 144     2194.0  1839.0  \n",
       "                     124 145     2347.0  1972.0  \n",
       "                     125 143     2163.0  1751.0  \n",
       "                     96  154     2775.0  2501.0  \n",
       "                     97  157     2316.0  1972.0  \n",
       "                     96  158     2377.0  2060.0  \n",
       "                     95  159     2102.0  1883.0  \n",
       "                     93  153     2836.0  2501.0  \n",
       "                     89  153     2560.0  2281.0  \n",
       "                     90  156     2499.0  2148.0  \n",
       "           2         20  109     2146.0  1486.0  \n",
       "                     21  117     2367.0  1556.0  \n",
       "                     37  17      2977.0  2007.0  \n",
       "                     33  29      2439.0  1694.0  \n",
       "                     27  51      2048.0  1451.0  \n",
       "\n",
       "[131 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aim is to get the data from the dataframe (which holds references to the pixel's location, along with the\n",
    "# assigned class for that pixel), use it to extract the spectral data for that pixel, format it appropriately\n",
    "# and pass it to the classification algorithm to teach it.\n",
    "\n",
    "# useful variables for pulling out data from Xarray\n",
    "sat_bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']\n",
    "dc_bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'cloud_mask']\n",
    "\n",
    "# make the required columns\n",
    "sat_bands_loc = []\n",
    "for band in sat_bands:\n",
    "    trainingdata[band] = np.nan\n",
    "    sat_bands_loc.append(trainingdata.columns.get_loc(band))\n",
    "    \n",
    "# loop through the different locations used for the training data.\n",
    "for loc in trainingdata.index.levels[0]:\n",
    "    \n",
    "    # build the Xarray for that location\n",
    "    data = getData(loc)\n",
    "    # only look at the training data for that location\n",
    "    subset = trainingdata.loc[loc]\n",
    "    # for each row (ie each pick) at that location\n",
    "    for i in range(len(subset)):\n",
    "        # unpack the multilevel pandas index into components for accessing the correct Xarray pixel\n",
    "        scene, y, x = subset.iloc[i].name\n",
    "        vals = data[x, y, scene].sel(band=dc_bands)\n",
    "        if vals.sel(band='cloud_mask').values == 0:\n",
    "            # if the pixel is valid (no cloud), take the spectral bands\n",
    "            if np.isfinite(vals.sel(band=sat_bands).values).all():\n",
    "                # if all the bands have readings (no NaNs), save the relevant bits into X and Y\n",
    "                trainingdata.loc[(loc, scene, y, x), sat_bands] = vals.sel(band=sat_bands).values\n",
    "\n",
    "# save the latest version of trainingdata somewhere good\n",
    "time = str(datetime.datetime.now()).split('.')[0].replace(' ','_')\n",
    "trainingdata.to_pickle('../traningdata_' + time + '.pkl')                \n",
    "                \n",
    "trainingdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# scale and normalize the data\n",
    "X_fortraining = preprocessing.scale(trainingdata.dropna(axis=0, how = 'any')[sat_bands].values)\n",
    "X_fortraining = preprocessing.normalize(X_fortraining)\n",
    "\n",
    "# create a support vector classifer, and fit the data to it\n",
    "# might be worth trying a RandomForest Classifier\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_fortraining,trainingdata.dropna(axis=0, how = 'any')['landcover'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting Remaining Data for Classification & Classifying It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just run the classification over mt barker, will need to loop this through all eventually\n",
    "# study_area = 'swmelb'\n",
    "# data = getData(study_area)\n",
    "\n",
    "#setting up the xarray to store the results for easy plotting later\n",
    "for newband in ['landcover','predicted_landcover']:\n",
    "    test = data[:,:,:].sel(band='red').copy()\n",
    "    test.band.values = newband\n",
    "    test.values[:] = np.nan\n",
    "    data = xr.concat([data, test], dim='band')\n",
    "\n",
    "# useful variable for down the track\n",
    "shape = data.values.shape\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "# classify one scene at a time, save the results to the xarray\n",
    "# rewrite this to work pixel by pixel on timeseries, will tie in better to change detection\n",
    "for scene in range(shape[2]):\n",
    "    \n",
    "    # setting up dataframe multilevel indexes\n",
    "    col_idx = list(range(shape[0])) * shape[1]\n",
    "    row_idx = []\n",
    "    for i in range(shape[1]):\n",
    "        row_idx += [i] * shape[0]\n",
    "    scene_idx = [scene] * (shape[0] * shape[1])\n",
    "\n",
    "    # reshape the data into a 2D flat array for scikit learn\n",
    "    flattened = data[:,:,scene].sel(band=dc_bands).values.reshape(shape[0] * shape[1], len(dc_bands))\n",
    "\n",
    "    # add the data to a new DataFrame, set up the columns and index\n",
    "    alldata = pd.DataFrame(flattened)\n",
    "    alldata.columns = dc_bands\n",
    "    alldata['row'] = row_idx\n",
    "    alldata['column'] = col_idx\n",
    "    alldata['scene_num'] = scene_idx\n",
    "    alldata['study_area'] = study_area\n",
    "    alldata = alldata.set_index(['study_area','scene_num','row','column'])\n",
    "\n",
    "    # join in the training data. This is a SQL left join, so only adds data to current study area\n",
    "    alldata = alldata.reset_index().join(trainingdata[['landcover']], on=trainingdata.index.names).set_index(alldata.index.names)\n",
    "\n",
    "    # reduce alldata down to valid pixels (ie cloudmask), and non-training pixels (ie landcover is still NaN)\n",
    "    datatoclassify = alldata[alldata['cloud_mask'] == 0 & np.isnan(alldata['landcover'])].copy()\n",
    "    # remove pixels with a np.nan as scikit-learn doesn't like them. Only keep spectral bands\n",
    "    datatoclassify = datatoclassify[np.isnan(datatoclassify['landcover'])][sat_bands]\n",
    "    # cast these relevant columns into a numpy array\n",
    "    datatoclassify_np = np.array(datatoclassify)\n",
    "    \n",
    "    # to deal with an entirely clouded scene\n",
    "    if len(datatoclassify_np) == 0:\n",
    "        continue\n",
    "        \n",
    "    # scale and normalize the data so it resembles the training data.\n",
    "    datatoclassify_np = preprocessing.scale(datatoclassify_np)\n",
    "    datatoclassify_np = preprocessing.normalize(datatoclassify_np)\n",
    "    \n",
    "    # results of predict() are a 1 dimensional numpy array of the same length as the input data\n",
    "    # assign these results to a new column in the dataframe\n",
    "    datatoclassify['predicted_landcover'] = clf.predict(datatoclassify_np)\n",
    "    \n",
    "    # SQL left join the results back onto the original data\n",
    "    alldata = alldata.reset_index().join(datatoclassify[['predicted_landcover']], on=trainingdata.index.names).set_index(alldata.index.names)\n",
    "    \n",
    "    #save the training data and classification results into the Xarray\n",
    "    data[:,:,scene].loc[dict(band='landcover')] = alldata['landcover'].values.reshape(shape[0],shape[1])\n",
    "    data[:,:,scene].loc[dict(band='predicted_landcover')] = alldata['predicted_landcover'].values.reshape(shape[0],shape[1])\n",
    "\n",
    "print('Time Taken: ' + str(datetime.datetime.now() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Viewing the Classification Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## function: drawClassifiedScene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def drawClassifiedScene(data, scene_num, alpha):\n",
    "    \n",
    "    # colour map included incase of need to display false colour or other in the future\n",
    "    # could change this to an ordereddict and remove the RGB list created below...?\n",
    "    colourmap = {'R':'red', 'G':'green', 'B':'blue'}\n",
    "    \n",
    "    # combine the data for the 3 bands to be displayed into a single numpy array\n",
    "    h = data.shape[1]\n",
    "    w = data.shape[0]\n",
    "    t = data.shape[2]\n",
    "    \n",
    "    if scene_num > (t -1):\n",
    "        scene_num = t - 1\n",
    "    RGB = ['R','G','B']\n",
    "    date = str(data[:,:,scene_num].date.values)\n",
    "    \n",
    "    # create array to store the RGB info in, and fill by looping through the colourmap variable\n",
    "    # note the .T at the end, because the data array is setup as a (x,y,t), but imshow works (y,x)\n",
    "    rawimg = np.zeros((h, w, 3), dtype=np.float32)\n",
    "    for i in range(len(RGB)):     \n",
    "        rawimg[:,:,i] = data[:,:,scene_num].sel(band=colourmap[RGB[i]]).T\n",
    "        \n",
    "    # equalizing for all bands together\n",
    "    # goal is to make is human interpretable\n",
    "    img_toshow = exposure.equalize_hist(rawimg, mask = np.isfinite(rawimg))    \n",
    "\n",
    "    # displaying the results and formatting the axes etc\n",
    "    plt.imshow(img_toshow)\n",
    "    ax = plt.gca()\n",
    "    ax.set_title('True Colour Landsat Scene, taken\\n' + date + ', over ' + study_area)\n",
    "    \n",
    "    # make the colour map for the cover classes\n",
    "    cmap = colors.ListedColormap(colours)\n",
    "    \n",
    "    # draw the classification results and the training data results\n",
    "    ax.imshow(data[:,:,scene_num].sel(band='predicted_landcover').values.T, cmap = cmap, alpha = alpha)\n",
    "    ax.imshow(data[:,:,scene_num].sel(band='landcover').values.T, cmap = cmap, alpha = 1)\n",
    "    \n",
    "    #draw a legend for the classification colours\n",
    "    legend_patches = []\n",
    "    for cover in landcover.keys():\n",
    "        legend_patches.append(mpatches.Patch(color = colours[landcover[cover]-1], label = cover))\n",
    "    ax.legend(handles = legend_patches)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## function: drawClassifiedPlots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def drawClassifiedPlots(data, scene_num, alpha):\n",
    "    \n",
    "    ax1 = plt.subplot2grid([2,4],[0,0], rowspan = 2, colspan = 2)\n",
    "    ax1.clear()\n",
    "    ax1 = drawClassifiedScene(data, scene_num, alpha = 0)\n",
    "    ax2 = plt.subplot2grid([2,4],[0,2], rowspan = 2, colspan = 2)\n",
    "    ax2.clear()\n",
    "    ax2 = drawClassifiedScene(data, scene_num, alpha)\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## function: check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def check(data, scene_num, alpha):\n",
    " \n",
    "    # control the figure size\n",
    "    fig = plt.figure(figsize=[12,7])\n",
    "    axs = fig.axes\n",
    "    plt.subplots_adjust(hspace = 0.6)\n",
    "    \n",
    "    # draw the figure\n",
    "    drawClassifiedPlots(data, scene_num, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Drawing the Reults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    interact(check,\n",
    "             data = fixed(data),\n",
    "             scene_num = IntSlider(value = 1, min = 0, max = 2000,description = \"Scene Number\"),\n",
    "             alpha= FloatSlider(value = 0.6, min = 0, max = 1, description = \"Classification Transparency\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series of Classifications into Change Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## function: modalFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this function gets called a lot. Needs to be significantly sped up!\n",
    "# dict instead of list.index()?\n",
    "# remove first line\n",
    "# remove length limit at start\n",
    "\n",
    "def modalFilter(df, column, index, span = 10):\n",
    "    df = df[~np.isnan(df[column])]\n",
    "    if index > (len(df) - span) + 1:\n",
    "        return np.nan\n",
    "    else:\n",
    "        mode_arr = df[column][index : index + span].mode().values\n",
    "        if len(mode_arr) > 1:\n",
    "            mode_arr2 = df[column][index : index + span + 1].mode().values\n",
    "            if len(mode_arr2) > 1:\n",
    "                heirarchy = [1, 3, 4, 2] # vegetation, earth, water, urban\n",
    "                max_priority = 4\n",
    "                i_max = 0\n",
    "                for i in mode_arr:\n",
    "                    if heirarchy.index(i) < max_priority:\n",
    "                        max_priority = heirarchy.index(i)\n",
    "                        i_max = i\n",
    "                return i_max\n",
    "            else:\n",
    "                return mode_arr2[0] \n",
    "        else:\n",
    "            return mode_arr[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## function: dateStringToFloat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def dateStringToFloat(s):\n",
    "    year = int(s[0:4])\n",
    "    month = int(s[5:7])\n",
    "    \n",
    "    if month in [1,2,3]:\n",
    "        year += 0.125\n",
    "    elif month in [4,5,6]:\n",
    "        year += 0.375\n",
    "    elif month in [7,8,9]:\n",
    "        year += 0.625\n",
    "    else:\n",
    "        year == 0.875\n",
    "    \n",
    "    return year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Change Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# setting up the results raster\n",
    "changedates_arr = np.zeros((shape[1], shape[0]), dtype=np.float32)\n",
    "changedates_arr[changedates_arr == 0] = np.nan\n",
    "\n",
    "# variables for the modal filtering\n",
    "mode_span = 10\n",
    "MoM_span = 3\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "print('Start Time: ' + str(start))\n",
    "\n",
    "# a very slow nested loop, keen to remove if possible\n",
    "for x in range(shape[0]):\n",
    "    for y in range(shape[1]):\n",
    "# for x in range(50, 70):\n",
    "#     for y in range(100, 120):\n",
    "        # make a dataframe of the time-series of the predicted classifications\n",
    "        pixeldata = pd.DataFrame(data[x, y, :].sel(band='predicted_landcover').values, index = data.date.values, columns = ['predicted_landcover'])\n",
    "        # drop NaNs\n",
    "        pixeldata = pixeldata.dropna(axis = 0, how='any')\n",
    "\n",
    "        #make a new column to store mode in\n",
    "        pixeldata['mode'] = np.nan\n",
    "        pixeldata['mode_of_modes'] = np.nan\n",
    "        mode_loc = pixeldata.columns.get_loc('mode')\n",
    "        \n",
    "        # remove any possibility of duplicate dates (based on an issue with swmelbourne study area)\n",
    "        pixeldata = pixeldata[~pixeldata.index.duplicated(keep = 'first')]\n",
    "        \n",
    "        # description of current decision rule for assigning urban change:\n",
    "        #  calculate the mode over each 10 scenes, and assign that value to the first scene of the 10\n",
    "        #     if the first two modal values are both urban, assume already developed and move to next pixel\n",
    "        #     if not, calculate the mode of the modes, over each 3 modes, assigning the value to the first\n",
    "        #  find the first mode of modes that is urban for that pixel\n",
    "        #     use that data (MOM_change date) to find the group of 3 modes that contributed to the mode of modes\n",
    "        #     within the individual classifications for that pixel within that group of 30 classifications\n",
    "        #        if there is an instance of 2 classifications in a row being urban, use the date of the first of those\n",
    "        #        if not, simply use the first instance of an urban pixel in that range of 30\n",
    "        \n",
    "        # Ideas for improvement:\n",
    "        #    - Deal with water pixels (eg if >25% of classifications are water, ignore)\n",
    "        \n",
    "        # Limitations of Method:\n",
    "        #    - Many of them!\n",
    "        #    - Algorithm needs first 20 to establish baseline - so it can't detect early change\n",
    "        #    - Algorithm needs last 30 to build mode of modes - so it can't detect most recent change\n",
    "        #    - It's very slow - mulitple nested loops. Main speed up would come from being able to do\n",
    "        #       Modal filter as a df.apply(modalFilter)\n",
    "        \n",
    "        # find mode of each 10 scenes, and store at first index of range\n",
    "        # for example the mode of scenes 10-19 will be stored in row 10.\n",
    "        for row in range(0, len(pixeldata), mode_span):\n",
    "            pixeldata.iloc[row, mode_loc] = modalFilter(pixeldata, 'predicted_landcover', row, span = mode_span)\n",
    "            \n",
    "        # if either of the first two modes are urban, assume pixel is already urban at start of landsat archive    \n",
    "        if (pixeldata[~np.isnan(pixeldata['mode'])].iloc[0:2].values ==  2).any():\n",
    "            continue\n",
    "        \n",
    "        # view or slice of data with modes\n",
    "        modes = pixeldata[~np.isnan(pixeldata['mode'])]\n",
    "        \n",
    "        # third level of nested of loops :(\n",
    "        # applying modal filter to the modes, to create mode of modes\n",
    "        # save the result in the pixeldata dataframe\n",
    "        for row in range(0,len(modes),MoM_span):\n",
    "            pixeldata.loc[modes.iloc[row].name, 'mode_of_modes'] = modalFilter(modes, 'mode', row, span = MoM_span)\n",
    "        \n",
    "        # decision criteria\n",
    "        if len(pixeldata[pixeldata['mode_of_modes'] == 2]) > 0:\n",
    "            MoM_changedate = pixeldata[pixeldata['mode_of_modes'] == 2].iloc[0].name\n",
    "            M_ss = pixeldata.loc[MoM_changedate::]\n",
    "            M_changedate = M_ss[M_ss['mode'] == 2].iloc[0].name\n",
    "            M_changedate_loc = pixeldata.index.get_loc(M_changedate)\n",
    "            pix_ss = pixeldata.iloc[M_changedate_loc - mode_span : M_changedate_loc + mode_span]\n",
    "            twoinarow = pix_ss[(pix_ss['predicted_landcover'] == pix_ss['predicted_landcover'].shift(-1)) & \n",
    "                                (pix_ss['predicted_landcover'][pix_ss['predicted_landcover'] == 2])]\n",
    "            if len(twoinarow) > 0:\n",
    "                changedate = twoinarow.iloc[0].name\n",
    "                changedates_arr[y, x] = dateStringToFloat(changedate)\n",
    "            else:\n",
    "                changedate = pix_ss[pix_ss['predicted_landcover'] == 2].iloc[0].name\n",
    "            changedates_arr[y, x] = dateStringToFloat(changedate)\n",
    "              \n",
    "# print how long the modal filtering and change detection took                \n",
    "print('Processing Time: ' + str(datetime.datetime.now() - start))           \n",
    "\n",
    "# save the results to a .pkl for future access\n",
    "results_save_location = '../' + study_area + '/changeresults_svm2.pkl'\n",
    "changedates_arr.dump(results_save_location)  \n",
    "print('Results have been saved to', results_save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[18,145,:].sel(band='predicted_landcover').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Modal Filter Testing and Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mt barker pixel that definitely changed\n",
    "# pixeldata = pd.DataFrame(data[129, 39, :].sel(band='predicted_landcover').values, index = data.date.values, columns = ['predicted_landcover'])\n",
    "\n",
    "# mt barker pixel that was always urban\n",
    "# pixeldata = pd.DataFrame(data[40, 63, :].sel(band='predicted_landcover').values, index = data.date.values, columns = ['predicted_landcover'])\n",
    "\n",
    "# mt barker pixel that was exception causing\n",
    "pixeldata = pd.DataFrame(data[106, 120, :].sel(band='predicted_landcover').values, index = data.date.values, columns = ['predicted_landcover'])\n",
    "\n",
    "pixeldata = pixeldata.dropna(axis = 0, how='any')\n",
    "pixeldata['mode'] = np.nan\n",
    "pixeldata['mode_of_modes'] = np.nan\n",
    "mode_loc = pixeldata.columns.get_loc('mode')\n",
    "\n",
    "\n",
    "for row in range(0, len(pixeldata), mode_span):\n",
    "    pixeldata.iloc[row, mode_loc] = modalFilter(pixeldata, 'predicted_landcover', row, span = mode_span)\n",
    "\n",
    "# # if either of the first two modes are urban, assume pixel is already urban at start of landsat archive    \n",
    "if (pixeldata[~np.isnan(pixeldata['mode'])].iloc[0:2].values ==  2).any():\n",
    "    print('Already Urban')\n",
    "\n",
    "modes = pixeldata[~np.isnan(pixeldata['mode'])]\n",
    "\n",
    "for row in range(0,len(modes),MoM_span):\n",
    "    pixeldata.loc[modes.iloc[row].name, 'mode_of_modes'] = modalFilter(modes, 'mode', row, span = MoM_span)\n",
    "    \n",
    "if len(pixeldata[pixeldata['mode_of_modes'] == 2]) > 0:\n",
    "    MoM_changedate = pixeldata[pixeldata['mode_of_modes'] == 2].iloc[0].name\n",
    "    M_ss = pixeldata.loc[MoM_changedate::]\n",
    "    M_changedate = M_ss[M_ss['mode'] == 2].iloc[0].name\n",
    "    M_changedate_loc = pixeldata.index.get_loc(M_changedate)\n",
    "    \n",
    "    pix_ss = pixeldata.iloc[M_changedate_loc - mode_span : M_changedate_loc + mode_span]\n",
    "# pixeldata[np.isfinite(pixeldata['mode_of_modes'])]\n",
    "    twoinarow = pix_ss[(pix_ss['predicted_landcover'] == pix_ss['predicted_landcover'].shift(-1)) & \n",
    "                        (pix_ss['predicted_landcover'][pix_ss['predicted_landcover'] == 2])]\n",
    "\n",
    "    if len(twoinarow) > 0:\n",
    "        changedate = twoinarow.iloc[0].name\n",
    "    else:\n",
    "        changedate = pix_ss[pix_ss['predicted_landcover'] == 2].iloc[0].name\n",
    "changedate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "changedates_arr[106, 120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing the Change Detection Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## drawAnalysedScene()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def drawAnalysedScene(data, scene_num, alpha, change_grid):\n",
    "    \n",
    "    # colour map included incase of need to display false colour or other in the future\n",
    "    # could change this to an ordereddict and remove the RGB list created below...?\n",
    "    colourmap = {'R':'red', 'G':'green', 'B':'blue'}\n",
    "    \n",
    "    # combine the data for the 3 bands to be displayed into a single numpy array\n",
    "    h = data.shape[1]\n",
    "    w = data.shape[0]\n",
    "    t = data.shape[2]\n",
    "    \n",
    "    if scene_num > (t -1):\n",
    "        scene_num = t - 1\n",
    "    RGB = ['R','G','B']\n",
    "    date = str(data[:,:,scene_num].date.values)\n",
    "    \n",
    "    # create array to store the RGB info in, and fill by looping through the colourmap variable\n",
    "    # note the .T at the end, because the data array is setup as a (x,y,t), but imshow works (y,x)\n",
    "    rawimg = np.zeros((h, w, 3), dtype=np.float32)\n",
    "    for i in range(len(RGB)):     \n",
    "        rawimg[:,:,i] = data[:,:,scene_num].sel(band=colourmap[RGB[i]]).T\n",
    "        \n",
    "    # equalizing for all bands together\n",
    "    # goal is to make is human interpretable\n",
    "    img_toshow = exposure.equalize_hist(rawimg, mask = np.isfinite(rawimg))    \n",
    "\n",
    "    # displaying the results and formatting the axes etc\n",
    "    plt.imshow(img_toshow)\n",
    "    ax = plt.gca()\n",
    "    ax.set_title('True Colour Landsat Scene, taken\\n' + date + ', over ' + study_area)\n",
    "    \n",
    "    # define the current colour map to display the change results raster properly\n",
    "    current_cmap = matplotlib.cm.get_cmap('Reds_r')\n",
    "    current_cmap.set_under('k', alpha=0.0)\n",
    "    current_cmap.set_over('r', alpha=1.0)\n",
    "    current_cmap.set_bad('k', alpha=0.0)  \n",
    "    \n",
    "    # draw the change detection results mask\n",
    "    ax.imshow(change_grid, alpha = alpha, interpolation='none', cmap = current_cmap, clim = [0.5, 0.6])\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## drawAnalysedPlots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def drawAnalysedPlots(data, left_scene_num, left_alpha, left_change_grid,\n",
    "                      right_scene_num, right_alpha, right_change_grid):\n",
    "    \n",
    "    ax1 = plt.subplot2grid([2,4],[0,0], rowspan = 2, colspan = 2)\n",
    "    ax1.clear()\n",
    "    ax1 = drawAnalysedScene(data, left_scene_num, left_alpha, left_change_grid)\n",
    "    ax2 = plt.subplot2grid([2,4],[0,2], rowspan = 2, colspan = 2)\n",
    "    ax2.clear()\n",
    "    ax2 = drawAnalysedScene(data, right_scene_num, right_alpha, right_change_grid)\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## function: results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def results(data, left_scene_num, left_alpha, left_change_grid,\n",
    "                      right_scene_num, right_alpha, right_change_grid):\n",
    " \n",
    "    # control the figure size\n",
    "    fig = plt.figure(figsize=[12,7])\n",
    "#     axs = fig.axes\n",
    "#     plt.subplots_adjust(hspace = 0.6)\n",
    "    \n",
    "    # draw the figure\n",
    "    drawAnalysedPlots(data, left_scene_num, left_alpha, left_change_grid,\n",
    "                      right_scene_num, right_alpha, right_change_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Existing Change Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the results ready for comparison\n",
    "# no change should be np.NaN\n",
    "# chanage = 1\n",
    "change = gdal.Open('../' + study_area + '/change_time.img').ReadAsArray()\n",
    "change[change == 0] = np.nan\n",
    "\n",
    "peterchange = change.copy()\n",
    "peterchange[np.isfinite(peterchange)] = 1\n",
    "\n",
    "# mikechange = changedates_arr.copy()\n",
    "# mikechange[np.isfinite(mikechange)] = 1\n",
    "results_save_location = '../' + study_area + '/changeresults_svm2.pkl'\n",
    "mike_results = np.load(results_save_location)\n",
    "mikechange = mike_results.copy()\n",
    "mikechange[np.isfinite(mikechange)] = 1\n",
    "\n",
    "num_scenes = data.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw Results Analysis Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    interact(results,\n",
    "             data = fixed(data),\n",
    "             left_scene_num = IntSlider(value = 1, min = 0, max = num_scenes -1 ,description = \"Scene Number\"),\n",
    "             left_alpha= FloatSlider(value = 0.6, min = 0, max = 1, description = \"Left Alpha\"),\n",
    "             left_change_grid = fixed(peterchange),\n",
    "             right_scene_num = IntSlider(value = num_scenes - 1, min = 0, max = num_scenes - 1,description = \"Scene Number\"),\n",
    "             right_alpha= FloatSlider(value = 0.6, min = 0, max = 1, description = \"Right Alpha\"),\n",
    "             right_change_grid = fixed(mikechange))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "print(np.nanmean(mike_results - change))\n",
    "imshow(mike_results - change)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "930px",
    "left": "0px",
    "right": "1481px",
    "top": "111px",
    "width": "252px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "widgets": {
   "state": {
    "c52feb267d464c0681dc5b8825029c6a": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
