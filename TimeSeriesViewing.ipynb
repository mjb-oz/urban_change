{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook exists to allow time series analysis of pixels or areas.\n",
    "\n",
    "This notebook has been setup to work with table of contents and heading collapsing jupyter extensions. These are definitely worth using!\n",
    "\n",
    "The output is an interactive matplotlib figure with two scenes from the area of interest rendered in true colour, along with scatter plots for the two main bands of interest for each scene, and an average spectral signature for each scene.\n",
    "\n",
    "The main benefit to this visualistion though is the ability to see the the time series trend for a band or number of bands for a given pixel or area. This is one in a single chart, which also indicates the relative location of each of the scenes being viewed on the timeseries, and also shows some key threshold lines.\n",
    "\n",
    "This particular notebook has been written to look at output from Peter's urban change detection algorithm, so if that algorithm hasn't been run over the study area, this code will break without some tweaks.\n",
    "\n",
    "Written by Mike Barnes as part of his third graduate rotation, 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "import gdal\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider, FloatSlider, Dropdown\n",
    "from IPython.display import display\n",
    "\n",
    "from skimage import exposure\n",
    "from scipy.signal import lfilter\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Loading Data and Building the Xarray\n",
    "This project built on some existing work by Peter Tan. An output from Peter's urban change detection algorithm is raster files with all the relevant NBAR (analysis ready satellite derived surface reflectance readings) data saved to the output directory. To speed the loading and analysis during this script, this notebook will use those exisitng files if they are available. Otherwise it will load the data from the Digital Earth Australia archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: checkForLocalFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkForLocalFiles(study_area):\n",
    "    rootdir = os.listdir('../')\n",
    "    if study_area in rootdir:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: getData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(study_area):\n",
    "    # if the study area is a string, and is accessible locally, load it\n",
    "    if isinstance(study_area, str):\n",
    "        if checkForLocalFiles(study_area):\n",
    "            data = getLocalData(study_area)\n",
    "            print('Done')\n",
    "            return data\n",
    "    # if the study area is a string and is on the list, load it\n",
    "        else:\n",
    "            data = DCLoadName(study_area)\n",
    "    # if the study area is a list of coordinates, use them to load the data\n",
    "    elif isinstance(study_area, list) and len(study_area) == 4:\n",
    "        data = DCLoad(study_area)\n",
    "        \n",
    "    # if the study area isn't loaded locally, transfrom the DC originated xarray into \"my\" format  \n",
    "    if not checkForLocalFiles(study_area):\n",
    "        data = transformXarrayToCustomStyle(data)\n",
    "        print('Done')\n",
    "        return data\n",
    "    else:\n",
    "        print('Data Loading Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: DCLoadName\n",
    "This function is a wrapper for the DCLoad function, that allows previously used study areas to be easily restudied\n",
    "by easily loading exactly the same area of interest (AOI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DCLoadName(study_area):   \n",
    "    if study_area == 'mtbarker':\n",
    "        lat_min = -35.05\n",
    "        lat_max = -35.08\n",
    "        lon_min = 138.85\n",
    "        lon_max = 138.895  \n",
    "    elif study_area == 'swmelb':\n",
    "        lat_min = -37.879\n",
    "        lat_max = -37.91\n",
    "        lon_min = 144.705\n",
    "        lon_max = 144.76  \n",
    "    elif study_area == 'gunghalin':\n",
    "        lat_min = -35.18\n",
    "        lat_max = -35.21\n",
    "        lon_min = 149.14\n",
    "        lon_max = 149.17\n",
    "    elif study_area == 'goldengrove': \n",
    "        lat_min = -34.77\n",
    "        lat_max = -34.8\n",
    "        lon_min = 138.66\n",
    "        lon_max = 138.73\n",
    "    elif study_area == 'molonglo':\n",
    "        lat_min = -35.3\n",
    "        lat_max = -35.33\n",
    "        lon_min = 149.015\n",
    "        lon_max = 149.06\n",
    "    elif study_area == 'nperth':\n",
    "        lat_min = -31.686\n",
    "        lat_max = -31.73\n",
    "        lon_min = 115.79\n",
    "        lon_max = 115.813\n",
    "    elif study_area == 'swbris':\n",
    "        lat_min = -27.66\n",
    "        lat_max = -27.7 \n",
    "        lon_min = 152.877\n",
    "        lon_max = 152.93\n",
    "    elif study_area == 'swsyd':\n",
    "        lat_min = -33.993\n",
    "        lat_max = -34.04\n",
    "        lon_min = 150.715 \n",
    "        lon_max = 150.78\n",
    "    \n",
    "    return DCLoad([lat_min, lat_max, lon_min, lon_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: DCLoad\n",
    "This function is a variation of a datacube query supplied by Erin Telfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DCLoad(study_area):\n",
    "    # to time how long the load takes\n",
    "    start = datetime.datetime.now()\n",
    "    print('Loading data') \n",
    "    print('Load Started At: ' + str(start))\n",
    "    \n",
    "    # define temporal range \n",
    "    start_of_epoch = '1987-01-01'\n",
    "    end_of_epoch =  '2017-10-31'\n",
    "\n",
    "    # define bands of interest\n",
    "    bands_of_interest = ['blue', 'green', 'red', \n",
    "                         'nir', 'swir1', 'swir2']\n",
    "\n",
    "    # Landsat sensors of interest are defined\n",
    "    sensors = ['ls8', 'ls7', 'ls5'] \n",
    "\n",
    "    # unpack input parameter\n",
    "    lat_min, lat_max, lon_min, lon_max = study_area    \n",
    "\n",
    "    print('Bounding box: ' + str(lat_min) + ' S, ' + str(lon_min) +\n",
    "          ' E to ' + str(lat_max) + ' S, ' + str(lon_max) + ' E' )\n",
    "    print('Epoch: ' + start_of_epoch + ' to ' + end_of_epoch)\n",
    "    print('Sensors: ' + str(sensors))\n",
    "    print('Bands of Interest: ' + str(bands_of_interest))\n",
    "\n",
    "    # create query\n",
    "    query = {'time': (start_of_epoch, end_of_epoch),}\n",
    "    query['x'] = (lon_min, lon_max)\n",
    "    query['y'] = (lat_max, lat_min)\n",
    "    query['crs'] = 'EPSG:4326'\n",
    "\n",
    "    #Create cloud mask. This will define which pixel quality (PQ) artefacts are removed from the results.\n",
    "    # It should be noted the \"land_sea\" code will remove all ocean/sea pixels.\n",
    "    mask_components = {'cloud_acca':'no_cloud',\n",
    "    'cloud_shadow_acca' :'no_cloud_shadow',\n",
    "    'cloud_shadow_fmask' : 'no_cloud_shadow',\n",
    "    'cloud_fmask' :'no_cloud',\n",
    "    'blue_saturated' : False,\n",
    "    'green_saturated' : False,\n",
    "    'red_saturated' : False,\n",
    "    'nir_saturated' : False,\n",
    "    'swir1_saturated' : False,\n",
    "    'swir2_saturated' : False,\n",
    "    'contiguous':True,\n",
    "    'land_sea': 'land'}\n",
    "\n",
    "    # Connect to DataCube\n",
    "    dc = datacube.Datacube(app='Urban Change Detection')\n",
    "    \n",
    "    # Data for each Landsat sensor is retrieved and saved in a dict for concatenation\n",
    "    sensor_clean = {}\n",
    "    \n",
    "    for sensor in sensors:\n",
    "        # Load the NBAR and corresponding PQ\n",
    "        sensor_nbar = dc.load(product= sensor+'_nbar_albers', group_by='solar_day', \n",
    "                              measurements = bands_of_interest,  **query)\n",
    "        sensor_pq = dc.load(product= sensor+'_pq_albers', group_by='solar_day', \n",
    "                            fuse_func=ga_pq_fuser, **query)\n",
    "\n",
    "        # Retrieve the projection information before masking/sorting\n",
    "        crs = sensor_nbar.crs\n",
    "        crswkt = sensor_nbar.crs.wkt\n",
    "        affine = sensor_nbar.affine        \n",
    "\n",
    "        # Combing the pq so it is a single \n",
    "        sensor_all = xr.auto_combine([sensor_pq,sensor_nbar])\n",
    "        sensor_clean[sensor] = sensor_all\n",
    "\n",
    "        print('Loaded %s' % sensor) \n",
    "\n",
    "    print('Concatenating')\n",
    "    nbar_clean = xr.concat(sensor_clean.values(), 'time')\n",
    "    nbar_clean = nbar_clean.sortby('time')\n",
    "    nbar_clean.attrs['crs'] = crs\n",
    "    nbar_clean.attrs['affin|e'] = affine    \n",
    "\n",
    "    print ('Load and Xarray build complete')\n",
    "    print('Process took ' + str(datetime.datetime.now() - start))\n",
    "    \n",
    "    # return xarray\n",
    "    return nbar_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: getLocalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLocalData(study_area):\n",
    "    \"\"\"A quick helper function to load the output files from Peter's code for the given location.\n",
    "    It returns and Xarray of the landsat data for that study area.\"\"\"\n",
    "    # build a list of all files in the directory (ie the folder for that location)\n",
    "    location = '../' + study_area + '/'\n",
    "    files = os.listdir(location)\n",
    "\n",
    "    print('Loading data from: ' + location)\n",
    "    \n",
    "    # build a list of all the NBAR*.img file names and which bands they represent\n",
    "    NBARfiles = []\n",
    "    bands = []\n",
    "    for file in files:\n",
    "        if file[-4::] == '.img' and file[0:4] == 'NBAR':\n",
    "            NBARfiles.append(file)\n",
    "            bands.append(file.split('NBAR_')[1].split('.img')[0])\n",
    "\n",
    "    # open all the .img files with NBAR in the name, convert to numpy array, swap axes so order is (x, y, t)\n",
    "    # and save to dict\n",
    "    raw_data = {}\n",
    "    for i in range(len(NBARfiles)):\n",
    "        raw_data[bands[i]] = gdal.Open(location + NBARfiles[i]).ReadAsArray().swapaxes(0,2)\n",
    "#     num_scenes = len(raw_data['red'][0][0])   # delete this?\n",
    "\n",
    "    # build a list of all the dates represented by each band in the NBAR files\n",
    "    # reuse the list of NBAR file names, but this time access the .hdr file\n",
    "    in_dates = False\n",
    "    dates = []\n",
    "    for line in open(location + NBARfiles[0].split('.img')[0] + '.hdr'):\n",
    "        if line[0] == '}':\n",
    "            continue\n",
    "        if in_dates:\n",
    "            dates.append(line.split(',')[0].strip())\n",
    "        if line[0:10] == 'band names':\n",
    "            in_dates = True\n",
    "\n",
    "    # save list of satellite originated bands\n",
    "    sat_bands = bands.copy()\n",
    "\n",
    "    # add the yet to be calculated derivative bands to the overall bands list\n",
    "    bands += ['cloud_mask', 'evi','ndvi','albedo']\n",
    "\n",
    "    # building the Xarray\n",
    "    # define the size for the numpy array that will hold all the data for conversion into XArray\n",
    "    x = len(raw_data['red'])\n",
    "    y = len(raw_data['red'][0])\n",
    "    t = len(raw_data['red'][0][0])\n",
    "    n = len(bands)\n",
    "\n",
    "    # create an empty numpy array of the correct size\n",
    "    alldata = np.zeros((x, y, t, n), dtype=np.float32)\n",
    "\n",
    "    # populate the numpy array with the satellite data\n",
    "    # turn all no data NBAR values to NaNs\n",
    "    for i in range(len(sat_bands)):\n",
    "        alldata[:,:,:,i] = raw_data[sat_bands[i]]\n",
    "        alldata[:,:,:,i][alldata[:,:,:,i] == -999] = np.nan\n",
    "\n",
    "    # convert the numpy array into an xarray, with appropriate lables, and axes names\n",
    "    data = xr.DataArray(alldata, coords = {'x':range(x), 'y':range(y), 'date':dates, 'band':bands},\n",
    "                 dims=['x', 'y', 'date', 'band'])\n",
    "    \n",
    "    # import cloudmask and add to xarray\n",
    "    cloudmask = gdal.Open(location + 'tsmask.img').ReadAsArray().swapaxes(0,2)\n",
    "    data.loc[:,:,:,'cloud_mask'] = cloudmask\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: transformXarrayToCustomStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformXarrayToCustomStyle(data_new):\n",
    "    # loop through and extract all the values into a list of 3D numpy arrays\n",
    "    bands = ['blue','green','red','nir','swir1','swir2','pixelquality']\n",
    "    all_data = []\n",
    "    for band in bands:\n",
    "        all_data.append(data_new.variables[band].values.swapaxes(2,0).astype(np.float32))\n",
    "        \n",
    "    # making some placeholders for yet to be derived \"bands\" - specific to the TSViewer\n",
    "    extra_bands = ['evi','ndvi','albedo']\n",
    "    for band in extra_bands:\n",
    "        d = np.zeroslike(all_data[0])\n",
    "        d[d == 0] = np.nan\n",
    "        all_data.append(d)\n",
    "\n",
    "    # replace 'pixelquality' with 'cloud_mask'  \n",
    "    bands[bands.index('pixelquality')] = 'cloud_mask'\n",
    "\n",
    "    # stack the list of 3D arrays along a 4th dimension\n",
    "    data_flipped = np.stack(all_data, axis = -1)\n",
    "    # set all bad NBAR values to np.nan\n",
    "    data_flipped[data_flipped == -999] = np.nan\n",
    "    # set all good pixels to 0 based on PQ indicator\n",
    "    # see https://www.sciencedirect.com/science/article/pii/S0034425717301086 for PQ description\n",
    "    data_flipped[data_flipped[:,:,:,-1] == 16383] = 0\n",
    "\n",
    "    # save the new shape for use in defining the Xarray\n",
    "    new_shape = data_flipped.shape\n",
    "\n",
    "    # build a list of all the dates (as strings)\n",
    "    dates = []\n",
    "    for time in range(len(data_new.time)):\n",
    "        dates.append(np.datetime_as_string(data_new.time[time].values)[0:10])\n",
    "\n",
    "    # make the full list of bands\n",
    "    all_bands = bands + extra_bands\n",
    "    \n",
    "    # assemble the new Xarray\n",
    "    newdatafixed = xr.DataArray(data_flipped, coords = {'x': range(new_shape[0]), 'y': range(new_shape[1]),\n",
    "                                                        'date': dates, 'band': all_bands}, dims=['x','y','date','band'])\n",
    "    return newdatafixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: getChangeResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChangeResults(study_area):\n",
    "    # open the results of the change algorithm, and format for eay plotting\n",
    "    change = gdal.Open('../' + study_area + '/change_time.img').ReadAsArray()\n",
    "\n",
    "    return change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: getChangeMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChangeMask(study_area):\n",
    "    # open the results of the change algorithm, and format for eay plotting\n",
    "    change = gdal.Open('../' + study_area + '/change_time.img').ReadAsArray()\n",
    "    change_flat_mask = change.copy()\n",
    "    \n",
    "    # remove the dates, so you have a mask for change or no change\n",
    "    change_flat_mask[change_flat_mask != 0] = 1\n",
    "    change_flat_mask[change_flat_mask == 0] = np.nan\n",
    "    \n",
    "    return change_flat_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: populateExtraBands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populateExtraBands(study_area):\n",
    "    # calculate NDVI \"band\"\n",
    "    a = (data.loc[:,:,:,'nir'] - data.loc[:,:,:,'red'])\n",
    "    b = (data.loc[:,:,:,'nir'] + data.loc[:,:,:,'red'])\n",
    "    b_pos = b.where(b.values > 0)\n",
    "    data.loc[:,:,:,'ndvi'] = np.nan\n",
    "    data.loc[:,:,:,'ndvi'] = a/b_pos\n",
    "\n",
    "    # calculate EVI \"band\"\n",
    "    g  = 2.5\n",
    "    c1 = 6\n",
    "    c2 = 7.5\n",
    "    l  = 1\n",
    "    data.loc[:,:,:,'evi'] = g * ((a/10000) /\n",
    "        ((data.loc[:,:,:,'nir']/10000) + (c1 * (data.loc[:,:,:,'red']/10000)) - (c2 * (data.loc[:,:,:,'blue']/10000)) + l))\n",
    "\n",
    "    # calculate average albedo \"band\"\n",
    "    sat_bands = ['blue','green','red','nir','swir1','swir2']\n",
    "    for band in sat_bands:\n",
    "        data.loc[:,:,:,'albedo'] += data.loc[:,:,:,band]\n",
    "    data.loc[:,:,:,'albedo'] = data.loc[:,:,:,'albedo'] / (6 * 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Manipulation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getTimeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTimeSeries(xpos, ypos, ts_bands, apply_cloud_mask = True):\n",
    "    \"\"\"\n",
    "    This function retrives the time series data for a given pixel and band.\n",
    "    It then applies the cloud/shadow mask (if requested) to the results and to the dates for each scene.\n",
    "    Finally, it returns the good pixels for each of the requested bands as a multi column pandas dataframe,\n",
    "    with the date formatted as a datetime object and set as in the index\n",
    "    \n",
    "    Possible flaw with logic by only masking pixels when they are clouded, not entire query area if any or some\n",
    "    is partially clouded. Need to explore this thought further\n",
    "    \"\"\"       \n",
    "    if isinstance(xpos, list) and len(xpos) == 1:\n",
    "        xpos.append(xpos[0] + 1)\n",
    "    elif not isinstance(xpos, list) and is_number(xpos):\n",
    "        xpos = [int(xpos), xpos + 1]\n",
    "        \n",
    "    if isinstance(ypos, list) and len(ypos) == 1:\n",
    "        ypos.append(ypos[0] + 1)\n",
    "    elif not isinstance(ypos, list) and is_number(ypos):\n",
    "        ypos = [int(ypos), ypos + 1]\n",
    "    \n",
    "    # if looking for an area and not masking for cloud, do this\n",
    "    if not apply_cloud_mask:\n",
    "        ts_data = pd.DataFrame(data[xpos[0]:xpos[1], ypos[0]:ypos[1],:].sel(band = ts_bands).mean(dim=['x','y'], skipna = True).to_pandas())\n",
    "        \n",
    "    # if looking for an area and masking for cloud, do this\n",
    "    else:\n",
    "        ts_data = pd.DataFrame(data[xpos[0]:xpos[1], ypos[0]:ypos[1], :].sel(band = ts_bands).where(\n",
    "            data[xpos[0]:xpos[1], ypos[0]:ypos[1], :].sel(band ='cloud_mask') == 0).mean(dim=['x','y'], skipna = True).to_pandas())\n",
    "    \n",
    "    ts_data.index = pd.to_datetime(ts_data.index) \n",
    "    ts_data.columns = [ts_bands]\n",
    "    return ts_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filterNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterNoise(df):\n",
    "    \"\"\"\n",
    "    This function applies a scipy.signal.lfilter IIR filter to the time series data to smooth out\n",
    "    some of the spikes that are visible in this timeseries for each of the columns in the supplied\n",
    "    pandas dataframe.\n",
    "    Peters C++ code used a different filter, so this function may be replaced or deprecated in the\n",
    "    future\n",
    "    \"\"\"\n",
    "    # set up required parameters for the function\n",
    "    n = 15                 # bigger n means more smoothing\n",
    "    b = [1.0 / n] * n\n",
    "    a = 1      \n",
    "    for col in df.columns:\n",
    "        df[col] = lfilter(b, a, df[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calcRollingAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcRollingAvg(df, window = '365D', min_periods = 6):\n",
    "    \"\"\"\n",
    "    The function uses the pandas rolling average function to calculate the yearly rolling\n",
    "    average of the bands. It returns the dataframe it was given with the column values modified\n",
    "    by the rolling average function.\n",
    "    The window size and minimum number of periods for the average to be considered valid are optional\n",
    "    parameters\n",
    "    Peter's code used either 1 or 2 years windows depending on frequency of valid observations - this might\n",
    "    be included in the future\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].rolling(window = window, min_periods = min_periods).mean()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calcYearDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcYearDiff(df, years = 1):\n",
    "    \"\"\"\n",
    "    Returns the difference between a timeseries and the timeseries at some point in the future.\n",
    "    It does this by joining a time adjusted series onto the original data and interpolating any missing\n",
    "    dates, and then cleaning up the NaN mess resulting.\n",
    "    \"\"\"\n",
    "    # convert years to days\n",
    "    days = years * 365\n",
    "    \n",
    "    # duplicate data\n",
    "    df2 = df.copy()\n",
    "\n",
    "    # reduce the index by a year, essentially moving the average for the following year back one\n",
    "    # this might need a bit of tweaking to deal with leap years...?\n",
    "    df2.index = np.array(df2.index) - np.timedelta64(days, 'D')\n",
    "    \n",
    "    # building lists of column names that will be needed\n",
    "    cols = list(df.columns)\n",
    "    newcols = []\n",
    "    interpcols = []\n",
    "    diffcols = []\n",
    "    for col in cols:\n",
    "        newcols.append(col + '-1y')\n",
    "        interpcols.append(col + '-1y_interp')\n",
    "        diffcols.append(col + '_diff')\n",
    "    \n",
    "    # rename df2 columns to avoid clashes in the merge\n",
    "    df2.columns = newcols\n",
    "\n",
    "    # SQL style outer join (ie all data is kept, NaNs for blank values)\n",
    "    df = pd.merge(df, df2, left_index=True, right_index=True, how='outer')\n",
    " \n",
    "    # interpolate the timeseries values in the future and calculate the difference from that date\n",
    "    for i in range(len(cols)):\n",
    "        df[interpcols[i]] = df[newcols[i]].interpolate(method='time')\n",
    "        df[diffcols[i]] = df[interpcols[i]] - df[cols[i]]\n",
    "\n",
    "    # tidying up unwanted rows and columns\n",
    "    # can do dropna because if there was no original data, the difference calc result is NaN\n",
    "    df = df.dropna(subset=diffcols)\n",
    "    df = df.drop(cols + newcols + interpcols, axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gatherAndPrepTSData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gatherAndPrepTSData(xpos,ypos,ts_bands):\n",
    "    \"\"\"\n",
    "    This function takes the x and y coordinates of interest, along with the ts_bands dict.\n",
    "    It pulls the required data from the xarray, and applies the various processing steps specified in the dict\n",
    "    It returns a pandas dataframe of all the processed data, with a date index, and each column named as\n",
    "    specified in the dict.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    cols = []\n",
    "    # ts_bands is the very important dict, it specifies the name of the result, the band the result is built\n",
    "    # off, and which of the processing steps will be performed on that band data\n",
    "    for TS in ts_bands:\n",
    "        df = getTimeSeries(xpos, ypos, TS['band'], apply_cloud_mask = TS['cloud_mask'])\n",
    "        # drop rows with clouds masked\n",
    "        df = df.dropna(axis = 0, how = 'all')\n",
    "        \n",
    "        # drop any duplicate dates\n",
    "        df = df[~df.index.duplicated(keep = 'first')]\n",
    "\n",
    "        # filter noise\n",
    "        if TS['smoothed']:\n",
    "            df = filterNoise(df)\n",
    "\n",
    "        # calculate rolling average\n",
    "        if TS['RollingAve']:\n",
    "            df = calcRollingAvg(df)\n",
    "            # drop rows with no rolling average values\n",
    "            df = df.dropna(axis = 0, how = 'all')\n",
    "\n",
    "        # calculate difference between TS value and a period into the future\n",
    "        if TS['TSDiff']:\n",
    "            df = calcYearDiff(df)\n",
    "        \n",
    "        dfs.append(df)\n",
    "        cols.append(TS['name'])\n",
    "    dfs = pd.concat(dfs, axis = 1)\n",
    "    dfs.columns = cols\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for plotting and other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define_TS_YAxisRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_TS_YAxisRange(data, DiffTS = False, n_stddevs = 0.5):\n",
    "    \"\"\"\n",
    "    This function defines the y axis range for a timeseries plot of a given band (supplied as a pandas \n",
    "    single column dataframe or series, aka data. It calculates the standard deviation of the data, and \n",
    "    returns a list with the lowest and highest values for the axes based on the number of standard deviations\n",
    "    away from the mean (which can be varied through the second parameter as desired)\n",
    "    \"\"\"\n",
    "    if DiffTS:\n",
    "        mag = data.std(skipna = True) * n_stddevs\n",
    "        return [-mag, mag]\n",
    "    else:\n",
    "        mean = data.mean(skipna = True)\n",
    "        std = data.std(skipna = True)\n",
    "        low = mean - (n_stddevs * std)\n",
    "        hi = mean + (n_stddevs * std)\n",
    "        return [low,hi]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### is_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    \"\"\"\n",
    "    A quick helper function pinched from stack overflow to test if a variable is a float or int\n",
    "    \"\"\"\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getSceneFilteredIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSceneFilteredIndex(scene_num, df):\n",
    "    \"\"\"\n",
    "    This function find the X axis location needed for a given scene to be displayed on the Time Series View.\n",
    "    This is used draw the lines on the timeseries that represent the scenes being viewed.\n",
    "    If the scene being viewed is part of that pixel's timeseries, the line returned is a solid line.\n",
    "    If the scene being viewed is not part of that pixel's timeseries (due to cloud masking), the\n",
    "    line returned is a dashed line.\n",
    "    The line is returned as a number, linestyle pair, for plugging into the matplotlib.axvline function\n",
    "    \"\"\"\n",
    "    # find the date of the scene_number from the list of all dates before clouds mask application\n",
    "    linedate = pd.to_datetime(dates[scene_num])\n",
    "    \n",
    "    # for the simple case, when the scene is part of the pixel's time series\n",
    "    if linedate in df.index:\n",
    "        nearest_num = df.index.get_loc(linedate)\n",
    "        return [nearest_num, '-']\n",
    "    \n",
    "    # when the pixel is not part of the time series and is more recent than the most recent valid\n",
    "    # scene, use the last valid scenes index\n",
    "    elif linedate > df.index[-1]:\n",
    "        nearest_num = df.index.get_loc(df.index[-1])\n",
    "        return [nearest_num, '--']\n",
    "    \n",
    "    # when the schene is not part of the pixel's time series, draw the line at the first valid scene's\n",
    "    # index AFTER the scene in question\n",
    "    else:   \n",
    "        nearest_num = df.index.get_loc(df[df.index > linedate].index[0])\n",
    "        return [nearest_num, '--']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getChangePeriodTimeRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChangePeriodTimeRange(df, daterange):\n",
    "    \"\"\"\n",
    "    This function changes the calculated change date from Peter's algorithm and converts it\n",
    "    to the equivalent range of row numbers for the given timeseries (supplied as a pandas dataframe).\n",
    "    The change date calculated is converted to the start and end dates of the quarter, and this is \n",
    "    turned into a datetime object, and used in boolean masking to define the first and last row\n",
    "    of that interval\n",
    "    \"\"\"\n",
    "    # flaw with the logic here, if someone passes a daterange with more than 2 entries, only the first\n",
    "    # and last are used. So it works, as long as its used well.\n",
    "    for i in range(len(daterange)):\n",
    "        year = str(int(daterange[i]))    \n",
    "        # generate the string equivalents of the start and end dates for the quarter\n",
    "        if daterange[i] % 1 == 0.125:\n",
    "            start = '-01-01'\n",
    "            end = '-03-31'\n",
    "        if daterange[i] % 1 == 0.375:\n",
    "            start = '-04-01'\n",
    "            end = '-06-30'\n",
    "        if daterange[i] % 1 == 0.625:\n",
    "            start = '-07-01'\n",
    "            end = '-09-30'\n",
    "        if daterange[i] % 1 == 0.875:\n",
    "            start = '-10-01'\n",
    "            end = '-12-31'\n",
    "        if i == 0:\n",
    "            startdate = pd.to_datetime(year + start) # turn strings into datetime objects\n",
    "        else:\n",
    "            enddate = pd.to_datetime(year + end)    \n",
    "    \n",
    "    # calculdate the start and end date index of the range bounded by start and end datetimes\n",
    "    start_idx = df[(df.index >= startdate) & (df.index <= enddate)].index[0]\n",
    "    end_idx = df[(df.index >= startdate) & (df.index <= enddate)].index[-1]    \n",
    "  \n",
    "    # get the row number of the start and end indexes\n",
    "    start_row = df.index.get_loc(start_idx)    \n",
    "    end_row = df.index.get_loc(end_idx)\n",
    "    \n",
    "    # return result as a list, ready to directly be passed to axis.set_ylim()\n",
    "    return [start_row, end_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getChangeDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChangeDate(xpos, ypos):\n",
    "    \"\"\"\n",
    "    This function returns the max and min change date for a given range of pixels based on the\n",
    "    output of the algorithm. It works if the range is only a single pixel.\n",
    "    It returns a string if there was no change detected in that area.\n",
    "    \"\"\"\n",
    "    # select the pixels in the area of interest\n",
    "    pix = change[ypos[0]:ypos[1], xpos[0]:xpos[1]]\n",
    "    # mask for > 0\n",
    "    pix = pix[pix > 0]\n",
    "    # if there are no pixels > 0, break things\n",
    "    if len(pix) == 0:\n",
    "        return \"No Change\"\n",
    "    # find hte max and min and return them as a list\n",
    "    mini = pix.min()\n",
    "    maxi = pix.max()\n",
    "    return [mini, maxi]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drawScene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawScene(scene_num, change_trans, xpos, ypos):\n",
    "    \"\"\"\n",
    "    This function draws a landsat scene from the data in true colour, and overlays the results of the\n",
    "    change detection algorithm. The scene to be displayed is determined by the scene_num parameter, while the\n",
    "    transparency (alpha) of the change results is dictacted by the change_trans parameter (0 = transparent,\n",
    "    1 = totally opaque). It formats the axes appropriately and returns the axes to the caller.\n",
    "    It also displays the bounding box of the pixels being analysed.\n",
    "    \"\"\"\n",
    "    # colour map included incase of need to display false colour or other in the future\n",
    "    # could change this to an ordereddict and remove the RGB list created below...?\n",
    "    colourmap = {'R':'red', 'G':'green', 'B':'blue'}\n",
    "    \n",
    "    # define the current colour map to display the change results raster properly\n",
    "    current_cmap = matplotlib.cm.get_cmap('Reds_r')\n",
    "    current_cmap.set_under('k', alpha=0.0)\n",
    "    current_cmap.set_over('r', alpha=1.0)\n",
    "    current_cmap.set_bad('k', alpha=0.0)  \n",
    "    \n",
    "    # combine the data for the 3 bands to be displayed into a single numpy array\n",
    "    h = data.shape[1]\n",
    "    w = data.shape[0]\n",
    "    RGB = ['R','G','B']\n",
    "    \n",
    "    # create array to store the RGB info in, and fill by looping through the colourmap variable\n",
    "    # note the .T at the end, because the data array is setup as a (x,y,t), but imshow works (y,x)\n",
    "    rawimg = np.zeros((h, w, 3), dtype=np.float32)\n",
    "    for i in range(len(RGB)):     \n",
    "        rawimg[:,:,i] = data[:,:,scene_num].sel(band=colourmap[RGB[i]]).T\n",
    "        \n",
    "    # equalizing for all bands together\n",
    "    # goal is to make is human interpretable\n",
    "    img_toshow = exposure.equalize_hist(rawimg, mask = np.isfinite(rawimg))    \n",
    "\n",
    "    # displaying the results and formatting the axes etc\n",
    "    plt.imshow(img_toshow)\n",
    "    ax = plt.gca()\n",
    "    ax.set_title('True Colour Landsat Scene, taken\\n' + dates[scene_num] + ', over ' + study_area)\n",
    "    ax.imshow(change_flat_mask, alpha = change_trans, interpolation='none', cmap = current_cmap, clim = [0.5, 0.6])\n",
    "    \n",
    "    # plot up the displayed pixel, or draw a box around the queried pixels\n",
    "    # logic re . vs box could be improved here\n",
    "    height = abs(ypos[1] - ypos[0])\n",
    "    width = abs(xpos[1] - xpos[0])\n",
    "    if height > 2 and width > 2:\n",
    "        rect = matplotlib.patches.Rectangle((xpos[0],ypos[0]), abs(xpos[1] - xpos[0]), abs(ypos[1] - ypos[0]),\n",
    "                                            color = 'blue', linestyle = '-', fill = False, alpha = change_trans)\n",
    "        ax.add_patch(rect)\n",
    "    else:\n",
    "        ax.plot((xpos[0] + xpos[1])/2, (ypos[0] + ypos[1])/2, color = 'b', marker = '.', alpha = change_trans)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drawSpectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawSpectrum(xpos, ypos, scene_num):\n",
    "    \"\"\"\n",
    "    This function draws the spectral signature for a selected range of pixels in a selected scene.\n",
    "    It gathers the data together, averages the band values, and returns them plotted on nicely labelled\n",
    "    axes, which are returned to the caller\n",
    "    \"\"\"\n",
    "    # include Landsat-8 bands incase of future work\n",
    "    bandorder = {'ultrablue':1, 'blue':2, 'green':3, 'red':4, 'nir':5, 'cirrus':6, 'swir1':7,\n",
    "                 'swir2':8, 'thermal':9,'panchromatic':10}\n",
    "    sat_bands = ['blue','green','red','nir','swir1','swir2']\n",
    "    \n",
    "    # do lots of funkiness to pull the data together and order it so the data are displayed\n",
    "    # in order of increase wavelength\n",
    "    xvals = []\n",
    "    yvals = []\n",
    "    for band in sat_bands:\n",
    "        xvals.append(bandorder[band])\n",
    "        yvals.append(float(data[xpos[0]:xpos[1],ypos[0]:ypos[1],scene_num].sel(band=band).mean(dim=['x','y']).values))\n",
    "    yvals_sorted = [y for x, y in sorted(zip(xvals,yvals))]\n",
    "    xlabels_sorted = [y for x, y in sorted(zip(xvals, sat_bands))]\n",
    "    xvals_sorted = range(len(yvals))\n",
    "    \n",
    "    # plot the data, and make it look nice\n",
    "    plt.plot(xvals_sorted, yvals_sorted)\n",
    "    ax = plt.gca()\n",
    "    ax.set_title('Band Values for Selected Area')\n",
    "    ax.set_ylim([0,6000])\n",
    "    ax.set_ylabel('Average NBAR Value')\n",
    "    ax.set_xticks(xvals_sorted)\n",
    "    ax.set_xticklabels(xlabels_sorted)\n",
    "    ax.set_xlabel('Band')\n",
    "    return ax  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drawScatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawScatter(scene_num, ts_bands, xpos, ypos):\n",
    "    \"\"\"\n",
    "    This function draws a 2D scatter plot of the scene, based on the two bands of interest (specified by\n",
    "    x_axis and y_axis). It returns the scatter plot to the caller.\n",
    "    This basically assumes that the first and second bands in ts_bands are the two of most interest to the\n",
    "    user, so ts_bands needs to be structured appropriately.\n",
    "    The longer term goal would be to have dropdown boxes that would allow the user to select which band from the\n",
    "    xarray they would like for each axis.\n",
    "    \"\"\"\n",
    "    # set the axis bands\n",
    "    x_axis = ts_bands[0]['band']\n",
    "    y_axis = ts_bands[1]['band']\n",
    "    \n",
    "    # get the 1D array of the values for each axis\n",
    "    x = data[:,:,scene_num].sel(band = x_axis).values\n",
    "    y = data[:,:,scene_num].sel(band = y_axis).values\n",
    "    \n",
    "    # pull out the values from the bounding box selected\n",
    "    sel_x = data[xpos[0]:xpos[1], ypos[0]:ypos[1], scene_num].sel(band = x_axis).values\n",
    "    #create a full size array full of NaN\n",
    "    sel_x_all = np.full(x.shape, np.nan)\n",
    "    #set the corresponding area of the NaN array to the values from the bounding box\n",
    "    sel_x_all[xpos[0]:xpos[1],ypos[0]:ypos[1]] = sel_x\n",
    "    \n",
    "    # repeat\n",
    "    sel_y = data[xpos[0]:xpos[1], ypos[0]:ypos[1], scene_num].sel(band = y_axis).values\n",
    "    sel_y_all = np.zeros_like(y)\n",
    "    sel_y_all[xpos[0]:xpos[1],ypos[0]:ypos[1]] = sel_y\n",
    "    \n",
    "    \n",
    "    # build a mask of where the values are both valid (ie not NaN)\n",
    "    mask = np.isfinite(x) & np.isfinite(y)\n",
    "    mask2 = np.isfinite(sel_x_all) * np.isfinite(sel_y_all)\n",
    "    \n",
    "    # make plot, label axes and return\n",
    "    plt.plot(x[mask], y[mask],'.', color = 'blue', label = 'All Points')\n",
    "    plt.plot(sel_x_all[mask2], sel_y_all[mask2],'.', color = 'red', label = 'Selected Points')\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlabel(x_axis)\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylabel(y_axis)\n",
    "    ax.set_ylim([0,5])\n",
    "    ax.legend()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drawTimeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawTimeSeries(xpos,ypos,ts_bands,left_scene_num = 0, right_scene_num = 1):\n",
    "    \"\"\"\n",
    "    This function draws the time series for a given pixel and band (or list of 2 bands).\n",
    "    It calls the timeSeries function to gather the data and strip out the cloud, and then\n",
    "    formats the axes appropriately for up to 2 bands and returns them to the caller.\n",
    "    It also indicates where on the time axes each of the scenes being viewed sits, and displays them as\n",
    "    either solid or dashed lines depending on if the pixels in question are cloud masked or not.\n",
    "    It also displays as a yellow box the time range over which the change detection algorithm has indicated\n",
    "    change has occured.\n",
    "    Finally, it shows as horizontal lines the thresholds used for defining change in Peter's algorithm\n",
    "    \"\"\" \n",
    "    # get the data, filter cloud, filter noise, calculate rolling average, drop NaN rows, calc difference\n",
    "    df = gatherAndPrepTSData(xpos, ypos, ts_bands)\n",
    "    # can't remember what this is for, I think it can be deleted now\n",
    "    yearDiff = True\n",
    "\n",
    "    # see if/when that pixel changed\n",
    "    changedate = getChangeDate(xpos, ypos)\n",
    "    changed = False\n",
    "    if isinstance(changedate, list):     #changedate only returns a list if there was change\n",
    "        changed = True\n",
    "        # if it did change, get the row numbers of the first and last scene taken during the change quarter\n",
    "        # to draw the box aronud the potential change period\n",
    "        changetimerange = getChangePeriodTimeRange(df, changedate)\n",
    "    \n",
    "    # only plot up the first 4 bands requested (this might be changed soon)\n",
    "    cols = list(df.columns)\n",
    "    colours = ['green', 'red', 'blue', 'orange', 'black']\n",
    "    axs = []\n",
    "    n_axes = min(len(cols),5)\n",
    "\n",
    "    # setup some variables for use during the plotting\n",
    "    leftline = getSceneFilteredIndex(left_scene_num, df)    \n",
    "    rightline = getSceneFilteredIndex(right_scene_num, df)    \n",
    "    albedo_thresh = 0.04\n",
    "    evi_thresh = -0.05\n",
    "    resid_evi = 0.18    \n",
    "    \n",
    "    #start plotting\n",
    "    plt.plot(range(len(df)), df[cols[0]], color=colours[0], label = cols[0])\n",
    "    ax0 = plt.gca()\n",
    "    pos1 = ax0.get_position()\n",
    "    pos2 = pos1\n",
    "#     pos2 = [pos1.x0 + (0.03 * n_axes), pos1.y0,  pos1.width / (1 + (0.1 * (n_axes - 1))), pos1.height] \n",
    "    ax0.set_position(pos2)\n",
    "    ax0.spines['right'].set_color(colours[0])\n",
    "\n",
    "    # draw the scene viewer lines\n",
    "    ax0.axvline(x = leftline[0], linestyle = leftline[1]) \n",
    "    ax0.axvline(x = rightline[0], linestyle = rightline[1])\n",
    "    \n",
    "    # draw a zero line (x axis) for the TSDiff lines\n",
    "    ax0.axhline(y = 0, color = 'black', linestyle = '-')\n",
    "     \n",
    "    # if there was change, draw the yellow rectangle    \n",
    "    if changed:\n",
    "        ax0.axvspan(changetimerange[0],changetimerange[1], color = 'gold', alpha = 0.7)\n",
    "#         yrange = define_TS_YAxisRange(df[cols[0]], yearDiff, 2)\n",
    "#         ax0.text(changetimerange[1], yrange[1] - ((yrange[1] - yrange[0]) / 10), changedate)\n",
    "        \n",
    "    # twin the x axis for creating a date axis on the top\n",
    "    ax_date = ax0.twiny()\n",
    "    # plot some random data with 100% transparency to make the axis appear\n",
    "    ax_date.plot(df.index, df[cols[0]], alpha = 0)\n",
    "    # set the axis to overlap with the original axis\n",
    "    ax_date.set_position(pos2)\n",
    "    \n",
    "    # create a title that specifies the area being displayed\n",
    "    readable_coords = 'x = ' + str(xpos[0]) + ':' + str(xpos[1]) + ', y = ' + str(ypos[0]) + ':' + str(ypos[1])\n",
    "    ax0.set_title('Time Series values for pixels ' + readable_coords, y = 0)\n",
    "\n",
    "    # for all the subsequent axes (not the first column)\n",
    "    for i in range(1, n_axes):\n",
    "        # twin the original axes\n",
    "        axn = ax0.twinx()\n",
    "        axn.set_position(pos2)\n",
    "        # set the spines and the position and the colour\n",
    "        axn.spines['right'].set_position(('axes', - (0.1 * i)))\n",
    "        axn.spines['right'].set_color(colours[i])\n",
    "        # plot the data, coloured accordingly\n",
    "        axn.plot(range(len(df)), df[cols[i]], color=colours[i], label = cols[i])\n",
    "        # add to the axs list of axes\n",
    "        axs.append(axn)\n",
    "    \n",
    "    # the original axis to the start of the list\n",
    "    axs = [ax0] + axs\n",
    "    \n",
    "    for i, ax in enumerate(axs):\n",
    "        # for each axis, calculate the appropriate yaxis range, and set the xaxis range\n",
    "        yrange = define_TS_YAxisRange(df[cols[i]], ts_bands[i]['TSDiff'], 2)\n",
    "        ax.set_ylim(yrange)\n",
    "        ax.set_xlim([0, len(df)])\n",
    "        \n",
    "        if i == 0:\n",
    "            # if its the first axis, move the yaxis label\n",
    "            ax.set_ylabel(cols[i], color=colours[i], labelpad = -10)\n",
    "            # getting specific to the use case now, must be EVIDiff as first column\n",
    "            ax.axhline(y = evi_thresh, color = colours[i], linestyle = '--')\n",
    "            # print the EVI value for the RHS displayed scene\n",
    "            after_evi = float(data[xpos[0]:xpos[1], ypos[0]:ypos[1], right_scene_num].sel(band = 'evi').mean().values)\n",
    "            ax0.text(rightline[0], yrange[1] - ((yrange[1] - yrange[0]) / 10), 'EVI = ' + str(round(after_evi, 3)))       \n",
    "        else:\n",
    "            ax.set_ylabel(cols[i], color=colours[i], labelpad = -40)\n",
    "        # more use case specificness\n",
    "        if i == 1:\n",
    "            ax.axhline(y = albedo_thresh, color = colours[i], linestyle = '--')\n",
    "        if i == 2:\n",
    "            ax.axhline(y = resid_evi, color = colours[i], linestyle = '--')\n",
    "    return axs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drawAllSubplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawAllSubplots(left_scene_num, right_scene_num, change_trans, ts_bands, xpos, ypos, BBoxWidth, BBoxHeight):\n",
    "    \"\"\"\n",
    "    This function defines what plots will be shown, and in what order. This function is called\n",
    "    by the the analyse function and also during the onclick event.\n",
    "    It returns nothing, but draws the axes on command\n",
    "    \"\"\"\n",
    "    # the xpos and ypos here come from the mouse click event, ie are a pixel coordinate.\n",
    "    # need to convert this to a list for use as a bounding box\n",
    "     \n",
    "    if BBoxWidth == 1:\n",
    "        # if only a pixel is wanted, the list will be len == 1\n",
    "        xrange = [xpos, xpos + 1]\n",
    "    else:\n",
    "        # split the witdth in half and take either side\n",
    "        # might need to confirm edge cases here...?\n",
    "        xrange = [int(xpos - BBoxWidth/2), int(xpos + BBoxWidth/2)]\n",
    "    \n",
    "    if BBoxHeight == 1:\n",
    "        yrange = [ypos, ypos + 1]\n",
    "    else:\n",
    "        yrange = [int(ypos - BBoxHeight/2), int(ypos + BBoxHeight/2)]\n",
    "    \n",
    "    # might need some changing in the future regarding formating, plot spacing etc.   \n",
    "    ax1 = plt.subplot2grid([7,4],[0,0], rowspan = 2, colspan = 2)\n",
    "    ax1.clear()\n",
    "    ax1 = drawScene(left_scene_num, change_trans, xrange, yrange)    \n",
    "\n",
    "    ax2 = plt.subplot2grid([7,4],[0,2], rowspan = 2, colspan = 2)\n",
    "    ax2.clear()\n",
    "    ax2 = drawScene(right_scene_num, change_trans, xrange, yrange)  \n",
    "\n",
    "    ax3 = plt.subplot2grid([7,4],[2,0], rowspan = 2, colspan = 4)\n",
    "    ax3.clear()\n",
    "    ax3 = drawTimeSeries(xrange, yrange, ts_bands, left_scene_num, right_scene_num)\n",
    "\n",
    "    ax4 = plt.subplot2grid([7,4],[4,0], rowspan = 2, colspan = 2)\n",
    "    ax4.clear()\n",
    "    ax4 = drawScatter(left_scene_num, ts_bands, xrange, yrange)    \n",
    "\n",
    "    ax5 = plt.subplot2grid([7,4],[4,2], rowspan = 2, colspan = 2)\n",
    "    ax5.clear()\n",
    "    ax5 = drawScatter(right_scene_num, ts_bands, xrange, yrange) \n",
    "    \n",
    "    ax6 = plt.subplot2grid([7,4],[6,0], rowspan = 1, colspan = 2)        \n",
    "    ax6.clear()\n",
    "    ax6 = drawSpectrum(xrange, yrange, left_scene_num)                       \n",
    "\n",
    "    ax7 = plt.subplot2grid([7,4],[6,2], rowspan = 1, colspan = 2)\n",
    "    ax7.clear()\n",
    "    ax7 = drawSpectrum(xrange, yrange, right_scene_num)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "global xpos\n",
    "global ypos\n",
    "xpos = 0\n",
    "ypos = 0\n",
    "  \n",
    "def analysis(left_scene_num, right_scene_num, change_trans, ts_bands, BBoxWidth = 1, BBoxHeight = 1):\n",
    "\n",
    "    def onclick(event):\n",
    "        # defining what to do on a click event\n",
    "        \n",
    "        # I don't understand why this need to be declared global again, but it breaks without these lines\n",
    "        global xpos\n",
    "        global ypos\n",
    "        # need to cast to int as result is a float, and can't index a list with a float\n",
    "        xpos = int(event.xdata)\n",
    "        ypos = int(event.ydata)\n",
    "        drawAllSubplots(left_scene_num, right_scene_num, change_trans, ts_bands, xpos, ypos, BBoxWidth, BBoxHeight)\n",
    "    \n",
    "    # control the figure size\n",
    "    fig = plt.figure(figsize=[10,15])\n",
    "    plt.subplots_adjust(hspace = 0.6)\n",
    "    \n",
    "    # draw the figure\n",
    "    drawAllSubplots(left_scene_num, right_scene_num, change_trans, ts_bands, xpos, ypos, BBoxWidth, BBoxHeight)\n",
    "    #connect the click event action to the figure\n",
    "    cid = fig.canvas.mpl_connect('button_press_event', onclick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosing the Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e62a371e404d9998d06d302352225a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>Dropdown</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Dropdown(description='Study Area', options=('mtbarker', 'swmelb', 'gunghalin', 'goldengrove', 'molonglo', 'nperth', 'swbris', 'swsyd'), value='mtbarker')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a study area drop down list and display it\n",
    "study_areas = ['mtbarker', 'swmelb', 'gunghalin', 'goldengrove', 'molonglo', 'nperth', 'swbris', 'swsyd'] \n",
    "study_area_dd = Dropdown(options=study_areas, value = study_areas[0], description='Study Area', disabled = False)\n",
    "display(study_area_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: ../swmelb/\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in greater\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# importing the data getting everything ready for the visualisation\n",
    "study_area = study_area_dd.value\n",
    "data = getData(study_area)\n",
    "populateExtraBands(study_area)\n",
    "max_x, max_y, num_scenes, _ = data.shape\n",
    "dates = list(data.date.values)\n",
    "change = getChangeResults(study_area)\n",
    "change_flat_mask = getChangeMask(study_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e49c4ba249c4b558fe03f5565a74447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='Left Scene Number', max=1322), IntSlider(value=1322, description='Right Scene Number', max=1322), FloatSlider(value=0.6, description='Change Mask Transparency', max=1.0), IntSlider(value=1, description='BBox Width', max=208, min=1), IntSlider(value=1, description='BBox Height', max=156, min=1), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TS_BANDS is the key datastructure for the whole setup.\n",
    "# it is a list of dicts, each dict being the details required for the timeseries data gathering and preparation\n",
    "# the first 2 items in the list will have their bands used as the x and y axis for the scatter plot\n",
    "\n",
    "# *** dict keys and their use***\n",
    "# name = the name of the dataseries on the resultant plot\n",
    "# band = the band on which the analysis is based, this is used to directly access the xarray, so it must be identical\n",
    "    # to the band in the xarray. \n",
    "# cloud_mask = Boolean as to if you want to use the cloud mask\n",
    "# smoothed = Boolean as to if you want to use the smoothing filter\n",
    "# RollingAve = Boolean as to if you want to use an annual rolling average rather than a raw/smoothed reading\n",
    "# TSDiff = Boolean as to if you want to use the difference between that reading and the one 12 months ago.\n",
    "\n",
    "ts_bands = [\n",
    "    {'name':'evi_diff', 'band':'evi', 'cloud_mask':True, 'smoothed':True, 'RollingAve':True, 'TSDiff':True},\n",
    "    {'name':'albedo_diff', 'band':'albedo', 'cloud_mask':True, 'smoothed':True, 'RollingAve':True, 'TSDiff':True},\n",
    "    {'name':'evi_raw', 'band':'evi', 'cloud_mask':True, 'smoothed':True, 'RollingAve':False, 'TSDiff':False}]\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    interact(analysis,\n",
    "             left_scene_num = IntSlider(value = 1, min = 0, max = num_scenes -1,\n",
    "                                        description = \"Left Scene Number\"),\n",
    "             right_scene_num = IntSlider(value = num_scenes -1, min = 0, max = num_scenes -1,\n",
    "                                         description = \"Right Scene Number\"),\n",
    "             change_trans = FloatSlider(value = 0.6, min = 0, max = 1, description = \"Change Mask Transparency\"),\n",
    "             ts_bands = fixed(ts_bands),\n",
    "            BBoxWidth = IntSlider(value = 1, min = 1, max = max_x, description = \"BBox Width\"),\n",
    "            BBoxHeight = IntSlider(value = 1, min = 1, max = max_y, description = \"BBox Height\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "818px",
    "left": "0px",
    "right": "1044px",
    "top": "111px",
    "width": "252px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "widgets": {
   "state": {
    "c52feb267d464c0681dc5b8825029c6a": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
